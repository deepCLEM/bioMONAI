# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['coolwarm', 'warm_cmap', 'read_yaml', 'dictlist_to_funclist', 'fastTrainer', 'visionTrainer', 'compute_losses',
           'compute_metric', 'calculate_statistics', 'format_sig', 'plot_histogram_and_kde', 'display_statistics_table',
           'evaluate_model', 'evaluate_classification_model', 'attributesFromDict', 'get_device', 'img2float',
           'img2Tensor', 'apply_transforms']

# %% ../nbs/00_core.ipynb 5
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import gaussian_kde
from matplotlib.colors import LinearSegmentedColormap

from torch import Tensor as torchTensor
from torch import tensor
from monai.data import MetaTensor
from monai.utils import set_determinism

import torch.nn.functional as F
from torch.nn.init import kaiming_normal_

from random import randint, random as rand, choice

from skimage import util
from skimage.data import cells3d

import yaml

# %% ../nbs/00_core.ipynb 6
from torch import squeeze as torchsqueeze, max as torchmax, from_numpy as torch_from_numpy, device as torch_device
from torch.cuda import is_available as is_cuda_available

# %% ../nbs/00_core.ipynb 7
from collections.abc import MutableSequence
from typing import MutableSequence
    
from fastai.callback.core import Callback
from fastai.data.all import DataLoaders, Path, trainable_params, delegates, hasattrs, Path, List, L, typedispatch, Normalize
from fastai.optimizer import Adam, OptimWrapper, Optimizer
from fastai.vision.all import BypassNewMeta, DisplayedTransform, store_attr, DataBlock, Learner, ShowGraphCallback, CSVLogger, Any, minimum, steep, valley, slide, create_vision_model, create_timm_model, get_c, default_split, model_meta, ifnone, ClassificationInterpretation
from fastcore.script import risinstance
from fastai.callback.all import *
from fastai.vision.all import *
import fastai.optimizer
import fastai.losses
import fastai.metrics
import torch.optim as toptim
from .datasets import download_medmnist


# %% ../nbs/00_core.ipynb 22
def read_yaml(yaml_path):
    "Reads a YAML file and returns its contents as a dictionary"
    with open(yaml_path, 'r') as file:
        config = yaml.safe_load(file)
    return config 

# %% ../nbs/00_core.ipynb 23
def dictlist_to_funclist(transform_dicts):
    transforms = []
    for trans in transform_dicts:
        if isinstance(trans, str):  
            transform_obj = globals().get(trans)
            transforms.append(transform_obj)
        else: 
            name, params = next(iter(trans.items()))
            transform_obj = globals().get(name) or eval(name) 
            transforms.append(transform_obj(**params))

    return transforms

# %% ../nbs/00_core.ipynb 24
class fastTrainer(Learner):
    """
    A custom implementation of the FastAI Learner class for training models in bioinformatics applications.

    """
    
    def __init__(self, 
                 dataloaders: DataLoaders, # The DataLoader objects containing training and validation datasets.
                 model: callable, # A callable model that will be trained on the dataset.
                 loss_fn: Any | None = None, # The loss function to optimize during training. If None, defaults to a suitable default.
                 optimizer: Optimizer | OptimWrapper = Adam, # The optimizer function to use. Defaults to Adam if not specified.
                 lr: float | slice = 1e-3, # Learning rate for the optimizer. Can be a float or a slice object for learning rate scheduling.
                 splitter: callable = trainable_params, # 
                 callbacks: Callback | MutableSequence | None = None, # A callable that determines which parameters of the model should be updated during training.
                 metrics: Any | MutableSequence | None = None, # Optional list of callback functions to customize training behavior.
                 csv_log: bool = False, # Metrics to evaluate the performance of the model during training.
                 show_graph: bool = True, # Whether to log training history to a CSV file. If True, logs will be appended to 'history.csv'.
                 show_summary: bool = False, # The base directory where models are saved or loaded from. Defaults to None.
                 find_lr: bool = False, # Subdirectory within the base path where trained models are stored. Default is 'models'.
                 find_lr_fn = valley, # Weight decay factor for optimization. Defaults to None.
                 path: str | Path | None = None, # Whether to apply weight decay to batch normalization and bias parameters.
                 model_dir: str | Path = 'models', # Whether to update the batch normalization statistics during training.
                 wd: float | int | None = None, 
                 wd_bn_bias: bool = False, 
                 train_bn: bool = True, 
                 moms: tuple = (0.95,0.85,0.95), # Tuple of tuples representing the momentum values for different layers in the model. Defaults to FastAI's default settings if not specified.
                 default_cbs: bool = True, # Automatically include default callbacks such as ShowGraphCallback and CSVLogger.
                 ):
        cbs = callbacks if callbacks is not None else []  # Ensure cbs is a list
        if default_cbs:
            if show_graph:
                cbs.append(ShowGraphCallback())
            if csv_log:
                cbs.append(CSVLogger(fname='history.csv', append=False))
        
        super().__init__(dataloaders, model, loss_fn, optimizer, lr, splitter, cbs, metrics, path, model_dir, wd, wd_bn_bias, train_bn, moms)
        
        if show_summary:
                print(self.summary())
        if find_lr:
                lr = self.lr_find(suggest_funcs=find_lr_fn)
                self.lr = float('%.1g'%(lr))
                print('Inferred learning rate: ', self.lr)
        

    @classmethod
    def from_yaml(cls, dataloaders, model, yaml_path):
        """
        Method to read from a YAML file and obtain the parameters for FastTrainer.
        """
        # Read the configuration from the yaml file and replace None strings with Nonetype values
        config = read_yaml(yaml_path)
        config = {key: (None if value == "None" else value) for key, value in config.items()}

        
        
        # OBTAIN THE LOSS FUNCTION
        loss_str = config.get('loss_fn', None)
        # Look for the loss function within the variables and check if its valid
        if loss_str == None:
            loss_func = None
        else:
            lf_cls = globals().get(loss_str) or getattr(fm, loss_str, None)
            if lf_cls is None:
                raise ValueError(f"Loss function '{loss_str}' not found or invalid.")
            loss_func = lf_cls()


        # OBTAIN THE OPTIMIZER
        opt_str = config.get('optimizer', 'Adam')
        # Look for the optimizer within the variables and check if its valid
        if isinstance(opt_str, str):
            opt_cls = globals().get(opt_str, None)
            if opt_cls is None and hasattr(fastai.optimizer, opt_str):
                opt_cls = getattr(fastai.optimizer, opt_str)
            if opt_cls is None and hasattr(toptim, opt_str):
                opt_cls = getattr(toptim, opt_str)
            if opt_cls is None or not callable(opt_cls):
                raise ValueError(f"Optimizer '{opt_str}' not found or invalid.")
            opt_func = opt_cls


        # OBTAIN THE METRICS
        metrics_cfg = config.get('metrics', [])
        # Look for the metrics within the variables and check if they are valid
        valid_metrics = []
        if metrics_cfg == None:
            valid_metrics = None
        else:
            valid_metrics = dictlist_to_funclist(metrics_cfg)

        # OBTAIN THE CALLBACKS
        callbacks = []
        cbs = config.get('callbacks', [])
        callbacks = dictlist_to_funclist(cbs)

        # OBTAIN THE SPLITTER
        splitter_str = config.get('splitter', trainable_params)
        if isinstance(splitter_str, str):
            splitter_func = globals().get(splitter_str) or getattr(fastai.learner, splitter_str, None)
        elif splitter_str is None:
            splitter_func = trainable_params 
        else:
                splitter_func = splitter_str


        # OBTAIN OTHER PARAMETERS FROM THE YAML CONFIGURATION 
        lr = config.get('lr', 1e-3)
        csv_log = config.get('csv_log', False)
        show_graph = config.get('show_graph', True)
        show_summary = config.get('show_summary', False)
        path = config.get('path', None)
        model_dir = config.get('model_dir', 'models')
        wd = config.get('wd', None)
        wd_bn_bias = config.get('wd_bn_bias', False)
        train_bn = config.get('train_bn', True)
        moms = config.get('moms', (0.95,0.85,0.95))
   


        # Return all the parameters
        return cls(dataloaders = dataloaders, model = model, loss_fn = loss_func, optimizer = opt_func,
            lr = lr, splitter = splitter_func, callbacks = callbacks, metrics = valid_metrics, path = path,
            model_dir = model_dir, wd = wd, wd_bn_bias = wd_bn_bias, train_bn = train_bn, moms = moms,
            csv_log = csv_log, show_graph = show_graph, show_summary = show_summary          
        )        

# %% ../nbs/00_core.ipynb 30
def _add_norm(dls, meta, pretrained, n_in=3):
    if not pretrained: return
    stats = meta.get('stats')
    if stats is None: return
    if n_in != len(stats[0]): return
    if not dls.after_batch.fs.filter(risinstance(Normalize)):
        dls.add_tfms([Normalize.from_stats(*stats)],'after_batch')

def _timm_norm(dls, cfg, pretrained, n_in=3):
    if not pretrained: return
    if n_in != len(cfg['mean']): return
    if not dls.after_batch.fs.filter(risinstance(Normalize)):
        tfm = Normalize.from_stats(cfg['mean'],cfg['std'])
        dls.add_tfms([tfm],'after_batch')

# %% ../nbs/00_core.ipynb 31
@delegates(create_vision_model)
def visionTrainer(  dataloaders: DataLoaders, # The DataLoader objects containing training and validation datasets.
                    model: callable, # A callable model that will be trained on the dataset.
                    normalize=True, 
                    n_out=None, 
                    pretrained=True, 
                    weights=None,
                    # Trainer args
                    loss_fn: Any | None = None, # The loss function to optimize during training. If None, defaults to a suitable default.
                    optimizer: Optimizer | OptimWrapper = Adam, # The optimizer function to use. Defaults to Adam if not specified.
                    lr: float | slice = 1e-3, # Learning rate for the optimizer. Can be a float or a slice object for learning rate scheduling.
                    splitter: callable = trainable_params, # 
                    callbacks: Callback | MutableSequence | None = None, # A callable that determines which parameters of the model should be updated during training.
                    metrics: Any | MutableSequence | None = None, # Optional list of callback functions to customize training behavior.
                    csv_log: bool = False, # Metrics to evaluate the performance of the model during training.
                    show_graph: bool = True, # Whether to log training history to a CSV file. If True, logs will be appended to 'history.csv'.
                    show_summary: bool = False, # The base directory where models are saved or loaded from. Defaults to None.
                    find_lr: bool = False, # Subdirectory within the base path where trained models are stored. Default is 'models'.
                    find_lr_fn = valley, # Weight decay factor for optimization. Defaults to None.
                    path: str | Path | None = None, # Whether to apply weight decay to batch normalization and bias parameters.
                    model_dir: str | Path = 'models', # Whether to update the batch normalization statistics during training.
                    wd: float | int | None = None, 
                    wd_bn_bias: bool = False, 
                    train_bn: bool = True, 
                    moms: tuple = (0.95,0.85,0.95), # Tuple of tuples representing the momentum values for different layers in the model. Defaults to FastAI's default settings if not specified.
                    default_cbs: bool = True, # Automatically include default callbacks such as ShowGraphCallback and CSVLogger.
                    # model & head args
                    cut=None, 
                    init=kaiming_normal_, 
                    custom_head=None, 
                    concat_pool=True, 
                    pool=True,
                    lin_ftrs=None, 
                    ps=0.5, 
                    first_bn=True, 
                    bn_final=False, 
                    lin_first=False, 
                    y_range=None, 
                    **kwargs):
    "Build a vision trainer from `dataloaders` and `model`"
    if n_out is None: n_out = get_c(dataloaders)
    assert n_out, "`n_out` is not defined, and could not be inferred from data, set `dataloaders.c` or pass `n_out`"
    meta = model_meta.get(model, {'cut':cut, 'split':default_split})
    model_args = dict(init=init, custom_head=custom_head, concat_pool=concat_pool, pool=pool, lin_ftrs=lin_ftrs, ps=ps,
                      first_bn=first_bn, bn_final=bn_final, lin_first=lin_first, y_range=y_range, **kwargs)
    n_in = kwargs['n_in'] if 'n_in' in kwargs else 3
    if isinstance(model, str):
        model,cfg = create_timm_model(model, n_out, default_split, pretrained, **model_args)
        if normalize: _timm_norm(dataloaders, cfg, pretrained, n_in)
    else:
        if normalize: _add_norm(dataloaders, meta, pretrained, n_in)
        model = create_vision_model(model, n_out, pretrained=pretrained, weights=weights, **model_args)

    splitter = ifnone(splitter, meta['split'])
    trainer = fastTrainer(dataloaders, model, loss_fn=loss_fn, optimizer=optimizer, lr=lr, splitter=splitter, callbacks=callbacks, csv_log=csv_log, 
                        show_graph=show_graph, show_summary=show_summary, find_lr=find_lr, find_lr_fn=find_lr_fn, metrics=metrics, path=path, 
                        model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn, moms=moms, default_cbs=default_cbs)
    if pretrained: trainer.freeze()
    # keep track of args for loggers
    store_attr('model,normalize,n_out,pretrained', self=trainer, **kwargs)
    return trainer

# %% ../nbs/00_core.ipynb 33
def compute_losses(predictions, targets, loss_fn):
    """
    Compute the loss for each prediction-target pair.
    """
    return [loss_fn(p.unsqueeze(0), t.unsqueeze(0)).item() for p, t in zip(predictions, targets)]


def compute_metric(predictions, targets, metric_fn):
    """
    Compute the metric for each prediction-target pair.
    Handles cases where metric_fn has or does not have a 'func' attribute.
    """
    # Get the actual function to call (either metric_fn.func or metric_fn itself)
    metric_func = getattr(metric_fn, 'func', metric_fn)
    
    return [metric_func(p.unsqueeze(0), t.unsqueeze(0)).item() for p, t in zip(predictions, targets)]


def calculate_statistics(data):
    """
    Calculate key statistics for the data.
    """
    return {
        "Mean": np.mean(data),
        "Median": np.median(data),
        "Standard Deviation": np.std(data),
        "Min": np.min(data),
        "Max": np.max(data),
        "Q1": np.percentile(data, 25),
        "Q3": np.percentile(data, 75),
    }


def format_sig(value):
    """
    Format numbers with two significant digits.
    """
    if value == 0:
        return "0"
    elif abs(value) < 0.01 or abs(value) > 100:
        return f"{value:.2e}"  # 2 significant digits in scientific notation
    else:
        return f"{value:.2g}"  # Standard notation with 2 significant figures


def plot_histogram_and_kde(data, stats, bw_method=0.3, fn_name=''):
    """
    Plot the histogram and KDE of the data with key statistics marked.
    """
    kde = gaussian_kde(data, bw_method=bw_method)
    x = np.linspace(min(data), max(data), 1000)
    y = kde(x)

    gauss_color = 'lightslategrey'
    plt.figure(figsize=(8, 6))
    plt.hist(data, bins=30, density=True, color='darkgray', edgecolor='black', alpha=0.5)
    plt.plot(x, y, color=gauss_color, lw=2)
    plt.fill_between(x, y, color=gauss_color, alpha=0.3)

    # Add vertical lines for key statistics with formatted significant digits
    plt.axvline(stats["Mean"], color='crimson', linestyle='--', linewidth=1.5, label=f'Mean: {format_sig(stats["Mean"])}')
    plt.axvline(stats["Median"], color='steelblue', linestyle='--', linewidth=1.5, label=f'Median: {format_sig(stats["Median"])}')
    plt.axvline(stats["Q1"], color='purple', linestyle=':', linewidth=1.5, label=f'Q1: {format_sig(stats["Q1"])}')
    plt.axvline(stats["Q3"], color='purple', linestyle=':', linewidth=1.5, label=f'Q3: {format_sig(stats["Q3"])}')

    # Display min, max, std deviation as text on the plot
    plt.text(stats["Mean"] + stats["Standard Deviation"], 0.1, f'Std Dev: {format_sig(stats["Standard Deviation"])}', color='black', fontsize=10)
    plt.text(stats["Min"], 0.02, f'Min: {format_sig(stats["Min"])}', color='black', fontsize=10, ha='center')
    plt.text(stats["Max"], 0.02, f'Max: {format_sig(stats["Max"])}', color='black', fontsize=10, ha='center')

    # Add loss function name to the title
    plt.title(f"Combined Histogram and KDE with Statistics\n{fn_name}")
    plt.xlabel("Loss Value")
    plt.ylabel("Density")
    plt.legend()
    # plt.grid(True)
    plt.show()


def display_statistics_table(stats, fn_name='', as_dataframe=True):
    """
    Display a table of the key statistics.
    """
    if as_dataframe:
        # Convert statistics to a DataFrame and display
        df = pd.DataFrame.from_dict(stats, orient='index', columns=['Value'])
        df.index.name = f"{fn_name}"
        display(df)
    else:
        fig, ax = plt.subplots(figsize=(5, 2))
        ax.axis("off")
        
        # Header title row
        table_data = [[f"{fn_name}", ""]]
        table_data += [[key, format_sig(value)] for key, value in stats.items()]
        
        table = ax.table(cellText=table_data, colLabels=["Statistic", "Value"], cellLoc="center", loc="center")
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.2)
        
        # Style for header row
        header = table[0, 0]
        header.set_fontsize(12)
        header.set_text_props(weight="bold")
        plt.show()

# %% ../nbs/00_core.ipynb 36
# Retrieve the 'coolwarm' colormap
coolwarm = plt.get_cmap('coolwarm')
# Create a new colormap using only the warm colors
warm_cmap = LinearSegmentedColormap.from_list('warm_coolwarm', coolwarm(np.linspace(0.5, 1, coolwarm.N // 2)))

# %% ../nbs/00_core.ipynb 37
def evaluate_model(trainer:Learner,                                 # The model trainer object with a get_preds method.
                   test_data:DataLoaders=None,              # DataLoader containing test data.
                   loss=None,                               # Loss function to evaluate prediction-target pairs.
                   metrics=None,                            # Single metric or a list of metrics to evaluate. 
                   bw_method=0.3,                           # Bandwidth method for KDE. 
                   show_graph=True,                         # Boolean flag to show the histogram and KDE plot.
                   show_table=True,                         # Boolean flag to show the statistics table.
                   show_results=True,                       # Boolean flag to show model results on test data. 
                   as_dataframe=True,                       # Boolean flag to display table as a DataFrame. 
                   cmap='magma',                            # Colormap for visualization.
                   ):
    """
    Calculate and optionally plot the distribution of loss values from predictions
    made by the trainer on test data, with an optional table of key statistics.
    """
    out = dict()
    
    if loss is None:
        loss = trainer.loss_func
        
    if test_data is None:
        p, t = trainer.get_preds()
        # Show results for test data
        if show_results:
            trainer.show_results(cmap=cmap)
    else:
        p, t = trainer.get_preds(dl=test_data)
        # Show results for test data
        if show_results:
            trainer.show_results(dl=test_data, cmap=cmap)

    # Calculate loss for each prediction-target pair
    losses = compute_losses(p, t, loss)
    loss_stats = calculate_statistics(losses)
    loss_name = loss.__class__.__name__  # Get loss function name
    out[loss_name] = losses    
    if show_graph:
        plot_histogram_and_kde(losses, loss_stats, bw_method, loss_name)
    if show_table:
        display_statistics_table(loss_stats, loss_name, as_dataframe=as_dataframe)
            
    if metrics is not None:
            if not isinstance(metrics, list):
                metrics = [metrics]
            # Loop through each metric
            for metric in metrics:
                # Calculate metric values for each prediction-target pair
                metric_values = compute_metric(p, t, metric)
                metric_stats = calculate_statistics(metric_values)         
                # Get the name of the metric function
                metric_name = getattr(metric, 'func', metric).__name__  # Support AvgMetric or regular functions                
                out[metric_name] = metric_values       
                if show_graph:
                    plot_histogram_and_kde(metric_values, metric_stats, bw_method, metric_name)
                if show_table:
                    display_statistics_table(metric_stats, metric_name, as_dataframe=as_dataframe)

    return out


# %% ../nbs/00_core.ipynb 38
def evaluate_classification_model(trainer:Learner,              # The trained model (learner) to evaluate.
                                  test_data:DataLoaders=None,   # DataLoader with test data for evaluation. If None, the validation dataset is used.
                                  loss_fn=None,                 # Loss function used in the model for ClassificationInterpretation. If None, the loss function is loaded from trainer.
                                  most_confused_n:int=1,        # Number of most confused class pairs to display. 
                                  normalize:bool=True,          # Whether to normalize the confusion matrix.
                                  metrics=None,                 # Single metric or a list of metrics to evaluate. 
                                  bw_method=0.3,                # Bandwidth method for KDE. 
                                  show_graph=True,              # Boolean flag to show the histogram and KDE plot.
                                  show_table=True,              # Boolean flag to show the statistics table.
                                  show_results=True,            # Boolean flag to show model results on test data. 
                                  as_dataframe=True,            # Boolean flag to display table as a DataFrame. 
                                  cmap=warm_cmap,               # Color map for the confusion matrix plot. 
                                  ):
    """
    Evaluates a classification model by displaying results, confusion matrix, and most confused classes.
    """
    out = dict()
    
    if loss_fn is None:
            loss_fn = trainer.loss_func
    
    # Interpret the results on test data
    if test_data is None:
        class_int = ClassificationInterpretation.from_learner(trainer)
        p, t = trainer.get_preds()
        # Show results for test data
        if show_results:
            trainer.show_results()
    else:
        class_int = ClassificationInterpretation(trainer, test_data, loss_fn)
        p, t = trainer.get_preds(dl=test_data)
        # Show results for test data
        if show_results:
            trainer.show_results(dl=test_data)
    
    # Plot the confusion matrix
    class_int.plot_confusion_matrix(normalize=normalize, cmap=cmap)
    
    # Print Classification report
    class_int.print_classification_report()
    
    # Show the most confused classes
    print("\nMost Confused Classes:")
    print(class_int.most_confused(most_confused_n))

    # Calculate loss for each prediction-target pair
    losses = compute_losses(p, t, loss_fn)
    loss_stats = calculate_statistics(losses)
    loss_name = loss_fn.__class__.__name__  # Get loss function name
    out[loss_name] = losses
    if show_graph:
        plot_histogram_and_kde(losses, loss_stats, bw_method, loss_name)
    if show_table:
        display_statistics_table(loss_stats, loss_name, as_dataframe=as_dataframe)
            
    if metrics is not None:
            if not isinstance(metrics, list):
                metrics = [metrics]
            # Loop through each metric
            for metric in metrics:
                # Calculate metric values for each prediction-target pair
                metric_values = compute_metric(p, t, metric)
                metric_stats = calculate_statistics(metric_values)                
                # Get the name of the metric function
                metric_name = getattr(metric, 'func', metric).__name__  # Support AvgMetric or regular functions 
                out[metric_name] = metric_values                      
                if show_graph:
                    plot_histogram_and_kde(metric_values, metric_stats, bw_method, metric_name)
                if show_table:
                    display_statistics_table(metric_stats, metric_name, as_dataframe=as_dataframe)
    
    return out


# %% ../nbs/00_core.ipynb 41
def attributesFromDict(d):
    """
    The `attributesFromDict` function simplifies the conversion of dictionary keys and values into object attributes, allowing dynamic attribute creation for configuration objects. This utility is handy for initializing model or dataset configurations directly from dictionaries, improving code readability and maintainability.
    """
    self = d.pop('self')
    for n, v in d.items():
        setattr(self, n, v)

# %% ../nbs/00_core.ipynb 42
def get_device():
    return torch_device("cuda" if is_cuda_available() else "cpu")

# %% ../nbs/00_core.ipynb 43
def img2float(image, force_copy=False):
    return util.img_as_float(image, force_copy=force_copy)

# %% ../nbs/00_core.ipynb 44
def img2Tensor(image):
    return torchTensor(img2float(image))

# %% ../nbs/00_core.ipynb 45
def apply_transforms(image,         # The image to transform
                     transforms,    # A list of transformations to apply
                     ):
    """Apply a list of transformations, ensuring at least one is applied."""
    if not transforms:
        return image  # Return the original image if no transforms are provided
    
    # Randomly select transformations to apply
    applied_transforms = [t for t in transforms if rand() < t.p]
    # Ensure at least one transformation is applied
    if not applied_transforms:
        applied_transforms.append(choice(transforms))
    
    def apply_transform_to_image(img, transform):
        return transform.encodes(img)
    
    # Apply transformations
    if isinstance(image, tuple):
        image1, image2 = image
        for transform in applied_transforms:
            image1 = apply_transform_to_image(image1, transform)
            image2 = apply_transform_to_image(image2, transform)
        return image1, image2
    else:
        for transform in applied_transforms:
            image = apply_transform_to_image(image, transform)
        return image
