{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Neural networks\n",
    "output-file: nets.html\n",
    "title: Networks\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### create_custom_unet\n",
       "\n",
       ">      create_custom_unet (resnet_version, output_channels, img_size=(128, 128),\n",
       ">                          pretrained=True, n_in=1, cut=4)\n",
       "\n",
       "*Create a U-Net model with a ResNet backbone.\n",
       "\n",
       "Returns:\n",
       "- U-Net model with the specified ResNet backbone.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| resnet_version |  |  | Choose a ResNet model between: 'resnet18', 'resnet34', 'resnet50', 'resnet101', and 'resnet152'. |\n",
       "| output_channels |  |  | Number of output channels. |\n",
       "| img_size | tuple | (128, 128) | Tuple for the input image size, default is (128, 128). |\n",
       "| pretrained | bool | True | If True, use a pretrained ResNet backbone. |\n",
       "| n_in | int | 1 | Number of input channels, default is 1 (e.g., grayscale). |\n",
       "| cut | int | 4 | The cut point for the ResNet model, default is 4. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### create_custom_unet\n",
       "\n",
       ">      create_custom_unet (resnet_version, output_channels, img_size=(128, 128),\n",
       ">                          pretrained=True, n_in=1, cut=4)\n",
       "\n",
       "*Create a U-Net model with a ResNet backbone.\n",
       "\n",
       "Returns:\n",
       "- U-Net model with the specified ResNet backbone.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| resnet_version |  |  | Choose a ResNet model between: 'resnet18', 'resnet34', 'resnet50', 'resnet101', and 'resnet152'. |\n",
       "| output_channels |  |  | Number of output channels. |\n",
       "| img_size | tuple | (128, 128) | Tuple for the input image size, default is (128, 128). |\n",
       "| pretrained | bool | True | If True, use a pretrained ResNet backbone. |\n",
       "| n_in | int | 1 | Number of input channels, default is 1 (e.g., grayscale). |\n",
       "| cut | int | 4 | The cut point for the ResNet model, default is 4. |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(create_custom_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L70){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DnCNN\n",
       "\n",
       ">      DnCNN (spatial_dims=2, in_channels=1, out_channels=1, num_of_layers=9,\n",
       ">             features=64, kernel_size=3)\n",
       "\n",
       "*A Deep Neural Network for Image Denoising (DnCNN) model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| spatial_dims | int | 2 | Number of spatial dimensions |\n",
       "| in_channels | int | 1 | Number of input channels |\n",
       "| out_channels | int | 1 | Number of output channels |\n",
       "| num_of_layers | int | 9 | Number of convolutional layers |\n",
       "| features | int | 64 | Number of feature maps |\n",
       "| kernel_size | int | 3 | Size of the convolution kernel |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L70){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DnCNN\n",
       "\n",
       ">      DnCNN (spatial_dims=2, in_channels=1, out_channels=1, num_of_layers=9,\n",
       ">             features=64, kernel_size=3)\n",
       "\n",
       "*A Deep Neural Network for Image Denoising (DnCNN) model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| spatial_dims | int | 2 | Number of spatial dimensions |\n",
       "| in_channels | int | 1 | Number of input channels |\n",
       "| out_channels | int | 1 | Number of output channels |\n",
       "| num_of_layers | int | 9 | Number of convolutional layers |\n",
       "| features | int | 64 | Number of feature maps |\n",
       "| kernel_size | int | 3 | Size of the convolution kernel |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DnCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "x = torch_randn(16, 1, 32, 64)\n",
    "\n",
    "tst = DnCNN(2,1)\n",
    "test_eq(tst(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLab v3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L149){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### interpolate\n",
       "\n",
       ">      interpolate (x:torch.Tensor, size:Union[List[int],Tuple[int,...]],\n",
       ">                   dims:int)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| x | Tensor | Input tensor |\n",
       "| size | Union | Size of the output tensor |\n",
       "| dims | int | Number of spatial dimensions |\n",
       "| **Returns** | **Tensor** | **Output tensor** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L149){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### interpolate\n",
       "\n",
       ">      interpolate (x:torch.Tensor, size:Union[List[int],Tuple[int,...]],\n",
       ">                   dims:int)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| x | Tensor | Input tensor |\n",
       "| size | Union | Size of the output tensor |\n",
       "| dims | int | Number of spatial dimensions |\n",
       "| **Returns** | **Tensor** | **Output tensor** |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L144){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_padding\n",
       "\n",
       ">      get_padding (kernel_size:int, dilation:int)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kernel_size | int | Size of the convolution kernel |\n",
       "| dilation | int | Dilation rate |\n",
       "| **Returns** | **int** | **Padding size** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L144){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_padding\n",
       "\n",
       ">      get_padding (kernel_size:int, dilation:int)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kernel_size | int | Size of the convolution kernel |\n",
       "| dilation | int | Dilation rate |\n",
       "| **Returns** | **int** | **Padding size** |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(get_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L132){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeeplabConfig\n",
       "\n",
       ">      DeeplabConfig (dimensions:int, in_channels:int, out_channels:int,\n",
       ">                     backbone:str='xception', pretrained:bool=False,\n",
       ">                     middle_flow_blocks:int=16,\n",
       ">                     aspp_dilations:List[int]=<factory>,\n",
       ">                     entry_block3_stride:int=2, middle_block_dilation:int=1,\n",
       ">                     exit_block_dilations:Tuple[int,int]=(1, 2))"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L132){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeeplabConfig\n",
       "\n",
       ">      DeeplabConfig (dimensions:int, in_channels:int, out_channels:int,\n",
       ">                     backbone:str='xception', pretrained:bool=False,\n",
       ">                     middle_flow_blocks:int=16,\n",
       ">                     aspp_dilations:List[int]=<factory>,\n",
       ">                     entry_block3_stride:int=2, middle_block_dilation:int=1,\n",
       ">                     exit_block_dilations:Tuple[int,int]=(1, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DeeplabConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L201){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Block\n",
       "\n",
       ">      Block (config:__main__.DeeplabConfig, inplanes:int, planes:int, reps:int,\n",
       ">             stride:int=1, dilation:int=1, start_with_relu:bool=True,\n",
       ">             grow_first:bool=True, is_last:bool=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config | DeeplabConfig |  | Configuration for the Deeplab model |\n",
       "| inplanes | int |  | Number of input channels |\n",
       "| planes | int |  | Number of output channels |\n",
       "| reps | int |  | Number of convolutional layers |\n",
       "| stride | int | 1 | Stride for the convolution |\n",
       "| dilation | int | 1 | Dilation rate for the convolution |\n",
       "| start_with_relu | bool | True | If True, start with a ReLU activation |\n",
       "| grow_first | bool | True | If True, increase the number of channels in the first convolution |\n",
       "| is_last | bool | False | If True, add a convolution layer at the end |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L201){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Block\n",
       "\n",
       ">      Block (config:__main__.DeeplabConfig, inplanes:int, planes:int, reps:int,\n",
       ">             stride:int=1, dilation:int=1, start_with_relu:bool=True,\n",
       ">             grow_first:bool=True, is_last:bool=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config | DeeplabConfig |  | Configuration for the Deeplab model |\n",
       "| inplanes | int |  | Number of input channels |\n",
       "| planes | int |  | Number of output channels |\n",
       "| reps | int |  | Number of convolutional layers |\n",
       "| stride | int | 1 | Stride for the convolution |\n",
       "| dilation | int | 1 | Dilation rate for the convolution |\n",
       "| start_with_relu | bool | True | If True, start with a ReLU activation |\n",
       "| grow_first | bool | True | If True, increase the number of channels in the first convolution |\n",
       "| is_last | bool | False | If True, add a convolution layer at the end |"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L161){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SeparableConv\n",
       "\n",
       ">      SeparableConv (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n",
       ">                     kernel_size:int=3, stride:int=1, dilation:int=1,\n",
       ">                     bias:bool=False, norm:Optional[str]=None)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config | DeeplabConfig |  | Configuration for the Deeplab model |\n",
       "| inplanes | int |  | Number of input channels |\n",
       "| planes | int |  | Number of output channels |\n",
       "| kernel_size | int | 3 | Size of the convolution kernel |\n",
       "| stride | int | 1 | Stride for the convolution |\n",
       "| dilation | int | 1 | Dilation rate for the convolution |\n",
       "| bias | bool | False | If True, add a bias term |\n",
       "| norm | Optional | None | Type of normalization layer |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L161){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SeparableConv\n",
       "\n",
       ">      SeparableConv (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n",
       ">                     kernel_size:int=3, stride:int=1, dilation:int=1,\n",
       ">                     bias:bool=False, norm:Optional[str]=None)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config | DeeplabConfig |  | Configuration for the Deeplab model |\n",
       "| inplanes | int |  | Number of input channels |\n",
       "| planes | int |  | Number of output channels |\n",
       "| kernel_size | int | 3 | Size of the convolution kernel |\n",
       "| stride | int | 1 | Stride for the convolution |\n",
       "| dilation | int | 1 | Dilation rate for the convolution |\n",
       "| bias | bool | False | If True, add a bias term |\n",
       "| norm | Optional | None | Type of normalization layer |"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(SeparableConv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligned Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L257){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Xception\n",
       "\n",
       ">      Xception (config:__main__.DeeplabConfig)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| config | DeeplabConfig | Configuration for the Deeplab model |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L257){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Xception\n",
       "\n",
       ">      Xception (config:__main__.DeeplabConfig)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| config | DeeplabConfig | Configuration for the Deeplab model |"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Xception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L316){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ASPP_module\n",
       "\n",
       ">      ASPP_module (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n",
       ">                   dilation:int)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| config | DeeplabConfig | Configuration for the Deeplab model |\n",
       "| inplanes | int | Number of input channels |\n",
       "| planes | int | Number of output channels |\n",
       "| dilation | int | Dilation rate for the convolution |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L316){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ASPP_module\n",
       "\n",
       ">      ASPP_module (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n",
       ">                   dilation:int)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| config | DeeplabConfig | Configuration for the Deeplab model |\n",
       "| inplanes | int | Number of input channels |\n",
       "| planes | int | Number of output channels |\n",
       "| dilation | int | Dilation rate for the convolution |"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ASPP_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLab V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L335){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Deeplab\n",
       "\n",
       ">      Deeplab (config:__main__.DeeplabConfig)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L335){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Deeplab\n",
       "\n",
       ">      Deeplab (config:__main__.DeeplabConfig)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Deeplab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output channels: 512\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet backbone\n",
    "resnet_backbone = ResNetFeatures('resnet10', pretrained=False, in_channels=1, spatial_dims=3)\n",
    "\n",
    "# Forward pass through the backbone to get the output before the final classifier\n",
    "dummy_input = torch_randn(1, 1, 64, 224, 224)  # Example input size; adjust based on your needs\n",
    "output = resnet_backbone(dummy_input)\n",
    "\n",
    "# The shape of 'output' will give you the number of channels at this stage in the backbone\n",
    "print(\"Output channels:\", output[-1].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# For 2D images\n",
    "config_2d = DeeplabConfig(\n",
    "    dimensions=2,\n",
    "    in_channels=3,  # For RGB images\n",
    "    out_channels=4,\n",
    "    backbone=\"xception\",  # or whatever backbone you're using\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "model_2d = Deeplab(config_2d)\n",
    "\n",
    "# For 3D images\n",
    "config_3d = DeeplabConfig(\n",
    "    dimensions=3,\n",
    "    in_channels=1,  # For single-channel 3D medical images\n",
    "    out_channels=4,\n",
    "    middle_flow_blocks=16,\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "model_3d = Deeplab(config_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from torch import no_grad as torch_no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def test_deeplab(config, input_shape, expected_output_shape):\n",
    "    set_determinism(0)  # For reproducibility\n",
    "    \n",
    "    model = Deeplab(config)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Generate random input tensor\n",
    "    x = torch_randn(*input_shape)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch_no_grad():\n",
    "        output = model(x)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == expected_output_shape, f\"Expected shape {expected_output_shape}, but got {output.shape}\"\n",
    "    \n",
    "    print(f\"Test passed for {config.dimensions}D model with backbone {config.backbone}\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for 2D model with backbone xception\n",
      "Input shape: (1, 3, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64])\n",
      "---\n",
      "Test passed for 2D model with backbone resnet50\n",
      "Input shape: (1, 3, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64])\n",
      "---\n",
      "Test passed for 3D model with backbone xception\n",
      "Input shape: (1, 1, 64, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64, 64])\n",
      "---\n",
      "Test passed for 3D model with backbone resnet10\n",
      "Input shape: (1, 1, 64, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64, 64])\n",
      "---\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test 2D model\n",
    "config_2d = DeeplabConfig(\n",
    "    dimensions=2,\n",
    "    in_channels=3,\n",
    "    out_channels=4,\n",
    "    backbone=\"xception\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_2d, (1, 3, 64, 64), (1, 4, 64, 64))\n",
    "\n",
    "# Test 2D model with ResNet50 backbone\n",
    "config_2d_resnet = DeeplabConfig(\n",
    "    dimensions=2,\n",
    "    in_channels=3,\n",
    "    out_channels=4,\n",
    "    backbone=\"resnet50\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_2d_resnet, (1, 3, 64, 64), (1, 4, 64, 64))\n",
    "\n",
    "# Test 3D model\n",
    "config_3d = DeeplabConfig(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    backbone=\"xception\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_3d, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n",
    "\n",
    "# Test 3D model with ResNet10 backbone\n",
    "config_3d_resnet = DeeplabConfig(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    backbone=\"resnet10\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_3d_resnet, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n",
    "\n",
    "print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L420){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MambaLayer\n",
       "\n",
       ">      MambaLayer (dim, d_state=16, d_conv=4, expand=2)\n",
       "\n",
       "*A custom neural network layer that incorporates the Mamba block from the Mamba model, \n",
       "along with layer normalization and optional mixed precision handling.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dim |  |  | Dimension of the input tensor |\n",
       "| d_state | int | 16 | Expansion factor for the state in the Mamba block |\n",
       "| d_conv | int | 4 | Width of the local convolution in the Mamba block |\n",
       "| expand | int | 2 | Factor by which to expand the dimensions in the Mamba block |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L420){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MambaLayer\n",
       "\n",
       ">      MambaLayer (dim, d_state=16, d_conv=4, expand=2)\n",
       "\n",
       "*A custom neural network layer that incorporates the Mamba block from the Mamba model, \n",
       "along with layer normalization and optional mixed precision handling.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dim |  |  | Dimension of the input tensor |\n",
       "| d_state | int | 16 | Expansion factor for the state in the Mamba block |\n",
       "| d_conv | int | 4 | Width of the local convolution in the Mamba block |\n",
       "| expand | int | 2 | Factor by which to expand the dimensions in the Mamba block |"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MambaLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L471){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### UMamba\n",
       "\n",
       ">      UMamba (spatial_dims:int, in_channels:int, out_channels:int,\n",
       ">              kernel_size:Sequence[Union[Sequence[int],int]],\n",
       ">              strides:Sequence[Union[Sequence[int],int]],\n",
       ">              upsample_kernel_size:Sequence[Union[Sequence[int],int]],\n",
       ">              filters:Optional[Sequence[int]]=None,\n",
       ">              dropout:Union[Tuple,str,float,NoneType]=None,\n",
       ">              norm_name:Union[Tuple,str]=('INSTANCE', {'affine': True}),\n",
       ">              act_name:Union[Tuple,str]=('leakyrelu', {'inplace': True,\n",
       ">              'negative_slope': 0.01}), deep_supervision:bool=False,\n",
       ">              deep_supr_num:int=1, res_block:bool=False, trans_bias:bool=False)\n",
       "\n",
       "*A custom subclass of DynUNet that integrates the Mamba layer into the model's bottleneck.\n",
       "\n",
       "This class inherits from `DynUNet` and adds a specific bottleneck structure containing a convolution block followed by a MambaLayer.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/deepCLEM/bioMONAI/blob/main/bioMONAI/nets.py#L471){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### UMamba\n",
       "\n",
       ">      UMamba (spatial_dims:int, in_channels:int, out_channels:int,\n",
       ">              kernel_size:Sequence[Union[Sequence[int],int]],\n",
       ">              strides:Sequence[Union[Sequence[int],int]],\n",
       ">              upsample_kernel_size:Sequence[Union[Sequence[int],int]],\n",
       ">              filters:Optional[Sequence[int]]=None,\n",
       ">              dropout:Union[Tuple,str,float,NoneType]=None,\n",
       ">              norm_name:Union[Tuple,str]=('INSTANCE', {'affine': True}),\n",
       ">              act_name:Union[Tuple,str]=('leakyrelu', {'inplace': True,\n",
       ">              'negative_slope': 0.01}), deep_supervision:bool=False,\n",
       ">              deep_supr_num:int=1, res_block:bool=False, trans_bias:bool=False)\n",
       "\n",
       "*A custom subclass of DynUNet that integrates the Mamba layer into the model's bottleneck.\n",
       "\n",
       "This class inherits from `DynUNet` and adds a specific bottleneck structure containing a convolution block followed by a MambaLayer.*"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(UMamba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch_randn(16, 1, 32, 64)\n",
    "\n",
    "tst = DynUNet(2,1,1,[3,3,3],[1,1,1],[1,1])\n",
    "print(tst(x).shape)\n",
    "test_eq(tst(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch_randn(16, 1, 32, 64).cuda()\n",
    "\n",
    "tst = UMamba(2,1,1,[3,3,3],[1,1,1],[1,1]).cuda()\n",
    "print(tst(x).shape)\n",
    "test_eq(tst(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
