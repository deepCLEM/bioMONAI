{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks\n",
    "\n",
    "> Neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# from IPython.display import clear_output, DisplayHandle\n",
    "\n",
    "\n",
    "# def update_patch(self, obj):\n",
    "#     clear_output(wait=True)\n",
    "#     self.display(obj)\n",
    "\n",
    "\n",
    "# DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastai.vision.all import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "from torch import cat as torch_cat\n",
    "from torch import Tensor as torch_Tensor, randn as torch_randn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import float16 as torch_float16, float32 as torch_float32\n",
    "\n",
    "from fastai.vision.all import create_unet_model, resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "from monai.networks.blocks import Convolution\n",
    "from monai.networks.layers.factories import Act, Norm, Pool\n",
    "from monai.utils import set_determinism\n",
    "from monai.networks.nets import BasicUNet, AttentionUnet, DynUNet, UNet, BasicUNet, ResNet, ResNetFeatures\n",
    "\n",
    "from bioMONAI.core import get_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_custom_unet(resnet_version,      # Choose a ResNet model between: 'resnet18', 'resnet34', 'resnet50', 'resnet101', and 'resnet152'.\n",
    "                       n_in=1,              # Number of input channels, default is 1 (e.g., grayscale).\n",
    "                       n_out=1,               # Number of output channels.\n",
    "                       img_size=(128, 128), # Tuple for the input image size, default is (128, 128).\n",
    "                       pretrained=True,     # If True, use a pretrained ResNet backbone.\n",
    "                       cut=4,               # The cut point for the ResNet model, default is 4.\n",
    "                       ):\n",
    "    \"\"\"\n",
    "    Create a U-Net model with a ResNet backbone.\n",
    "\n",
    "    Returns:\n",
    "    - U-Net model with the specified ResNet backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to map model names to Fastai ResNet functions\n",
    "    resnet_versions = {\n",
    "        'resnet18': resnet18,\n",
    "        'resnet34': resnet34,\n",
    "        'resnet50': resnet50,\n",
    "        'resnet101': resnet101,\n",
    "        'resnet152': resnet152\n",
    "    }\n",
    "    \n",
    "    # Select the chosen ResNet model\n",
    "    if resnet_version not in resnet_versions:\n",
    "        raise ValueError(f\"Invalid resnet_version '{resnet_version}'. Choose from: {list(resnet_versions.keys())}\")\n",
    "    \n",
    "    resnet_fn = resnet_versions[resnet_version]\n",
    "\n",
    "    # Create and return the U-Net model\n",
    "    return create_unet_model(resnet_fn, n_out, img_size, pretrained, n_in=n_in, cut=cut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "\n",
    "class DnCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Deep Neural Network for Image Denoising (DnCNN) model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, spatial_dims=2, # Number of spatial dimensions\n",
    "                 in_channels=1, # Number of input channels\n",
    "                 out_channels=1, # Number of output channels\n",
    "                 num_of_layers=9, # Number of convolutional layers\n",
    "                 features=64, # Number of feature maps\n",
    "                 kernel_size=3, # Size of the convolution kernel\n",
    "                 ):\n",
    "        super(DnCNN, self).__init__()\n",
    "        \n",
    "        # Create a list to hold the layers of the network\n",
    "        layers = []\n",
    "        \n",
    "        # Append the first convolution layer with specific parameters\n",
    "        layers.append(Convolution(spatial_dims=spatial_dims,\n",
    "                                  in_channels=in_channels,\n",
    "                                  out_channels=features,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  bias=False,\n",
    "                                  norm=None))\n",
    "        \n",
    "        # Append the remaining convolution layers with default parameters\n",
    "        for _ in range(num_of_layers-2):\n",
    "            layers.append(Convolution(spatial_dims=spatial_dims,\n",
    "                                      in_channels=features,\n",
    "                                      out_channels=features,\n",
    "                                      kernel_size=kernel_size))\n",
    "        \n",
    "        # Append the final convolution layer with specific parameters\n",
    "        layers.append(Convolution(spatial_dims=spatial_dims,\n",
    "                                  in_channels=features,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  bias=False,\n",
    "                                  norm=None))\n",
    "        \n",
    "        # Convert the list of layers into a sequential container\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, # Input image tensor with shape [batch_size, in_channels, height, width].\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Forward pass of the DnCNN model.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The denoised output image tensor with shape [batch_size, out_channels, height, width].\n",
    "        \"\"\"\n",
    "        # Compute the residual by passing the input through the network\n",
    "        residual = self.dncnn(x)\n",
    "        \n",
    "        # Subtract the residual from the original input to get the denoised output\n",
    "        denoised = x - residual\n",
    "        \n",
    "        return denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch_randn(16, 1, 32, 64)\n",
    "\n",
    "tst = DnCNN(2,1)\n",
    "test_eq(tst(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLab v3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DeepLabv3` is a semantic segmentation architecture that handles the problem of segmenting objects at multiple scales. It uses the Atroys Spatial Pyramid Pooling module, and introduces various updates with respect to other versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DeeplabConfig` class has been created to centralize all settings and hyperparameters in one place. It uses two main functions: Get_padding and interpolate. Get_padding is a function that calculates the amount of padding needed for a convolution operation to get the desired output size. Interpolate is a function that allows the resizing of a tensor using interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DeeplabConfig:\n",
    "    dimensions: int # Number of spatial dimensions\n",
    "    in_channels: int # Number of input channels\n",
    "    out_channels: int # Number of output channels\n",
    "    backbone: str = \"xception\" # Backbone model for the encoder\n",
    "    pretrained: bool = False # If True, use a pretrained backbone\n",
    "    middle_flow_blocks: int = 16 # Number of middle flow blocks\n",
    "    aspp_dilations: List[int] = field(default_factory=lambda: [1, 6, 12, 18]) # Dilation rates for the ASPP module\n",
    "    entry_block3_stride: int = 2 # Stride for the third entry block\n",
    "    middle_block_dilation: int = 1 # Dilation rate for the middle block\n",
    "    exit_block_dilations: Tuple[int, int] = (1, 2) # Dilation rates for the exit block\n",
    "\n",
    "def get_padding(kernel_size: int, # Size of the convolution kernel\n",
    "                dilation: int, # Dilation rate\n",
    "                ) -> int: # Padding size\n",
    "    return (kernel_size - 1) * dilation // 2\n",
    "    \n",
    "def interpolate(x: torch_Tensor, # Input tensor\n",
    "                size: Union[List[int], Tuple[int, ...]], # Size of the output tensor\n",
    "                dims: int, # Number of spatial dimensions\n",
    "                ) -> torch_Tensor: # Output tensor\n",
    "    if dims == 2:\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=True)\n",
    "    elif dims == 3:\n",
    "        return F.interpolate(x, size=size, mode='trilinear', align_corners=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported number of dimensions: {dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bmandracchia/bioMONAI/blob/main/bioMONAI/nets.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeeplabConfig\n",
       "\n",
       ">      DeeplabConfig (dimensions:int, in_channels:int, out_channels:int,\n",
       ">                     backbone:str='xception', pretrained:bool=False,\n",
       ">                     middle_flow_blocks:int=16,\n",
       ">                     aspp_dilations:List[int]=<factory>,\n",
       ">                     entry_block3_stride:int=2, middle_block_dilation:int=1,\n",
       ">                     exit_block_dilations:Tuple[int,int]=(1, 2))"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bmandracchia/bioMONAI/blob/main/bioMONAI/nets.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DeeplabConfig\n",
       "\n",
       ">      DeeplabConfig (dimensions:int, in_channels:int, out_channels:int,\n",
       ">                     backbone:str='xception', pretrained:bool=False,\n",
       ">                     middle_flow_blocks:int=16,\n",
       ">                     aspp_dilations:List[int]=<factory>,\n",
       ">                     entry_block3_stride:int=2, middle_block_dilation:int=1,\n",
       ">                     exit_block_dilations:Tuple[int,int]=(1, 2))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DeeplabConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SeparableConv` is a class that carries out a type of convolution operation that splits the traditional convolution into two parts: depthwise convolution (a convolution filter for each channel independently), and pointwise convolution (combines the outputs of the depthwise convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeparableConv(nn.Module):\n",
    "    def __init__(self, config: DeeplabConfig, # Configuration for the Deeplab model\n",
    "                 inplanes: int, # Number of input channels\n",
    "                 planes: int, # Number of output channels\n",
    "                 kernel_size: int = 3, # Size of the convolution kernel\n",
    "                 stride: int = 1,  # Stride for the convolution\n",
    "                 dilation: int = 1, # Dilation rate for the convolution\n",
    "                 bias: bool = False, # If True, add a bias term\n",
    "                 norm: Optional[str] = None, # Type of normalization layer\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Convolution(\n",
    "            spatial_dims=config.dimensions, \n",
    "            in_channels=inplanes, \n",
    "            out_channels=inplanes, \n",
    "            kernel_size=kernel_size,\n",
    "            groups=inplanes, \n",
    "            padding=get_padding(kernel_size, dilation), \n",
    "            dilation=dilation, \n",
    "            bias=bias, \n",
    "            strides=stride\n",
    "        )\n",
    "        self.pointwise = Convolution(\n",
    "            spatial_dims=config.dimensions, \n",
    "            in_channels=inplanes, \n",
    "            out_channels=planes, \n",
    "            kernel_size=1, \n",
    "            strides=1,\n",
    "            padding=0, \n",
    "            dilation=1, \n",
    "            groups=1, \n",
    "            bias=bias,\n",
    "            norm=Norm.BATCH if norm else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch_Tensor) -> torch_Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Block` is a class that combines multiple separable convolutional layers and residual connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: DeeplabConfig, # Configuration for the Deeplab model\n",
    "                 inplanes: int, # Number of input channels\n",
    "                 planes: int, # Number of output channels\n",
    "                 reps: int, # Number of convolutional layers\n",
    "                 stride: int = 1,  # Stride for the convolution\n",
    "                 dilation: int = 1, # Dilation rate for the convolution\n",
    "                 start_with_relu: bool = True, # If True, start with a ReLU activation\n",
    "                 grow_first: bool = True, # If True, increase the number of channels in the first convolution\n",
    "                 is_last: bool = False, # If True, add a convolution layer at the end\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if planes != inplanes or stride != 1:\n",
    "            self.skip = Convolution(config.dimensions, inplanes, planes, kernel_size=1, bias=False, \n",
    "                                    strides=stride, norm=Norm.BATCH)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        filters = inplanes\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv(config, inplanes, planes, 3, stride=1, dilation=dilation, norm=Norm.BATCH))\n",
    "            filters = planes\n",
    "\n",
    "        for _ in range(reps - 1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv(config, filters, filters, 3, stride=1, dilation=dilation, norm=Norm.BATCH))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv(config, inplanes, planes, 3, stride=1, dilation=dilation, norm=Norm.BATCH))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "\n",
    "        if stride != 1:\n",
    "            rep.append(SeparableConv(config, planes, planes, 3, stride=2))\n",
    "\n",
    "        if stride == 1 and is_last:\n",
    "            rep.append(SeparableConv(config, planes, planes, 3, stride=1))\n",
    "\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, inp: torch_Tensor) -> torch_Tensor:\n",
    "        x = self.rep(inp)\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "        else:\n",
    "            skip = inp\n",
    "        x += skip\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligned Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Xception` class defines the Xception Neural Network used in DeepLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Xception(nn.Module):\n",
    "    def __init__(self, config: DeeplabConfig, # Configuration for the Deeplab model\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = Convolution(config.dimensions, config.in_channels, 32, kernel_size=3,\n",
    "                                 bias=False, strides=2, padding=1, norm=Norm.BATCH)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = Convolution(config.dimensions, 32, 64, kernel_size=3,\n",
    "                                 bias=False, strides=1, padding=1, norm=Norm.BATCH)\n",
    "\n",
    "        self.block1 = Block(config, 64, 128, reps=2, stride=2, start_with_relu=False)\n",
    "        self.block2 = Block(config, 128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(config, 256, 728, reps=2, stride=config.entry_block3_stride, \n",
    "                            start_with_relu=True, grow_first=True, is_last=True)\n",
    "\n",
    "        # Middle flow\n",
    "        self.middle_flow = nn.Sequential(*[\n",
    "            Block(config, 728, 728, reps=3, stride=1, dilation=config.middle_block_dilation,\n",
    "                  start_with_relu=True, grow_first=True)\n",
    "            for _ in range(config.middle_flow_blocks)\n",
    "        ])\n",
    "\n",
    "        # Exit flow\n",
    "        self.exit_block = Block(config, 728, 1024, reps=2, stride=1, dilation=config.exit_block_dilations[0],\n",
    "                             start_with_relu=True, grow_first=False, is_last=True)\n",
    "\n",
    "        self.conv3 = SeparableConv(config, 1024, 1536, 3, stride=1, dilation=config.exit_block_dilations[1], norm=Norm.BATCH)\n",
    "        self.conv4 = SeparableConv(config, 1536, 1536, 3, stride=1, dilation=config.exit_block_dilations[1], norm=Norm.BATCH)\n",
    "        self.conv5 = SeparableConv(config, 1536, 2048, 3, stride=1, dilation=config.exit_block_dilations[1], norm=Norm.BATCH)\n",
    "\n",
    "    def forward(self, x: torch_Tensor) -> Tuple[torch_Tensor, torch_Tensor]:\n",
    "        # Entry flow\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        low_level_feat = x\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        # Middle flow\n",
    "        x = self.middle_flow(x)\n",
    "\n",
    "        # Exit flow\n",
    "        x = self.exit_block(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x, low_level_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `ASPP_module` to compute the Atroys Spatial Pyramid Pooling method and create the convolution that uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ASPP_module(nn.Module):\n",
    "    def __init__(self, config: DeeplabConfig, # Configuration for the Deeplab model\n",
    "                 inplanes: int,               # Number of input channels\n",
    "                 planes: int,                 # Number of output channels\n",
    "                 dilation: int,               # Dilation rate for the convolution\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        kernel_size = 1 if dilation == 1 else 3\n",
    "        padding = 0 if dilation == 1 else dilation\n",
    "        self.atrous_convolution = Convolution(config.dimensions, inplanes, planes, kernel_size=kernel_size,\n",
    "                                              strides=1, padding=padding, dilation=dilation, \n",
    "                                              bias=False, norm=Norm.BATCH)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch_Tensor) -> torch_Tensor:\n",
    "        x = self.atrous_convolution(x)\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLab V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Deeplab` class combines the different modules to make the DeepLab V3 architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Deeplab(nn.Module):\n",
    "    def __init__(self, config: DeeplabConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Choose backbone based on configuration\n",
    "        if config.backbone == \"xception\":\n",
    "            self.backbone = Xception(config)\n",
    "            backbone_out_channels = 2048\n",
    "            low_level_channels = 128  # This might need adjustment based on Xception implementation\n",
    "        else:\n",
    "            usePretrained = config.pretrained if config.in_channels==1 and config.dimensions==3 else False\n",
    "            self.backbone = ResNetFeatures(config.backbone, pretrained=usePretrained, spatial_dims=config.dimensions, in_channels=config.in_channels)\n",
    "            del self.backbone.fc\n",
    "            del self.backbone.avgpool\n",
    "            \n",
    "            # Forward pass through the backbone to get the output before the final classifier\n",
    "            if config.dimensions == 3:\n",
    "                dummy_input = torch_randn(1, config.in_channels, 16, 128, 128) \n",
    "            elif config.dimensions == 2:\n",
    "                dummy_input = torch_randn(1, config.in_channels, 128, 128) \n",
    "            dummy_output = self.backbone(dummy_input)\n",
    "            \n",
    "            # print(\"Output channels:\", dummy_output[-1].shape[1])\n",
    "            \n",
    "            backbone_out_channels = dummy_output[-1].shape[1]\n",
    "            low_level_channels = dummy_output[1].shape[1]  # first layer output\n",
    "        \n",
    "        # else:\n",
    "        #     raise ValueError(f\"Unsupported backbone: {config.backbone}\")\n",
    "\n",
    "        # ASPP\n",
    "        self.aspp_modules = nn.ModuleList([\n",
    "            ASPP_module(config, backbone_out_channels, 256, dilation=dilation)\n",
    "            for dilation in config.aspp_dilations\n",
    "        ])\n",
    "\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            Pool[Pool.ADAPTIVEAVG, config.dimensions](1),\n",
    "            Convolution(config.dimensions, backbone_out_channels, 256, kernel_size=1, strides=1, bias=False, norm=Norm.BATCH),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = Convolution(config.dimensions, 1280, 256, kernel_size=1, bias=False, norm=Norm.BATCH)\n",
    "        self.conv2 = Convolution(config.dimensions, low_level_channels, 48, kernel_size=1, bias=False, norm=Norm.BATCH)\n",
    "\n",
    "        self.last_conv = nn.Sequential(\n",
    "            Convolution(config.dimensions, 304, 256, kernel_size=3, strides=1, padding=1, bias=False, norm=Norm.BATCH),\n",
    "            nn.ReLU(),\n",
    "            Convolution(config.dimensions, 256, 256, kernel_size=3, strides=1, padding=1, bias=False, norm=Norm.BATCH),\n",
    "            nn.ReLU(),\n",
    "            Convolution(config.dimensions, 256, config.out_channels, kernel_size=1, strides=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch_Tensor) -> torch_Tensor:\n",
    "        if self.config.backbone == \"xception\":\n",
    "            x, low_level_features = self.backbone(input)\n",
    "        else:\n",
    "            x = self.backbone.conv1(input)\n",
    "            x = self.backbone.bn1(x)\n",
    "            x = self.backbone.act(x)\n",
    "            x = self.backbone.maxpool(x)\n",
    "\n",
    "            low_level_features = self.backbone.layer1(x)\n",
    "            x = self.backbone.layer2(low_level_features)\n",
    "            x = self.backbone.layer3(x)\n",
    "            x = self.backbone.layer4(x)\n",
    "\n",
    "        aspp_results = [module(x) for module in self.aspp_modules]\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = interpolate(x5, size=x.shape[2:], dims=self.config.dimensions)\n",
    "        x = torch_cat(aspp_results + [x5], dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = interpolate(x, size=low_level_features.shape[2:], dims=self.config.dimensions)\n",
    "\n",
    "        low_level_features = self.conv2(low_level_features)\n",
    "\n",
    "        x = torch_cat((x, low_level_features), dim=1)\n",
    "        x = self.last_conv(x)\n",
    "        x = interpolate(x, size=input.shape[2:], dims=self.config.dimensions)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output channels: 512\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet backbone\n",
    "resnet_backbone = ResNetFeatures('resnet10', pretrained=False, in_channels=1, spatial_dims=3)\n",
    "\n",
    "# Forward pass through the backbone to get the output before the final classifier\n",
    "dummy_input = torch_randn(1, 1, 64, 224, 224)  # Example input size; adjust based on your needs\n",
    "output = resnet_backbone(dummy_input)\n",
    "\n",
    "# The shape of 'output' will give you the number of channels at this stage in the backbone\n",
    "print(\"Output channels:\", output[-1].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2D images\n",
    "config_2d = DeeplabConfig(\n",
    "    dimensions=2,\n",
    "    in_channels=3,  # For RGB images\n",
    "    out_channels=4,\n",
    "    backbone=\"xception\",  # or whatever backbone you're using\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "model_2d = Deeplab(config_2d)\n",
    "\n",
    "# For 3D images\n",
    "config_3d = DeeplabConfig(\n",
    "    dimensions=3,\n",
    "    in_channels=1,  # For single-channel 3D medical images\n",
    "    out_channels=4,\n",
    "    middle_flow_blocks=16,\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "model_3d = Deeplab(config_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad as torch_no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deeplab(config, input_shape, expected_output_shape):\n",
    "    set_determinism(0)  # For reproducibility\n",
    "    \n",
    "    model = Deeplab(config)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Generate random input tensor\n",
    "    x = torch_randn(*input_shape)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch_no_grad():\n",
    "        output = model(x)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == expected_output_shape, f\"Expected shape {expected_output_shape}, but got {output.shape}\"\n",
    "    \n",
    "    print(f\"Test passed for {config.dimensions}D model with backbone {config.backbone}\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for 2D model with backbone xception\n",
      "Input shape: (1, 3, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64])\n",
      "---\n",
      "Test passed for 2D model with backbone resnet50\n",
      "Input shape: (1, 3, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64])\n",
      "---\n",
      "Test passed for 3D model with backbone xception\n",
      "Input shape: (1, 1, 64, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64, 64])\n",
      "---\n",
      "Test passed for 3D model with backbone resnet10\n",
      "Input shape: (1, 1, 64, 64, 64)\n",
      "Output shape: torch.Size([1, 4, 64, 64, 64])\n",
      "---\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test 2D model\n",
    "config_2d = DeeplabConfig(\n",
    "    dimensions=2,\n",
    "    in_channels=3,\n",
    "    out_channels=4,\n",
    "    backbone=\"xception\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_2d, (1, 3, 64, 64), (1, 4, 64, 64))\n",
    "\n",
    "# Test 2D model with ResNet50 backbone\n",
    "config_2d_resnet = DeeplabConfig(\n",
    "    dimensions=2,\n",
    "    in_channels=3,\n",
    "    out_channels=4,\n",
    "    backbone=\"resnet50\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_2d_resnet, (1, 3, 64, 64), (1, 4, 64, 64))\n",
    "\n",
    "# Test 3D model\n",
    "config_3d = DeeplabConfig(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    backbone=\"xception\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_3d, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n",
    "\n",
    "# Test 3D model with ResNet10 backbone\n",
    "config_3d_resnet = DeeplabConfig(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    backbone=\"resnet10\",\n",
    "    aspp_dilations=[1, 6, 12, 18]\n",
    ")\n",
    "test_deeplab(config_3d_resnet, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n",
    "\n",
    "print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MambaLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom neural network layer that incorporates the Mamba block from the Mamba model, \n",
    "    along with layer normalization and optional mixed precision handling.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim,  # Dimension of the input tensor\n",
    "                 d_state=16, # Expansion factor for the state in the Mamba block\n",
    "                 d_conv=4,   # Width of the local convolution in the Mamba block\n",
    "                 expand=2,   # Factor by which to expand the dimensions in the Mamba block\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dim = dim                      # Dimension of the input tensor\n",
    "        self.norm = nn.LayerNorm(dim)       # Layer normalization\n",
    "        self.mamba = Mamba(                 # Mamba block\n",
    "            d_model=dim,  # Model dimension d_model\n",
    "            d_state=d_state,  # SSM state expansion factor\n",
    "            d_conv=d_conv,  # Local convolution width\n",
    "            expand=expand  # Block expansion factor\n",
    "        )\n",
    "    \n",
    "    @autocast(enabled=False)\n",
    "    def forward(self, x, # Input tensor of shape [batch_size, dim, height, width].\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Forward pass of the MambaLayer. Applies layer normalization and optionally converts input precision.\n",
    "   \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying Mamba block and normalization.\n",
    "        \"\"\"\n",
    "        if x.dtype == torch_float16:\n",
    "            x = x.type(torch_float32)  # Convert input to float32 for mixed precision handling\n",
    "        \n",
    "        B, C = x.shape[:2]\n",
    "        assert C == self.dim  # Ensure the feature size matches the dimension of the layer\n",
    "        \n",
    "        n_tokens = x.shape[2:].numel()\n",
    "        img_dims = x.shape[2:]\n",
    "        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)  # Flatten and transpose for Mamba input\n",
    "        \n",
    "        x_norm = self.norm(x_flat)  # Apply layer normalization\n",
    "        \n",
    "        x_mamba = self.mamba(x_norm)  # Pass through the Mamba block\n",
    "        \n",
    "        out = x_mamba.transpose(-1, -2).reshape(B, C, *img_dims)  # Reshape and transpose back to original dimensions\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UMamba(DynUNet):\n",
    "    \"\"\"\n",
    "    A custom subclass of DynUNet that integrates the Mamba layer into the model's bottleneck.\n",
    "    \n",
    "    This class inherits from `DynUNet` and adds a specific bottleneck structure containing a convolution block followed by a MambaLayer.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def get_bottleneck(self):\n",
    "        \"\"\"\n",
    "        Constructs the bottleneck part of the network.\n",
    "        \n",
    "        The bottleneck consists of a convolution block followed by a MambaLayer. Both components are added to a sequential container.\n",
    "        \n",
    "        Returns:\n",
    "            nn.Sequential: A PyTorch sequential container with the convolution block and MambaLayer for the bottleneck.\n",
    "        \"\"\"\n",
    "        mamba_bottleneck = []\n",
    "        # Add a convolution block before the MambaLayer in the bottleneck\n",
    "        mamba_bottleneck.append(\n",
    "            self.conv_block(\n",
    "                self.spatial_dims,          # Spatial dimensions of the input data\n",
    "                self.filters[-2],           # Number of filters for the previous layer\n",
    "                self.filters[-1],           # Number of filters for this layer (output)\n",
    "                self.kernel_size[-1],       # Kernel size for the convolution\n",
    "                self.strides[-1],           # Stride for the convolution\n",
    "                self.norm_name,             # Normalization method name\n",
    "                self.act_name,              # Activation function name\n",
    "                dropout=self.dropout        # Dropout probability\n",
    "            )\n",
    "        )\n",
    "        # Add the MambaLayer to the bottleneck\n",
    "        mamba_bottleneck.append(\n",
    "            MambaLayer(dim = self.filters[-1])  # Initialize the MambaLayer with the current filter size as dimension\n",
    "        )\n",
    "        return nn.Sequential(*mamba_bottleneck)  # Return the sequential container holding both components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch_randn(16, 1, 32, 64)\n",
    "\n",
    "tst = DynUNet(2,1,1,[3,3,3],[1,1,1],[1,1])\n",
    "print(tst(x).shape)\n",
    "test_eq(tst(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch_randn(16, 1, 32, 64).cuda()\n",
    "\n",
    "tst = UMamba(2,1,1,[3,3,3],[1,1,1],[1,1]).cuda()\n",
    "print(tst(x).shape)\n",
    "test_eq(tst(x).shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
