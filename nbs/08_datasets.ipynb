{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> Download and store datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from pooch import create as pooch_create, retrieve as pooch_retrieve, Decompress, Unzip, Untar\n",
    "from sklearn.model_selection import train_test_split\n",
    "import quilt3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "\n",
    "import medmnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedMNIST Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_medmnist(dataset: str, # The name of the MedMNIST dataset (e.g., 'pathmnist', 'bloodmnist', etc.).\n",
    "                      output_dir: str = '.', # The path to the directory where the datasets will be saved.\n",
    "                      download_only: bool = False, # If True, only download the dataset into the output directory without processing.\n",
    "                      save_images: bool = True, # If True, save the images into the output directory as .png (2D datasets) or multipage .tiff (3D datasets) files.\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Downloads the specified MedMNIST dataset and saves the training, validation, and test datasets \n",
    "    into the specified output directory. Images are saved as .png for 2D data and multi-page .tiff for 3D data,\n",
    "    organized into folders named after their labels.\n",
    "\n",
    "    Returns: None, saves images in the specified output directory if save_images is True.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the dataset is available in the MedMNIST information dictionary\n",
    "    if dataset not in medmnist.INFO:\n",
    "        raise ValueError(f\"The dataset '{dataset}' is not available. Please select from the available datasets.\")\n",
    "\n",
    "    # Retrieve dataset information\n",
    "    info = medmnist.INFO[dataset]\n",
    "\n",
    "    # Get the appropriate dataset class from MedMNIST\n",
    "    dataset_class = getattr(medmnist, info['python_class'])\n",
    "    \n",
    "    # Check if the dataset has already been downloaded and processed\n",
    "    dataset_path = os.path.join(output_dir, dataset)\n",
    "    if os.path.exists(dataset_path) and len(os.listdir(dataset_path)) > 0:\n",
    "        print(f\"Dataset '{dataset}' is already downloaded and available in '{dataset_path}'.\")\n",
    "        if download_only:\n",
    "            return info\n",
    "        elif save_images:\n",
    "            print(\"Skipping download and image saving, as data already exists.\")\n",
    "            return None\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "\n",
    "    # Download the datasets\n",
    "    train_dataset = dataset_class(split='train', download=True, root=dataset_path)\n",
    "    val_dataset = dataset_class(split='val', download=True, root=dataset_path)\n",
    "    test_dataset = dataset_class(split='test', download=True, root=dataset_path)\n",
    "\n",
    "    # Save the images into directories by their label\n",
    "    def save_images(dataset, split):\n",
    "        \"\"\"Helper function to save images and labels into directories.\"\"\"\n",
    "        split_dir = os.path.join(dataset_path, split)\n",
    "        if not os.path.exists(split_dir):\n",
    "            os.makedirs(split_dir)\n",
    "\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            img, label = dataset[i]\n",
    "            label_dir = os.path.join(split_dir, str(label).replace(\"[\", \"\").replace(\"]\", \"\"))  # Remove parentheses\n",
    "            if not os.path.exists(label_dir):\n",
    "                os.makedirs(label_dir)\n",
    "\n",
    "            # Save 2D images as .png\n",
    "            if info['n_channels'] == 1:  \n",
    "                if img.ndim == 3:   # Check if it's 2D (single-channel)\n",
    "                    img_path = os.path.join(label_dir, f'{split}_{i}.png')\n",
    "                    img = Image.fromarray(img.squeeze(), mode='L')  # 'L' mode for grayscale\n",
    "                    img.save(img_path)\n",
    "                elif img.ndim == 4:\n",
    "                    img_path = os.path.join(label_dir, f'{split}_{i}.tiff')\n",
    "                    tiff.imwrite(img_path, img)\n",
    "            elif info['n_channels'] == 3:  # Check if it's RGB\n",
    "                img_path = os.path.join(label_dir, f'{split}_{i}.png')\n",
    "                img.save(img_path)\n",
    "            # Save 3D images as multi-page .tiff\n",
    "            else:\n",
    "                img_path = os.path.join(label_dir, f'{split}_{i}.tiff')\n",
    "                tiff.imwrite(img_path, img)\n",
    "\n",
    "    # Save training, validation, and test data if save_images is True\n",
    "    if save_images:\n",
    "        print(f\"Saving training images to {dataset_path}...\")\n",
    "        save_images(train_dataset, 'train')\n",
    "\n",
    "        print(f\"Saving validation images to {dataset_path}...\")\n",
    "        save_images(val_dataset, 'val')\n",
    "\n",
    "        print(f\"Saving test images to {dataset_path}...\")\n",
    "        save_images(test_dataset, 'test')\n",
    "        \n",
    "        # Clean up: remove .npz files if present\n",
    "        for file in os.listdir(dataset_path):\n",
    "            if file.endswith('.npz'):\n",
    "                os.remove(os.path.join(dataset_path, file))\n",
    "                print(f\"Removed {file}\")\n",
    "\n",
    "    # If download_only is True, skip returning the dataset objects and just download the files\n",
    "    if download_only:\n",
    "        print(f\"Datasets downloaded to {dataset_path}\")\n",
    "        print(f\"Dataset info for '{dataset}': {info}\")\n",
    "        return info\n",
    "\n",
    "    # Return the datasets if download_only is False and save_images is False\n",
    "    return train_dataset, val_dataset, test_dataset if not save_images else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def medmnist2df(train_dataset,     # MedMNIST training dataset with images and labels\n",
    "                val_dataset=None,  # (Optional) MedMNIST validation dataset with images and labels\n",
    "                test_dataset=None, # (Optional) MedMNIST test dataset with images and labels\n",
    "                mode='RGB'         # Mode for PIL Image conversion, e.g., 'RGB', 'L'\n",
    "               ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame): # (df_train, df_val, df_test): DataFrames with columns 'image' and 'label'\n",
    "    \"\"\"\n",
    "    Convert MedMNIST datasets to DataFrames, with images as PIL Image objects and labels as DataFrame columns.\n",
    "    \n",
    "    Missing datasets (if None) are represented by None in the return tuple.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to convert dataset to DataFrame\n",
    "    def dataset_to_df(dataset, mode):\n",
    "        images, labels = dataset.imgs, dataset.labels\n",
    "        return pd.DataFrame({\n",
    "            'image': [Image.fromarray(img, mode) for img in images], \n",
    "            'label': labels.squeeze()\n",
    "        })\n",
    "    \n",
    "    # Convert each dataset to a DataFrame if it exists\n",
    "    df_train = dataset_to_df(train_dataset, mode) if train_dataset is not None else None\n",
    "    df_val = dataset_to_df(val_dataset, mode) if val_dataset is not None else None\n",
    "    df_test = dataset_to_df(test_dataset, mode) if test_dataset is not None else None\n",
    "    \n",
    "    return df_train, df_val, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data via Pooch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_file(url, # The URL of the file to be downloaded\n",
    "                  output_dir=\"data\", # The directory where the downloaded file will be saved\n",
    "                  extract=True, # If True, decompresses the file if it's in a compressed format\n",
    "                  hash=None, # Optional: You can add a checksum for integrity verification\n",
    "                  extract_dir=None, # Directory to extract the files to\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Download and optionally decompress a single file using Pooch.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set processor to Decompress if extract=True\n",
    "    processor = None\n",
    "    if extract:\n",
    "        if url.endswith('.zip'):\n",
    "            processor = Unzip(extract_dir=extract_dir)\n",
    "        elif url.endswith(('.tar', '.tar.gz', '.tar.bz2', '.tar.xz')):\n",
    "            processor = Untar(extract_dir=extract_dir)\n",
    "        elif url.endswith(('.gz', '.bz2', '.xz')):\n",
    "            processor = Decompress()\n",
    "\n",
    "    # Download the file, decompressing if extract=True and file is compressed\n",
    "    downloaded_file = pooch_retrieve(\n",
    "        url=url,\n",
    "        path=output_dir,\n",
    "        processor=processor,\n",
    "        known_hash=hash,  # Optional: You can add a checksum for integrity verification\n",
    "    )\n",
    "    \n",
    "    print(\"The file has been downloaded and saved to:\", os.path.abspath(output_dir))\n",
    "    # Get the name of the extraction subdirectory if extract = True\n",
    "    if extract:\n",
    "        output_dir_len = len(Path(os.path.abspath(output_dir)).parts)\n",
    "        file0 = os.path.abspath(downloaded_file[0])\n",
    "        extract_dir_abs = os.path.join(*Path(file0).parts[0:output_dir_len+1])\n",
    "        print(\"Extracted files have been saved to:\", extract_dir_abs)\n",
    "        return extract_dir_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_dataset(base_url, # The base URL from which the files will be downloaded.\n",
    "                     expected_checksums, #  A dictionary mapping file names to their expected checksums.\n",
    "                     file_names, # A dictionary mapping task identifiers to file names.\n",
    "                     output_dir, # The directory where the downloaded files will be saved.\n",
    "                     processor=None, # A function to process the downloaded data.\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Download a dataset using Pooch and save it to the specified output directory.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a Pooch object with the base URL, output directory, and expected checksums\n",
    "    pooch_instance = pooch_create(\n",
    "        path=output_dir,\n",
    "        base_url=base_url,\n",
    "        registry=expected_checksums,\n",
    "    )\n",
    "    \n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Download each file if it is not already present in the output directory\n",
    "    for _, file_name in file_names.items():\n",
    "        pooch_instance.fetch(file_name, progressbar=True, processor=processor)\n",
    "    \n",
    "    print(\"The dataset has been successfully downloaded and saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'Samples/actin-20x-noise1-highsnr-sample.png' from 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100888/Samples/actin-20x-noise1-highsnr-sample.png' to '/home/biagio/Code/bioMONAI/nbs/_test_folder'.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|█████████████████████████████████████| 2.38M/2.38M [00:00<00:00, 1.75GB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been successfully downloaded and saved to: ./_test_folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# # Specify the directory where you want to save the downloaded files\n",
    "output_directory = \"./_test_folder\"\n",
    "\n",
    "# Define the base URL for the MSD dataset\n",
    "base_url = 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100888/'\n",
    "\n",
    "# Define the expected checksums for the files in the dataset\n",
    "expected_checksums = {\n",
    "'Samples/actin-20x-noise1-highsnr-sample.png': 'md5:7995383f95473a4e74a3b49ed2d6a846'\n",
    "}\n",
    "\n",
    "# Define the names of the files to be downloaded\n",
    "file_names = {\n",
    "'1': 'Samples/actin-20x-noise1-highsnr-sample.png'\n",
    "}\n",
    "\n",
    "# Download the dataset\n",
    "download_dataset(base_url, expected_checksums, file_names, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_dataset_from_csv(csv_file, # Path to the CSV file containing file names and checksums.\n",
    "                              base_url, # The base URL from which the files will be downloaded.\n",
    "                              output_dir, # The directory where the downloaded files will be saved.\n",
    "                              processor=None, # A function to process the downloaded data.\n",
    "                              rows=None, # Specific row indices to download. If None, download all rows.\n",
    "                              prepend_mdf5=True, # If True, prepend 'md5:' to the checksums.\n",
    "                              ):\n",
    "    \"\"\"\n",
    "    Download a dataset using Pooch and save it to the specified output directory, reading file names and checksums from a CSV file.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # If specific rows are provided, filter the DataFrame\n",
    "    if rows is not None:\n",
    "        df = df.iloc[rows]\n",
    "        \n",
    "    # Create a dictionary for expected checksums\n",
    "    if prepend_mdf5:\n",
    "        expected_checksums = pd.Series(df['MD5 Checksum'].apply(lambda x: f\"md5:{x}\").values, index=df['Filename']).to_dict()\n",
    "    else:\n",
    "        expected_checksums = pd.Series(df['MD5 Checksum'].values, index=df['Filename']).to_dict()\n",
    "\n",
    "    # Create a Pooch object with the base URL, output directory, and expected checksums\n",
    "    pooch_instance = pooch_create(\n",
    "        path=output_dir,\n",
    "        base_url=base_url,\n",
    "        registry=expected_checksums,\n",
    "    )\n",
    "    \n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Download each file if it is not already present in the output directory\n",
    "    for file_name in df['Filename']:\n",
    "        pooch_instance.fetch(file_name, progressbar=True, processor=processor)\n",
    "    \n",
    "    print(\"The dataset has been successfully downloaded and saved to:\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been successfully downloaded and saved to: ./_test_folder\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory where you want to save the downloaded files\n",
    "output_directory = \"./_test_folder\"\n",
    "# Define the base URL for the MSD dataset\n",
    "base_url = 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100888/'\n",
    "\n",
    "download_dataset_from_csv('./data_examples/FMD_dataset_info.csv', base_url, output_directory, rows=[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data via Quilt/T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allen Institute Cell Science (AICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def aics_pipeline(n_images_to_download=40, # Number of images to download\n",
    "                  image_save_dir=None, # Directory to save the images\n",
    "                  col=\"SourceReadPath\", # Column name for image paths in the data manifest\n",
    "                  ): \n",
    "    # Set default save directory if not provided\n",
    "    if image_save_dir is None:\n",
    "        image_save_dir = os.getcwd()\n",
    "\n",
    "    # Ensure the save directory exists; create it if not\n",
    "    os.makedirs(image_save_dir, exist_ok=True)\n",
    "\n",
    "    # Load the package\n",
    "    package = quilt3.Package.browse(\"aics/pipeline_integrated_cell\", registry=\"s3://allencell\")\n",
    "    \n",
    "    # Load metadata\n",
    "    data_manifest = package[\"metadata.csv\"]()\n",
    "\n",
    "    # Keep only unique FOVs\n",
    "    unique_fov_indices = np.unique(data_manifest['FOVId'], return_index=True)[1]\n",
    "    data_manifest = data_manifest.iloc[unique_fov_indices]\n",
    "\n",
    "    # Select first n_images_to_download\n",
    "    data_manifest = data_manifest.iloc[:n_images_to_download]\n",
    "\n",
    "    # Get source and target paths\n",
    "    image_source_paths = data_manifest[col]\n",
    "    image_target_paths = [os.path.join(image_save_dir, os.path.basename(image_source_path)) \n",
    "                          for image_source_path in image_source_paths]\n",
    "\n",
    "    # Download images\n",
    "    downloaded_image_paths = []\n",
    "    for image_source_path, image_target_path in zip(image_source_paths, image_target_paths):\n",
    "        if os.path.exists(image_target_path):\n",
    "            downloaded_image_paths.append(image_target_path)\n",
    "            continue  # Skip if already downloaded\n",
    "        \n",
    "        try:\n",
    "            package[image_source_path].fetch(image_target_path)\n",
    "            downloaded_image_paths.append(image_target_path)\n",
    "        except (OSError, FileNotFoundError) as e:\n",
    "            print(f\"Failed to fetch {image_source_path}: {e}\")\n",
    "                \n",
    "    return downloaded_image_paths, data_manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading manifest: 100%|██████████| 77165/77165 [00:01<00:00, 45.2k/s]\n"
     ]
    }
   ],
   "source": [
    "image_target_paths, data_manifest = aics_pipeline(1, \"../_data/aics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../_data/aics/9e5d8f2e_3500001004_100X_20170623_5-Scene-1-P24-E06.czi_nucWholeIndexImageScale.tiff', '../_data/aics/77a69ff1_3500001004_100X_20170623_5-Scene-3-P26-F05.czi_nucWholeIndexImageScale.tiff']\n"
     ]
    }
   ],
   "source": [
    "print(image_target_paths)\n",
    "data_manifest.to_csv('../_data/aics/aics_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading manifest: 100%|██████████| 77165/77165 [00:01<00:00, 46.5k/s]\n",
      "100%|██████████| 491k/491k [00:02<00:00, 171kB/s] \n"
     ]
    }
   ],
   "source": [
    "image_target_paths, data_manifest = aics_pipeline(1, \"../_data/aics\", col=\"NucleusSegmentationReadPath\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities to make a list of all of the files of the train and test dataset in csv form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def manifest2csv(signal,                # List of paths to signal images\n",
    "                 target,                # List of paths to target images\n",
    "                 paths=None,            # List of paths to images\n",
    "                 train_fraction=0.8,    # Fraction of data to use for training\n",
    "                 data_save_path='./',   # Path to save the CSV files\n",
    "                 train='train.csv',     # Name of the training CSV file\n",
    "                 test='test.csv',       # Name of the test CSV file\n",
    "                 identifier=None,       # Identifier to add to the paths\n",
    "                 ):\n",
    "    \n",
    "    if paths is None:\n",
    "        df = pd.DataFrame(columns=[\"path_signal\", \"path_target\"])\n",
    "        df[\"path_signal\"] = signal\n",
    "        df[\"path_target\"] = target \n",
    "        length_dataset = len(signal)\n",
    "    elif identifier is None:\n",
    "        df = pd.DataFrame(columns=[\"path_tiff\", \"channel_signal\", \"channel_target\"])\n",
    "        df[\"path_tiff\"] = paths\n",
    "        df[\"channel_signal\"] = signal\n",
    "        df[\"channel_target\"] = target \n",
    "        length_dataset = len(paths)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"signal (path+identifier)\", \"target (path+identifier)\"])\n",
    "        df[\"signal (path+identifier)\"] = paths\n",
    "        df[\"signal (path+identifier)\"] = df[\"signal (path+identifier)\"] + identifier + [*signal.astype(str)]\n",
    "        df[\"target (path+identifier)\"] = paths\n",
    "        df[\"target (path+identifier)\"] = df[\"target (path+identifier)\"] + identifier + [*target.astype(str)]       \n",
    "        length_dataset = len(paths)\n",
    "        \n",
    "\n",
    "    n_train_images = int(length_dataset * train_fraction)\n",
    "    df_train = df[:n_train_images]\n",
    "    df_test = df[n_train_images:]\n",
    "\n",
    "    df_test.to_csv(data_save_path+test, index=False)\n",
    "    df_train.to_csv(data_save_path+train, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manifest2csv(data_manifest[\"ChannelNumberBrightfield\"],data_manifest[\"ChannelNumber405\"], image_target_paths, data_save_path='./data_examples/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_dataframe(input_data, # Path to CSV file or DataFrame\n",
    "                    train_fraction=0.7, # Proportion of data to use for the training set\n",
    "                    valid_fraction=0.1, # Proportion of data to use for the validation set\n",
    "                    split_column=None, # Column name that indicates pre-defined split\n",
    "                    stratify=False, # If True, stratify by split_column during random split\n",
    "                    add_is_valid=False, # If True, adds 'is_valid' column in the train set to mark validation samples\n",
    "                    train_path=\"train.csv\", # Path to save the training CSV file\n",
    "                    test_path=\"test.csv\", # Path to save the test CSV file\n",
    "                    valid_path=\"valid.csv\", # Path to save the validation CSV file\n",
    "                    data_save_path=None, # Path to save the data files\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame or CSV file into train, test, and optional validation sets.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    if isinstance(input_data, str):\n",
    "        df = pd.read_csv(input_data)\n",
    "    elif isinstance(input_data, pd.DataFrame):\n",
    "        df = input_data.copy()\n",
    "    else:\n",
    "        raise ValueError(\"input_data must be a path to a CSV file or a DataFrame\")\n",
    "    \n",
    "    if data_save_path:\n",
    "        train_path = os.path.join(data_save_path, train_path)\n",
    "        test_path = os.path.join(data_save_path, test_path)\n",
    "        valid_path = os.path.join(data_save_path, valid_path)\n",
    "\n",
    "    # Check if split_column exists and has \"train\", \"test\", or \"validation\" values\n",
    "    if split_column and split_column in df.columns:\n",
    "        # Use pre-defined split values if available\n",
    "        train_df = df[df[split_column] == \"train\"].copy()\n",
    "        test_df = df[df[split_column] == \"test\"].copy()\n",
    "        valid_df = df[df[split_column] == \"validation\"].copy() if \"validation\" in df[split_column].unique() else None\n",
    "    else:\n",
    "        # Otherwise, calculate test and validation splits based on fractions\n",
    "        test_fraction = 1.0 - train_fraction - valid_fraction\n",
    "        if test_fraction <= 0:\n",
    "            raise ValueError(\"train_fraction and valid_fraction must sum to less than 1.\")\n",
    "\n",
    "        # Randomly split data\n",
    "        train_df, temp_df = train_test_split(df, test_size=(1 - train_fraction), stratify=df[split_column] if stratify and split_column else None)\n",
    "        train_df = train_df.copy()\n",
    "        temp_df = temp_df.copy()\n",
    "\n",
    "        if valid_fraction > 0:\n",
    "            valid_size = valid_fraction / (valid_fraction + test_fraction)\n",
    "            valid_df, test_df = train_test_split(temp_df, test_size=(1 - valid_size), stratify=temp_df[split_column] if stratify and split_column else None)\n",
    "            valid_df = valid_df.copy()\n",
    "            test_df = test_df.copy()\n",
    "        else:\n",
    "            test_df = temp_df\n",
    "            valid_df = None\n",
    "\n",
    "    # Optionally add 'is_valid' column in train set if valid_fraction > 0\n",
    "    if add_is_valid and valid_fraction > 0:\n",
    "        train_df.loc[:, 'is_valid'] = 0  # Avoid SettingWithCopyError by using .loc\n",
    "        if valid_df is None:\n",
    "            # Sample validation rows from the training set if no pre-defined validation split\n",
    "            valid_indices = train_df.sample(frac=valid_fraction / train_fraction).index\n",
    "            train_df.loc[valid_indices, 'is_valid'] = 1\n",
    "        else:\n",
    "            print(f\"'is_valid' column added to '{train_path}' for validation samples within the training set.\")\n",
    "    elif valid_df is not None:\n",
    "        # Save validation set as a separate CSV if not adding 'is_valid' in training set\n",
    "        valid_df.to_csv(valid_path, index=False)\n",
    "        print(f\"Validation file saved as '{valid_path}'.\")\n",
    "\n",
    "    # Save train and test sets as CSV files\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    \n",
    "    print(f\"Train and test files saved as '{train_path}' and '{test_path}' respectively.\")\n",
    "    if add_is_valid and valid_fraction > 0:\n",
    "        print(f\"'is_valid' column added to '{train_path}' for validation samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def add_columns_to_csv(csv_path, # Path to the input CSV file\n",
    "                       column_data, # Dictionary of column names and values to add. Each value can be a scalar (single value for all rows) or a list matching the number of rows.\n",
    "                       output_path=None, # Path to save the updated CSV file. If None, it overwrites the input CSV file.\n",
    "                       ):\n",
    "    \"\"\"\n",
    "    Adds one or more new columns to an existing CSV file.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Iterate over each column and add to the DataFrame\n",
    "    for column_name, column_values in column_data.items():\n",
    "        # Check if column_values is a list and matches DataFrame length\n",
    "        if isinstance(column_values, list) and len(column_values) != len(df):\n",
    "            raise ValueError(f\"Length of values for column '{column_name}' does not match the number of rows in the CSV.\")\n",
    "        \n",
    "        # Add the new column\n",
    "        df[column_name] = column_values\n",
    "\n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    output_path = output_path or csv_path\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Columns {list(column_data.keys())} added successfully. Updated file saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
