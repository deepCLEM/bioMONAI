[
  {
    "objectID": "nets.html",
    "href": "nets.html",
    "title": "Networks",
    "section": "",
    "text": "source\n\n\n\n create_custom_unet (resnet_version, output_channels, img_size=(128, 128),\n                     pretrained=True, n_in=1, cut=4)\n\n*Create a U-Net model with a ResNet backbone.\nReturns: - U-Net model with the specified ResNet backbone.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresnet_version\n\n\nChoose a ResNet model between: ‘resnet18’, ‘resnet34’, ‘resnet50’, ‘resnet101’, and ‘resnet152’.\n\n\noutput_channels\n\n\nNumber of output channels.\n\n\nimg_size\ntuple\n(128, 128)\nTuple for the input image size, default is (128, 128).\n\n\npretrained\nbool\nTrue\nIf True, use a pretrained ResNet backbone.\n\n\nn_in\nint\n1\nNumber of input channels, default is 1 (e.g., grayscale).\n\n\ncut\nint\n4\nThe cut point for the ResNet model, default is 4."
  },
  {
    "objectID": "nets.html#unet",
    "href": "nets.html#unet",
    "title": "Networks",
    "section": "",
    "text": "source\n\n\n\n create_custom_unet (resnet_version, output_channels, img_size=(128, 128),\n                     pretrained=True, n_in=1, cut=4)\n\n*Create a U-Net model with a ResNet backbone.\nReturns: - U-Net model with the specified ResNet backbone.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresnet_version\n\n\nChoose a ResNet model between: ‘resnet18’, ‘resnet34’, ‘resnet50’, ‘resnet101’, and ‘resnet152’.\n\n\noutput_channels\n\n\nNumber of output channels.\n\n\nimg_size\ntuple\n(128, 128)\nTuple for the input image size, default is (128, 128).\n\n\npretrained\nbool\nTrue\nIf True, use a pretrained ResNet backbone.\n\n\nn_in\nint\n1\nNumber of input channels, default is 1 (e.g., grayscale).\n\n\ncut\nint\n4\nThe cut point for the ResNet model, default is 4."
  },
  {
    "objectID": "nets.html#denoising-cnn",
    "href": "nets.html#denoising-cnn",
    "title": "Networks",
    "section": "Denoising CNN",
    "text": "Denoising CNN\n\nsource\n\nDnCNN\n\n DnCNN (spatial_dims=2, in_channels=1, out_channels=1, num_of_layers=9,\n        features=64, kernel_size=3)\n\nA Deep Neural Network for Image Denoising (DnCNN) model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions\n\n\nin_channels\nint\n1\nNumber of input channels\n\n\nout_channels\nint\n1\nNumber of output channels\n\n\nnum_of_layers\nint\n9\nNumber of convolutional layers\n\n\nfeatures\nint\n64\nNumber of feature maps\n\n\nkernel_size\nint\n3\nSize of the convolution kernel\n\n\n\n\nx = torch_randn(16, 1, 32, 64)\n\ntst = DnCNN(2,1)\ntest_eq(tst(x).shape, x.shape)"
  },
  {
    "objectID": "nets.html#deeplab-v3",
    "href": "nets.html#deeplab-v3",
    "title": "Networks",
    "section": "DeepLab v3+",
    "text": "DeepLab v3+\n\nConfig\n\nsource\n\n\ninterpolate\n\n interpolate (x:torch.Tensor, size:Union[List[int],Tuple[int,...]],\n              dims:int)\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nInput tensor\n\n\nsize\nUnion\nSize of the output tensor\n\n\ndims\nint\nNumber of spatial dimensions\n\n\nReturns\nTensor\nOutput tensor\n\n\n\n\nsource\n\n\nget_padding\n\n get_padding (kernel_size:int, dilation:int)\n\n\n\n\n\nType\nDetails\n\n\n\n\nkernel_size\nint\nSize of the convolution kernel\n\n\ndilation\nint\nDilation rate\n\n\nReturns\nint\nPadding size\n\n\n\n\nsource\n\n\nDeeplabConfig\n\n DeeplabConfig (dimensions:int, in_channels:int, out_channels:int,\n                backbone:str='xception', pretrained:bool=False,\n                middle_flow_blocks:int=16,\n                aspp_dilations:List[int]=&lt;factory&gt;,\n                entry_block3_stride:int=2, middle_block_dilation:int=1,\n                exit_block_dilations:Tuple[int,int]=(1, 2))\n\n\n\nBlocks\n\nsource\n\n\nBlock\n\n Block (config:__main__.DeeplabConfig, inplanes:int, planes:int, reps:int,\n        stride:int=1, dilation:int=1, start_with_relu:bool=True,\n        grow_first:bool=True, is_last:bool=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nDeeplabConfig\n\nConfiguration for the Deeplab model\n\n\ninplanes\nint\n\nNumber of input channels\n\n\nplanes\nint\n\nNumber of output channels\n\n\nreps\nint\n\nNumber of convolutional layers\n\n\nstride\nint\n1\nStride for the convolution\n\n\ndilation\nint\n1\nDilation rate for the convolution\n\n\nstart_with_relu\nbool\nTrue\nIf True, start with a ReLU activation\n\n\ngrow_first\nbool\nTrue\nIf True, increase the number of channels in the first convolution\n\n\nis_last\nbool\nFalse\nIf True, add a convolution layer at the end\n\n\n\n\nsource\n\n\nSeparableConv\n\n SeparableConv (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n                kernel_size:int=3, stride:int=1, dilation:int=1,\n                bias:bool=False, norm:Optional[str]=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nDeeplabConfig\n\nConfiguration for the Deeplab model\n\n\ninplanes\nint\n\nNumber of input channels\n\n\nplanes\nint\n\nNumber of output channels\n\n\nkernel_size\nint\n3\nSize of the convolution kernel\n\n\nstride\nint\n1\nStride for the convolution\n\n\ndilation\nint\n1\nDilation rate for the convolution\n\n\nbias\nbool\nFalse\nIf True, add a bias term\n\n\nnorm\nOptional\nNone\nType of normalization layer\n\n\n\n\n\nAligned Xception\n\nsource\n\n\nXception\n\n Xception (config:__main__.DeeplabConfig)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDetails\n\n\n\n\nconfig\nDeeplabConfig\nConfiguration for the Deeplab model\n\n\n\n\n\nASPP\n\nsource\n\n\nASPP_module\n\n ASPP_module (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n              dilation:int)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDetails\n\n\n\n\nconfig\nDeeplabConfig\nConfiguration for the Deeplab model\n\n\ninplanes\nint\nNumber of input channels\n\n\nplanes\nint\nNumber of output channels\n\n\ndilation\nint\nDilation rate for the convolution\n\n\n\n\n\nDeepLab V3\n\nsource\n\n\nDeeplab\n\n Deeplab (config:__main__.DeeplabConfig)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nExample\n\n# Load a pre-trained ResNet backbone\nresnet_backbone = ResNetFeatures('resnet10', pretrained=False, in_channels=1, spatial_dims=3)\n\n# Forward pass through the backbone to get the output before the final classifier\ndummy_input = torch_randn(1, 1, 64, 224, 224)  # Example input size; adjust based on your needs\noutput = resnet_backbone(dummy_input)\n\n# The shape of 'output' will give you the number of channels at this stage in the backbone\nprint(\"Output channels:\", output[-1].shape[1])\n\nOutput channels: 512\n\n\n\n# For 2D images\nconfig_2d = DeeplabConfig(\n    dimensions=2,\n    in_channels=3,  # For RGB images\n    out_channels=4,\n    backbone=\"xception\",  # or whatever backbone you're using\n    aspp_dilations=[1, 6, 12, 18]\n)\nmodel_2d = Deeplab(config_2d)\n\n# For 3D images\nconfig_3d = DeeplabConfig(\n    dimensions=3,\n    in_channels=1,  # For single-channel 3D medical images\n    out_channels=4,\n    middle_flow_blocks=16,\n    aspp_dilations=[1, 6, 12, 18]\n)\nmodel_3d = Deeplab(config_3d)\n\n\nfrom torch import no_grad as torch_no_grad\n\n\ndef test_deeplab(config, input_shape, expected_output_shape):\n    set_determinism(0)  # For reproducibility\n    \n    model = Deeplab(config)\n    model.eval()  # Set the model to evaluation mode\n    \n    # Generate random input tensor\n    x = torch_randn(*input_shape)\n    \n    # Forward pass\n    with torch_no_grad():\n        output = model(x)\n    \n    # Check output shape\n    assert output.shape == expected_output_shape, f\"Expected shape {expected_output_shape}, but got {output.shape}\"\n    \n    print(f\"Test passed for {config.dimensions}D model with backbone {config.backbone}\")\n    print(f\"Input shape: {input_shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(\"---\")\n\n\n# Test 2D model\nconfig_2d = DeeplabConfig(\n    dimensions=2,\n    in_channels=3,\n    out_channels=4,\n    backbone=\"xception\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_2d, (1, 3, 64, 64), (1, 4, 64, 64))\n\n# Test 2D model with ResNet50 backbone\nconfig_2d_resnet = DeeplabConfig(\n    dimensions=2,\n    in_channels=3,\n    out_channels=4,\n    backbone=\"resnet50\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_2d_resnet, (1, 3, 64, 64), (1, 4, 64, 64))\n\n# Test 3D model\nconfig_3d = DeeplabConfig(\n    dimensions=3,\n    in_channels=1,\n    out_channels=4,\n    backbone=\"xception\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_3d, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n\n# Test 3D model with ResNet10 backbone\nconfig_3d_resnet = DeeplabConfig(\n    dimensions=3,\n    in_channels=1,\n    out_channels=4,\n    backbone=\"resnet10\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_3d_resnet, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n\nprint(\"All tests passed successfully!\")\n\nTest passed for 2D model with backbone xception\nInput shape: (1, 3, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64])\n---\nTest passed for 2D model with backbone resnet50\nInput shape: (1, 3, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64])\n---\nTest passed for 3D model with backbone xception\nInput shape: (1, 1, 64, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64, 64])\n---\nTest passed for 3D model with backbone resnet10\nInput shape: (1, 1, 64, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64, 64])\n---\nAll tests passed successfully!"
  },
  {
    "objectID": "nets.html#umamba",
    "href": "nets.html#umamba",
    "title": "Networks",
    "section": "UMamba",
    "text": "UMamba\n\nsource\n\nMambaLayer\n\n MambaLayer (dim, d_state=16, d_conv=4, expand=2)\n\nA custom neural network layer that incorporates the Mamba block from the Mamba model, along with layer normalization and optional mixed precision handling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndim\n\n\nDimension of the input tensor\n\n\nd_state\nint\n16\nExpansion factor for the state in the Mamba block\n\n\nd_conv\nint\n4\nWidth of the local convolution in the Mamba block\n\n\nexpand\nint\n2\nFactor by which to expand the dimensions in the Mamba block\n\n\n\n\nsource\n\n\nUMamba\n\n UMamba (spatial_dims:int, in_channels:int, out_channels:int,\n         kernel_size:Sequence[Union[Sequence[int],int]],\n         strides:Sequence[Union[Sequence[int],int]],\n         upsample_kernel_size:Sequence[Union[Sequence[int],int]],\n         filters:Optional[Sequence[int]]=None,\n         dropout:Union[Tuple,str,float,NoneType]=None,\n         norm_name:Union[Tuple,str]=('INSTANCE', {'affine': True}),\n         act_name:Union[Tuple,str]=('leakyrelu', {'inplace': True,\n         'negative_slope': 0.01}), deep_supervision:bool=False,\n         deep_supr_num:int=1, res_block:bool=False, trans_bias:bool=False)\n\n*A custom subclass of DynUNet that integrates the Mamba layer into the model’s bottleneck.\nThis class inherits from DynUNet and adds a specific bottleneck structure containing a convolution block followed by a MambaLayer.*\n\nExample\n\nx = torch_randn(16, 1, 32, 64)\n\ntst = DynUNet(2,1,1,[3,3,3],[1,1,1],[1,1])\nprint(tst(x).shape)\ntest_eq(tst(x).shape, x.shape)\n\ntorch.Size([16, 1, 32, 64])\n\n\n\nx = torch_randn(16, 1, 32, 64).cuda()\n\ntst = UMamba(2,1,1,[3,3,3],[1,1,1],[1,1]).cuda()\nprint(tst(x).shape)\ntest_eq(tst(x).shape, x.shape)\n\ntorch.Size([16, 1, 32, 64])"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "This section includes essential imports used throughout the core library, providing foundational tools for data handling, model training, and evaluation. Key imports cover areas such as data blocks, data loaders, custom loss functions, optimizers, callbacks, and logging.\n\nsource\n\n\n\n DataBlock (blocks:list=None, dl_type:TfmdDL=None, getters:list=None,\n            n_inp:int=None, item_tfms:list=None, batch_tfms:list=None,\n            get_items=None, splitter=None, get_y=None, get_x=None)\n\nGeneric container to quickly build Datasets and DataLoaders.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nblocks\nlist\nNone\nOne or more TransformBlocks\n\n\ndl_type\nTfmdDL\nNone\nTask specific TfmdDL, defaults to block’s dl_type orTfmdDL\n\n\ngetters\nlist\nNone\nGetter functions applied to results of get_items\n\n\nn_inp\nint\nNone\nNumber of inputs\n\n\nitem_tfms\nlist\nNone\nItemTransforms, applied on an item\n\n\nbatch_tfms\nlist\nNone\nTransforms or RandTransforms, applied by batch\n\n\nget_items\nNoneType\nNone\n\n\n\nsplitter\nNoneType\nNone\n\n\n\nget_y\nNoneType\nNone\n\n\n\nget_x\nNoneType\nNone\n\n\n\n\nThe DataBlock class acts as a container for creating data processing pipelines, allowing easy customization of datasets and data loaders. It enables the definition of item transformations, batch transformations, and dataset split methods, streamlining data preprocessing and loading across various stages of model training.\n\nsource\n\n\n\n\n DataLoaders (*loaders, path:str|pathlib.Path='.', device=None)\n\nBasic wrapper around several DataLoaders.\nThe DataLoaders class is a container for managing training and validation datasets. This class wraps one or more DataLoader instances, ensuring seamless data management and transfer across devices (CPU or GPU) for efficient training and evaluation.\n\nsource\n\n\n\nGroup together a model, some dls and a loss_func to handle training\nThe Learner class is the main interface for training machine learning models, encapsulating the model, data, loss function, optimizer, and training metrics. It simplifies the training process by providing built-in functionality for model evaluation, hyperparameter tuning, and training loop customization, allowing you to focus on model optimization.\n\nsource\n\n\n\n\n ShowGraphCallback (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\nThe ShowGraphCallback is a convenient callback for visualizing training progress. By plotting the training and validation loss, it helps users monitor convergence and performance, making it easy to assess if the model requires adjustments in learning rate, architecture, or data handling.\n\nsource\n\n\n\n\n CSVLogger (fname='history.csv', append=False)\n\nLog the results displayed in learn.path/fname\nThe CSVLogger is a tool for logging model training metrics to a CSV file, offering a permanent record of training history. This feature is especially useful for long-term experiments and fine-tuning, allowing you to track and analyze model performance over time.\n\n\n\n\n\n cells3d ()\n\n*3D fluorescence microscopy image of cells.\nThe returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.*\nThe cells3d function returns a sample 3D fluorescence microscopy image. This is a valuable test image for demonstration and analysis, consisting of both cell membrane and nucleus channels. It can serve as a default dataset for evaluating and benchmarking new models and transformations."
  },
  {
    "objectID": "core.html#imports",
    "href": "core.html#imports",
    "title": "Core",
    "section": "",
    "text": "This section includes essential imports used throughout the core library, providing foundational tools for data handling, model training, and evaluation. Key imports cover areas such as data blocks, data loaders, custom loss functions, optimizers, callbacks, and logging.\n\nsource\n\n\n\n DataBlock (blocks:list=None, dl_type:TfmdDL=None, getters:list=None,\n            n_inp:int=None, item_tfms:list=None, batch_tfms:list=None,\n            get_items=None, splitter=None, get_y=None, get_x=None)\n\nGeneric container to quickly build Datasets and DataLoaders.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nblocks\nlist\nNone\nOne or more TransformBlocks\n\n\ndl_type\nTfmdDL\nNone\nTask specific TfmdDL, defaults to block’s dl_type orTfmdDL\n\n\ngetters\nlist\nNone\nGetter functions applied to results of get_items\n\n\nn_inp\nint\nNone\nNumber of inputs\n\n\nitem_tfms\nlist\nNone\nItemTransforms, applied on an item\n\n\nbatch_tfms\nlist\nNone\nTransforms or RandTransforms, applied by batch\n\n\nget_items\nNoneType\nNone\n\n\n\nsplitter\nNoneType\nNone\n\n\n\nget_y\nNoneType\nNone\n\n\n\nget_x\nNoneType\nNone\n\n\n\n\nThe DataBlock class acts as a container for creating data processing pipelines, allowing easy customization of datasets and data loaders. It enables the definition of item transformations, batch transformations, and dataset split methods, streamlining data preprocessing and loading across various stages of model training.\n\nsource\n\n\n\n\n DataLoaders (*loaders, path:str|pathlib.Path='.', device=None)\n\nBasic wrapper around several DataLoaders.\nThe DataLoaders class is a container for managing training and validation datasets. This class wraps one or more DataLoader instances, ensuring seamless data management and transfer across devices (CPU or GPU) for efficient training and evaluation.\n\nsource\n\n\n\nGroup together a model, some dls and a loss_func to handle training\nThe Learner class is the main interface for training machine learning models, encapsulating the model, data, loss function, optimizer, and training metrics. It simplifies the training process by providing built-in functionality for model evaluation, hyperparameter tuning, and training loop customization, allowing you to focus on model optimization.\n\nsource\n\n\n\n\n ShowGraphCallback (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\nThe ShowGraphCallback is a convenient callback for visualizing training progress. By plotting the training and validation loss, it helps users monitor convergence and performance, making it easy to assess if the model requires adjustments in learning rate, architecture, or data handling.\n\nsource\n\n\n\n\n CSVLogger (fname='history.csv', append=False)\n\nLog the results displayed in learn.path/fname\nThe CSVLogger is a tool for logging model training metrics to a CSV file, offering a permanent record of training history. This feature is especially useful for long-term experiments and fine-tuning, allowing you to track and analyze model performance over time.\n\n\n\n\n\n cells3d ()\n\n*3D fluorescence microscopy image of cells.\nThe returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.*\nThe cells3d function returns a sample 3D fluorescence microscopy image. This is a valuable test image for demonstration and analysis, consisting of both cell membrane and nucleus channels. It can serve as a default dataset for evaluating and benchmarking new models and transformations."
  },
  {
    "objectID": "core.html#engine",
    "href": "core.html#engine",
    "title": "Core",
    "section": "Engine",
    "text": "Engine\nThe engine module provides advanced functionalities for model training, including configurable training loops and evaluation functions tailored for bioinformatics applications.\n\nsource\n\nfastTrainer\n\n fastTrainer (dataloaders:fastai.data.core.DataLoaders, model:&lt;built-\n              infunctioncallable&gt;, loss_fn:typing.Any|None=None, optimizer\n              :fastai.optimizer.Optimizer|fastai.optimizer.OptimWrapper=&lt;f\n              unction Adam&gt;, lr:float|slice=0.001, splitter:&lt;built-\n              infunctioncallable&gt;=&lt;function trainable_params&gt;, callbacks:U\n              nion[fastai.callback.core.Callback,MutableSequence,NoneType]\n              =None, metrics:Union[Any,MutableSequence,NoneType]=None,\n              csv_log:bool=False, show_graph:bool=True,\n              show_summary:bool=False, find_lr:bool=False,\n              find_lr_fn=&lt;function valley&gt;,\n              path:str|pathlib.Path|None=None,\n              model_dir:str|pathlib.Path='models', wd:float|int|None=None,\n              wd_bn_bias:bool=False, train_bn:bool=True, moms:tuple=(0.95,\n              0.85, 0.95), default_cbs:bool=True)\n\nA custom implementation of the FastAI Learner class for training models in bioinformatics applications.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataloaders\nDataLoaders\n\nThe DataLoader objects containing training and validation datasets.\n\n\nmodel\ncallable\n\nA callable model that will be trained on the dataset.\n\n\nloss_fn\ntyping.Any | None\nNone\nThe loss function to optimize during training. If None, defaults to a suitable default.\n\n\noptimizer\nfastai.optimizer.Optimizer | fastai.optimizer.OptimWrapper\nAdam\nThe optimizer function to use. Defaults to Adam if not specified.\n\n\nlr\nfloat | slice\n0.001\nLearning rate for the optimizer. Can be a float or a slice object for learning rate scheduling.\n\n\nsplitter\ncallable\ntrainable_params\n\n\n\ncallbacks\nUnion\nNone\nA callable that determines which parameters of the model should be updated during training.\n\n\nmetrics\nUnion\nNone\nOptional list of callback functions to customize training behavior.\n\n\ncsv_log\nbool\nFalse\nMetrics to evaluate the performance of the model during training.\n\n\nshow_graph\nbool\nTrue\nWhether to log training history to a CSV file. If True, logs will be appended to ‘history.csv’.\n\n\nshow_summary\nbool\nFalse\nThe base directory where models are saved or loaded from. Defaults to None.\n\n\nfind_lr\nbool\nFalse\nSubdirectory within the base path where trained models are stored. Default is ‘models’.\n\n\nfind_lr_fn\nfunction\nvalley\nWeight decay factor for optimization. Defaults to None.\n\n\npath\nstr | pathlib.Path | None\nNone\nWhether to apply weight decay to batch normalization and bias parameters.\n\n\nmodel_dir\nstr | pathlib.Path\nmodels\nWhether to update the batch normalization statistics during training.\n\n\nwd\nfloat | int | None\nNone\n\n\n\nwd_bn_bias\nbool\nFalse\n\n\n\ntrain_bn\nbool\nTrue\n\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nTuple of tuples representing the momentum values for different layers in the model. Defaults to FastAI’s default settings if not specified.\n\n\ndefault_cbs\nbool\nTrue\nAutomatically include default callbacks such as ShowGraphCallback and CSVLogger.\n\n\n\n\nsource\n\n\nvisionTrainer\n\n visionTrainer (dataloaders:fastai.data.core.DataLoaders, model:&lt;built-\n                infunctioncallable&gt;, normalize=True, n_out=None,\n                pretrained=True, weights=None,\n                loss_fn:typing.Any|None=None, optimizer:fastai.optimizer.O\n                ptimizer|fastai.optimizer.OptimWrapper=&lt;function Adam&gt;,\n                lr:float|slice=0.001, splitter:&lt;built-\n                infunctioncallable&gt;=&lt;function trainable_params&gt;, callbacks\n                :Union[fastai.callback.core.Callback,MutableSequence,NoneT\n                ype]=None,\n                metrics:Union[Any,MutableSequence,NoneType]=None,\n                csv_log:bool=False, show_graph:bool=True,\n                show_summary:bool=False, find_lr:bool=False,\n                find_lr_fn=&lt;function valley&gt;,\n                path:str|pathlib.Path|None=None,\n                model_dir:str|pathlib.Path='models',\n                wd:float|int|None=None, wd_bn_bias:bool=False,\n                train_bn:bool=True, moms:tuple=(0.95, 0.85, 0.95),\n                default_cbs:bool=True, cut=None, init=&lt;function\n                kaiming_normal_&gt;, custom_head=None, concat_pool=True,\n                pool=True, lin_ftrs=None, ps=0.5, first_bn=True,\n                bn_final=False, lin_first=False, y_range=None, n_in=3)\n\nBuild a vision trainer from dataloaders and model\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataloaders\nDataLoaders\n\nThe DataLoader objects containing training and validation datasets.\n\n\nmodel\ncallable\n\nA callable model that will be trained on the dataset.\n\n\nnormalize\nbool\nTrue\n\n\n\nn_out\nNoneType\nNone\n\n\n\npretrained\nbool\nTrue\n\n\n\nweights\nNoneType\nNone\n\n\n\nloss_fn\ntyping.Any | None\nNone\nThe loss function to optimize during training. If None, defaults to a suitable default.\n\n\noptimizer\nfastai.optimizer.Optimizer | fastai.optimizer.OptimWrapper\nAdam\nThe optimizer function to use. Defaults to Adam if not specified.\n\n\nlr\nfloat | slice\n0.001\nLearning rate for the optimizer. Can be a float or a slice object for learning rate scheduling.\n\n\nsplitter\ncallable\ntrainable_params\n\n\n\ncallbacks\nUnion\nNone\nA callable that determines which parameters of the model should be updated during training.\n\n\nmetrics\nUnion\nNone\nOptional list of callback functions to customize training behavior.\n\n\ncsv_log\nbool\nFalse\nMetrics to evaluate the performance of the model during training.\n\n\nshow_graph\nbool\nTrue\nWhether to log training history to a CSV file. If True, logs will be appended to ‘history.csv’.\n\n\nshow_summary\nbool\nFalse\nThe base directory where models are saved or loaded from. Defaults to None.\n\n\nfind_lr\nbool\nFalse\nSubdirectory within the base path where trained models are stored. Default is ‘models’.\n\n\nfind_lr_fn\nfunction\nvalley\nWeight decay factor for optimization. Defaults to None.\n\n\npath\nstr | pathlib.Path | None\nNone\nWhether to apply weight decay to batch normalization and bias parameters.\n\n\nmodel_dir\nstr | pathlib.Path\nmodels\nWhether to update the batch normalization statistics during training.\n\n\nwd\nfloat | int | None\nNone\n\n\n\nwd_bn_bias\nbool\nFalse\n\n\n\ntrain_bn\nbool\nTrue\n\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nTuple of tuples representing the momentum values for different layers in the model. Defaults to FastAI’s default settings if not specified.\n\n\ndefault_cbs\nbool\nTrue\nAutomatically include default callbacks such as ShowGraphCallback and CSVLogger.\n\n\ncut\nNoneType\nNone\nmodel & head args\n\n\ninit\nfunction\nkaiming_normal_\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\nconcat_pool\nbool\nTrue\n\n\n\npool\nbool\nTrue\n\n\n\nlin_ftrs\nNoneType\nNone\n\n\n\nps\nfloat\n0.5\n\n\n\nfirst_bn\nbool\nTrue\n\n\n\nbn_final\nbool\nFalse\n\n\n\nlin_first\nbool\nFalse\n\n\n\ny_range\nNoneType\nNone\n\n\n\nn_in\nint\n3"
  },
  {
    "objectID": "core.html#evaluation",
    "href": "core.html#evaluation",
    "title": "Core",
    "section": "Evaluation",
    "text": "Evaluation\n\nsource\n\ndisplay_statistics_table\n\n display_statistics_table (stats, fn_name='', as_dataframe=True)\n\nDisplay a table of the key statistics.\n\nsource\n\n\nplot_histogram_and_kde\n\n plot_histogram_and_kde (data, stats, bw_method=0.3, fn_name='')\n\nPlot the histogram and KDE of the data with key statistics marked.\n\nsource\n\n\nformat_sig\n\n format_sig (value)\n\nFormat numbers with two significant digits.\n\nsource\n\n\ncalculate_statistics\n\n calculate_statistics (data)\n\nCalculate key statistics for the data.\n\nsource\n\n\ncompute_metric\n\n compute_metric (predictions, targets, metric_fn)\n\nCompute the metric for each prediction-target pair. Handles cases where metric_fn has or does not have a ‘func’ attribute.\n\nsource\n\n\ncompute_losses\n\n compute_losses (predictions, targets, loss_fn)\n\nCompute the loss for each prediction-target pair.\n\nfrom numpy.random import standard_normal\n\n\na = standard_normal(1000)\n\nstats = calculate_statistics(a)\n\nplot_histogram_and_kde(a, stats)\n\n\n\n\n\nsource\n\n\nevaluate_model\n\n evaluate_model (trainer:fastai.learner.Learner,\n                 test_data:fastai.data.core.DataLoaders=None, loss=None,\n                 metrics=None, bw_method=0.3, show_graph=True,\n                 show_table=True, show_results=True, as_dataframe=True,\n                 cmap='magma')\n\nCalculate and optionally plot the distribution of loss values from predictions made by the trainer on test data, with an optional table of key statistics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrainer\nLearner\n\nThe model trainer object with a get_preds method.\n\n\ntest_data\nDataLoaders\nNone\nDataLoader containing test data.\n\n\nloss\nNoneType\nNone\nLoss function to evaluate prediction-target pairs.\n\n\nmetrics\nNoneType\nNone\nSingle metric or a list of metrics to evaluate.\n\n\nbw_method\nfloat\n0.3\nBandwidth method for KDE.\n\n\nshow_graph\nbool\nTrue\nBoolean flag to show the histogram and KDE plot.\n\n\nshow_table\nbool\nTrue\nBoolean flag to show the statistics table.\n\n\nshow_results\nbool\nTrue\nBoolean flag to show model results on test data.\n\n\nas_dataframe\nbool\nTrue\nBoolean flag to display table as a DataFrame.\n\n\ncmap\nstr\nmagma\nColormap for visualization.\n\n\n\n\nsource\n\n\nevaluate_classification_model\n\n evaluate_classification_model (trainer:fastai.learner.Learner,\n                                test_data:fastai.data.core.DataLoaders=Non\n                                e, loss_fn=None, most_confused_n:int=1,\n                                normalize:bool=True, metrics=None,\n                                bw_method=0.3, show_graph=True,\n                                show_table=True, show_results=True,\n                                as_dataframe=True, cmap=&lt;matplotlib.colors\n                                .LinearSegmentedColormap object at\n                                0x7fb948fffb50&gt;)\n\nEvaluates a classification model by displaying results, confusion matrix, and most confused classes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrainer\nLearner\n\nThe trained model (learner) to evaluate.\n\n\ntest_data\nDataLoaders\nNone\nDataLoader with test data for evaluation. If None, the validation dataset is used.\n\n\nloss_fn\nNoneType\nNone\nLoss function used in the model for ClassificationInterpretation. If None, the loss function is loaded from trainer.\n\n\nmost_confused_n\nint\n1\nNumber of most confused class pairs to display.\n\n\nnormalize\nbool\nTrue\nWhether to normalize the confusion matrix.\n\n\nmetrics\nNoneType\nNone\nSingle metric or a list of metrics to evaluate.\n\n\nbw_method\nfloat\n0.3\nBandwidth method for KDE.\n\n\nshow_graph\nbool\nTrue\nBoolean flag to show the histogram and KDE plot.\n\n\nshow_table\nbool\nTrue\nBoolean flag to show the statistics table.\n\n\nshow_results\nbool\nTrue\nBoolean flag to show model results on test data.\n\n\nas_dataframe\nbool\nTrue\nBoolean flag to display table as a DataFrame.\n\n\ncmap\nLinearSegmentedColormap\n&lt;matplotlib.colors.LinearSegmentedColormap object at 0x7fb948fffb50&gt;\nColor map for the confusion matrix plot."
  },
  {
    "objectID": "core.html#utils",
    "href": "core.html#utils",
    "title": "Core",
    "section": "Utils",
    "text": "Utils\nThe utils module contains helper functions and classes to facilitate data manipulation, model setup, and training. These utilities add flexibility and convenience, supporting rapid experimentation and efficient data handling.\n\nsource\n\nattributesFromDict\n\n attributesFromDict (d)\n\nThe attributesFromDict function simplifies the conversion of dictionary keys and values into object attributes, allowing dynamic attribute creation for configuration objects. This utility is handy for initializing model or dataset configurations directly from dictionaries, improving code readability and maintainability.\n\nsource\n\n\nget_device\n\n get_device ()\n\n\nsource\n\n\nimg2float\n\n img2float (image, force_copy=False)\n\n\nsource\n\n\nimg2Tensor\n\n img2Tensor (image)"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "MedMNIST Datasets\n\nsource\n\n\ndownload_medmnist\n\n download_medmnist (dataset:str, output_dir:str='.',\n                    download_only:bool=False, save_images:bool=True)\n\n*Downloads the specified MedMNIST dataset and saves the training, validation, and test datasets into the specified output directory. Images are saved as .png for 2D data and multi-page .tiff for 3D data, organized into folders named after their labels.\nReturns: None, saves images in the specified output directory if save_images is True.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nstr\n\nThe name of the MedMNIST dataset (e.g., ‘pathmnist’, ‘bloodmnist’, etc.).\n\n\noutput_dir\nstr\n.\nThe path to the directory where the datasets will be saved.\n\n\ndownload_only\nbool\nFalse\nIf True, only download the dataset into the output directory without processing.\n\n\nsave_images\nbool\nTrue\nIf True, save the images into the output directory as .png (2D datasets) or multipage .tiff (3D datasets) files.\n\n\n\n\nsource\n\n\nmedmnist2df\n\n medmnist2df (train_dataset, val_dataset=None, test_dataset=None,\n              mode='RGB')\n\n*Convert MedMNIST datasets to DataFrames, with images as PIL Image objects and labels as DataFrame columns.\nMissing datasets (if None) are represented by None in the return tuple.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_dataset\n\n\nMedMNIST training dataset with images and labels\n\n\nval_dataset\nNoneType\nNone\n(Optional) MedMNIST validation dataset with images and labels\n\n\ntest_dataset\nNoneType\nNone\n(Optional) MedMNIST test dataset with images and labels\n\n\nmode\nstr\nRGB\nMode for PIL Image conversion, e.g., ‘RGB’, ‘L’\n\n\nReturns\n(&lt;class ‘pandas.core.frame.DataFrame’&gt;, &lt;class ‘pandas.core.frame.DataFrame’&gt;, &lt;class ‘pandas.core.frame.DataFrame’&gt;)\n\n(df_train, df_val, df_test): DataFrames with columns ‘image’ and ‘label’\n\n\n\n\n\nDownload data via Pooch\n\nsource\n\n\ndownload_file\n\n download_file (url, output_dir='data', extract=True, hash=None,\n                extract_dir=None)\n\nDownload and optionally decompress a single file using Pooch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\n\n\nThe URL of the file to be downloaded\n\n\noutput_dir\nstr\ndata\nThe directory where the downloaded file will be saved\n\n\nextract\nbool\nTrue\nIf True, decompresses the file if it’s in a compressed format\n\n\nhash\nNoneType\nNone\nOptional: You can add a checksum for integrity verification\n\n\nextract_dir\nNoneType\nNone\nDirectory to extract the files to\n\n\n\n\nsource\n\n\ndownload_dataset\n\n download_dataset (base_url, expected_checksums, file_names, output_dir,\n                   processor=None)\n\nDownload a dataset using Pooch and save it to the specified output directory.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbase_url\n\n\nThe base URL from which the files will be downloaded.\n\n\nexpected_checksums\n\n\nA dictionary mapping file names to their expected checksums.\n\n\nfile_names\n\n\nA dictionary mapping task identifiers to file names.\n\n\noutput_dir\n\n\nThe directory where the downloaded files will be saved.\n\n\nprocessor\nNoneType\nNone\nA function to process the downloaded data.\n\n\n\n\nsource\n\n\ndownload_dataset_from_csv\n\n download_dataset_from_csv (csv_file, base_url, output_dir,\n                            processor=None, rows=None, prepend_mdf5=True)\n\nDownload a dataset using Pooch and save it to the specified output directory, reading file names and checksums from a CSV file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncsv_file\n\n\nPath to the CSV file containing file names and checksums.\n\n\nbase_url\n\n\nThe base URL from which the files will be downloaded.\n\n\noutput_dir\n\n\nThe directory where the downloaded files will be saved.\n\n\nprocessor\nNoneType\nNone\nA function to process the downloaded data.\n\n\nrows\nNoneType\nNone\nSpecific row indices to download. If None, download all rows.\n\n\nprepend_mdf5\nbool\nTrue\nIf True, prepend ‘md5:’ to the checksums.\n\n\n\n\n# Specify the directory where you want to save the downloaded files\noutput_directory = \"./_test_folder\"\n# Define the base URL for the MSD dataset\nbase_url = 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100888/'\n\ndownload_dataset_from_csv('./data_examples/FMD_dataset_info.csv', base_url, output_directory, rows=[6])\n\nThe dataset has been successfully downloaded and saved to: ./_test_folder\n\n\n\n\nDownload data via Quilt/T4\nAllen Institute Cell Science (AICS)\n\nsource\n\n\naics_pipeline\n\n aics_pipeline (n_images_to_download=40, image_save_dir=None,\n                col='SourceReadPath')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_images_to_download\nint\n40\nNumber of images to download\n\n\nimage_save_dir\nNoneType\nNone\nDirectory to save the images\n\n\ncol\nstr\nSourceReadPath\nColumn name for image paths in the data manifest\n\n\n\n\nimage_target_paths, data_manifest = aics_pipeline(1, \"../_data/aics\")\n\nLoading manifest: 100%|██████████| 77165/77165 [00:01&lt;00:00, 45.2k/s]\n\n\n\nprint(image_target_paths)\ndata_manifest.to_csv('../_data/aics/aics_dataset.csv')\n\n['../_data/aics/9e5d8f2e_3500001004_100X_20170623_5-Scene-1-P24-E06.czi_nucWholeIndexImageScale.tiff', '../_data/aics/77a69ff1_3500001004_100X_20170623_5-Scene-3-P26-F05.czi_nucWholeIndexImageScale.tiff']\n\n\n\nimage_target_paths, data_manifest = aics_pipeline(1, \"../_data/aics\", col=\"NucleusSegmentationReadPath\")\n\nLoading manifest: 100%|██████████| 77165/77165 [00:01&lt;00:00, 46.5k/s]\n100%|██████████| 491k/491k [00:02&lt;00:00, 171kB/s] \n\n\n\n\nDataset Manifest\nUtilities to make a list of all of the files of the train and test dataset in csv form.\n\nsource\n\n\nmanifest2csv\n\n manifest2csv (signal, target, paths=None, train_fraction=0.8,\n               data_save_path='./', train='train.csv', test='test.csv',\n               identifier=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsignal\n\n\nList of paths to signal images\n\n\ntarget\n\n\nList of paths to target images\n\n\npaths\nNoneType\nNone\nList of paths to images\n\n\ntrain_fraction\nfloat\n0.8\nFraction of data to use for training\n\n\ndata_save_path\nstr\n./\nPath to save the CSV files\n\n\ntrain\nstr\ntrain.csv\nName of the training CSV file\n\n\ntest\nstr\ntest.csv\nName of the test CSV file\n\n\nidentifier\nNoneType\nNone\nIdentifier to add to the paths\n\n\n\n\nmanifest2csv(data_manifest[\"ChannelNumberBrightfield\"],data_manifest[\"ChannelNumber405\"], image_target_paths, data_save_path='./data_examples/')\n\n\nsource\n\n\nsplit_dataframe\n\n split_dataframe (input_data, train_fraction=0.7, valid_fraction=0.1,\n                  split_column=None, stratify=False, add_is_valid=False,\n                  train_path='train.csv', test_path='test.csv',\n                  valid_path='valid.csv', data_save_path=None)\n\nSplits a DataFrame or CSV file into train, test, and optional validation sets.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_data\n\n\nPath to CSV file or DataFrame\n\n\ntrain_fraction\nfloat\n0.7\nProportion of data to use for the training set\n\n\nvalid_fraction\nfloat\n0.1\nProportion of data to use for the validation set\n\n\nsplit_column\nNoneType\nNone\nColumn name that indicates pre-defined split\n\n\nstratify\nbool\nFalse\nIf True, stratify by split_column during random split\n\n\nadd_is_valid\nbool\nFalse\nIf True, adds ‘is_valid’ column in the train set to mark validation samples\n\n\ntrain_path\nstr\ntrain.csv\nPath to save the training CSV file\n\n\ntest_path\nstr\ntest.csv\nPath to save the test CSV file\n\n\nvalid_path\nstr\nvalid.csv\nPath to save the validation CSV file\n\n\ndata_save_path\nNoneType\nNone\nPath to save the data files\n\n\n\n\nsource\n\n\nadd_columns_to_csv\n\n add_columns_to_csv (csv_path, column_data, output_path=None)\n\nAdds one or more new columns to an existing CSV file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncsv_path\n\n\nPath to the input CSV file\n\n\ncolumn_data\n\n\nDictionary of column names and values to add. Each value can be a scalar (single value for all rows) or a list matching the number of rows.\n\n\noutput_path\nNoneType\nNone\nPath to save the updated CSV file. If None, it overwrites the input CSV file."
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "# load sample data\ndata4D = img2float(cells3d())\ndata = data4D[:, 1, :, :] # load the nuclei channel\n\nDownloading file 'data/cells3d.tif' from 'https://gitlab.com/scikit-image/data/-/raw/master/cells3d.tif' to '/home/biagio/.var/app/com.visualstudio.code/cache/scikit-image/0.19.3'."
  },
  {
    "objectID": "visualize.html#display-2d-images",
    "href": "visualize.html#display-2d-images",
    "title": "Visualize",
    "section": "Display 2D images",
    "text": "Display 2D images\nFunction to quickly display 2D images\n\nsource\n\nplot_image\n\n plot_image (values)\n\nPlot a 2D image using Matplotlib. The function assumes that ‘values’ is a 2D array representing an image, typically in grayscale.\n\n\n\n\nDetails\n\n\n\n\nvalues\nA 2D array of pixel values representing the image.\n\n\n\n\n# Example usage:\nplot_image(data[35])"
  },
  {
    "objectID": "visualize.html#display-multichannel-images",
    "href": "visualize.html#display-multichannel-images",
    "title": "Visualize",
    "section": "Display Multichannel images",
    "text": "Display Multichannel images\nFunction to display RGB and multichannel images\n\nsource\n\nshow_multichannel\n\n show_multichannel (img, ax=None, figsize=None, title=None, max_slices=3,\n                    ctx=None, layout='horizontal', num_cols=3, cmap=None,\n                    norm=None, aspect=None, interpolation=None,\n                    alpha=None, vmin=None, vmax=None, origin=None,\n                    extent=None, interpolation_stage=None,\n                    filternorm=True, filterrad=4.0, resample=None,\n                    url=None, data=None, **kwargs)\n\nShow multi-channel CYX image with options for horizontal, square, or multi-row layout.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg\n\n\nA tensor or numpy array representing a multi-channel image.\n\n\nax\nNoneType\nNone\nThe Matplotlib axis to use for plotting.\n\n\nfigsize\nNoneType\nNone\nThe size of the figure.\n\n\ntitle\nNoneType\nNone\nThe title of the image.\n\n\nmax_slices\nint\n3\nThe maximum number of slices to display.\n\n\nctx\nNoneType\nNone\nThe context to use for plotting.\n\n\nlayout\nstr\nhorizontal\nThe layout type: ‘horizontal’, ‘square’, or ‘multirow’.\n\n\nnum_cols\nint\n3\nThe number of columns for the ‘multirow’ layout. Ignored for other layouts.\n\n\ncmap\nNoneType\nNone\n\n\n\nnorm\nNoneType\nNone\n\n\n\naspect\nNoneType\nNone\n\n\n\ninterpolation\nNoneType\nNone\n\n\n\nalpha\nNoneType\nNone\n\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\n\n\n\nextent\nNoneType\nNone\n\n\n\ninterpolation_stage\nNoneType\nNone\n\n\n\nfilternorm\nbool\nTrue\n\n\n\nfilterrad\nfloat\n4.0\n\n\n\nresample\nNoneType\nNone\n\n\n\nurl\nNoneType\nNone\n\n\n\ndata\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nprint(data4D[35].shape)\nshow_multichannel(data4D[35], cmap='gray', layout='multirow', num_cols=1);\n\n(2, 256, 256)"
  },
  {
    "objectID": "visualize.html#display-3d-images",
    "href": "visualize.html#display-3d-images",
    "title": "Visualize",
    "section": "Display 3D images",
    "text": "Display 3D images\nFunction to display 3D images\n\nsource\n\nmosaic_image_3d\n\n mosaic_image_3d (t:(&lt;class'numpy.ndarray'&gt;,&lt;class'torch.Tensor'&gt;),\n                  axis:int=0, figsize:tuple=(15, 15), cmap:str='gray',\n                  nrow:int=10, alpha=1.0, return_grid=False,\n                  add_to_existing=False, **kwargs)\n\nPlots 2D slices of a 3D image alongside a prior specified axis.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\n(&lt;class ‘numpy.ndarray’&gt;, &lt;class ‘torch.Tensor’&gt;)\n\n3D image to plot\n\n\naxis\nint\n0\naxis to split 3D array to 2D images\n\n\nfigsize\ntuple\n(15, 15)\nsize of the figure\n\n\ncmap\nstr\ngray\ncolormap to use\n\n\nnrow\nint\n10\nnumber of images per row\n\n\nalpha\nfloat\n1.0\ntransparency of the image\n\n\nreturn_grid\nbool\nFalse\nreturn the grid for further processing\n\n\nadd_to_existing\nbool\nFalse\nadd to existing figure\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nshow_images_grid\n\n show_images_grid (images, ax=None, ncols=10, figsize=None, title=None,\n                   spacing=0.02, max_slices=3, ctx=None, cmap=None,\n                   norm=None, aspect=None, interpolation=None, alpha=None,\n                   vmin=None, vmax=None, origin=None, extent=None,\n                   interpolation_stage=None, filternorm=True,\n                   filterrad=4.0, resample=None, url=None, data=None,\n                   **kwargs)\n\n*Show a list of images arranged in a grid.\nReturns: - axes: matplotlib axes containing the grid of images.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimages\n\n\nA list of images to display.\n\n\nax\nNoneType\nNone\nThe Matplotlib axis to use for plotting.\n\n\nncols\nint\n10\nThe number of columns in the grid.\n\n\nfigsize\nNoneType\nNone\nThe size of the figure.\n\n\ntitle\nNoneType\nNone\nThe title of the image.\n\n\nspacing\nfloat\n0.02\nThe spacing between subplots.\n\n\nmax_slices\nint\n3\nThe maximum number of slices to display.\n\n\nctx\nNoneType\nNone\nThe context to use for plotting.\n\n\ncmap\nNoneType\nNone\n\n\n\nnorm\nNoneType\nNone\n\n\n\naspect\nNoneType\nNone\n\n\n\ninterpolation\nNoneType\nNone\n\n\n\nalpha\nNoneType\nNone\n\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\n\n\n\nextent\nNoneType\nNone\n\n\n\ninterpolation_stage\nNoneType\nNone\n\n\n\nfilternorm\nbool\nTrue\n\n\n\nfilterrad\nfloat\n4.0\n\n\n\nresample\nNoneType\nNone\n\n\n\nurl\nNoneType\nNone\n\n\n\ndata\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nmosaic_image_3d(torch_from_numpy(data), figsize=None)\n\n\n\n\n\nshow_images_grid(data, cmap='gray');"
  },
  {
    "objectID": "visualize.html#show-slices",
    "href": "visualize.html#show-slices",
    "title": "Visualize",
    "section": "Show slices",
    "text": "Show slices\n\nsource\n\nshow_plane\n\n show_plane (ax, plane, cmap='gray', title=None, lines=None,\n             linestyle='--', linecolor='white')\n\nDisplay a slice of the image tensor on a given axis with optional dashed lines.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nax\n\n\nThe axis object to display the slice on.\n\n\nplane\n\n\nA 2D numpy array representing the slice of the image tensor.\n\n\ncmap\nstr\ngray\nColormap to use for displaying the image.\n\n\ntitle\nNoneType\nNone\nTitle for the plot.\n\n\nlines\nNoneType\nNone\nA list of indices where dashed lines should be drawn on the plane.\n\n\nlinestyle\nstr\n–\nThe style of the dashed lines.\n\n\nlinecolor\nstr\nwhite\nThe color of the dashed lines.\n\n\n\n\nsource\n\n\nvisualize_slices\n\n visualize_slices (data, planes=None, showlines=True, **kwargs)\n\nVisualize slices of a 3D image tensor along its planes, rows, and columns.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nA 3D numpy array representing the image tensor.\n\n\nplanes\nNoneType\nNone\nA tuple containing the indices of the planes to visualize.\n\n\nshowlines\nbool\nTrue\nWhether to show dashed lines on the planes, rows, and columns.\n\n\nkwargs\n\n\n\n\n\n\n\nvisualize_slices(data, showlines=False)\n\n\n\n\n\nvisualize_slices(data, (25,100,150), linestyle=':')\n\n\n\n\n\nsource\n\n\nslice_explorer\n\n slice_explorer (data, order='CZYX', **kwargs)\n\nVisualizes the provided data using Plotly’s interactive imshow function with animation support.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nA 3D numpy array representing the image tensor.\n\n\norder\nstr\nCZYX\nThe order of dimensions in the data.\n\n\nkwargs\n\n\n\n\n\n\n\nslice_explorer(data4D, order='ZCYX', title='Cells 3D')\n\n\n                                                \n\n\n\n\n\nstatic image of slice explorer\n\n\n\nsource\n\n\nplot_volume\n\n plot_volume (values, opacity=0.1, min=0.1, max=0.8, surface_count=5,\n              width=800, height=600)\n\nInteractive visualization of a 3D volume using Plotly. The function assumes that ‘values’ is a 3D array representing the volume data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalues\n\n\nA 3D array of pixel values representing the volume.\n\n\nopacity\nfloat\n0.1\nOpacity level for the surfaces in the volume plot.\n\n\nmin\nfloat\n0.1\nMinimum threshold multiplier for the visualization.\n\n\nmax\nfloat\n0.8\nMaximum threshold multiplier for the visualization.\n\n\nsurface_count\nint\n5\nNumber of surfaces to display in the volume plot.\n\n\nwidth\nint\n800\nWidth of the plotted figure.\n\n\nheight\nint\n600\nHeight of the plotted figure.\n\n\n\n\nplot_volume(data[:, 50:150, 50:150])\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nstatic image of interactive 3D plot"
  },
  {
    "objectID": "Tutorials/tutorial_classification3d.html",
    "href": "Tutorials/tutorial_classification3d.html",
    "title": "Image Classification 3D",
    "section": "",
    "text": "Setup imports\n\nfrom bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.losses import *\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_medmnist\nfrom bioMONAI.visualize import show_images_grid, mosaic_image_3d\nfrom bioMONAI.data import get_image_files\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\n\nDownload and store the dataset\nThese lines of code will download the SynapseMNIST3D dataset and set up the paths for training, validation, and test datasets.\nThe download_medmnist function is used to download the dataset, and the paths are organized to easily access different parts of the dataset for training, validation, and testing purposes. The data_path is updated to point to the ‘synapsemnist3d’ directory, and then separate paths are created for the ‘train’, ‘val’, and ‘test’ subdirectories. This organization helps in easily accessing the different parts of the dataset for training, validation, and testing purposes.\nYou can customize the data_flag to download different datasets available in the MedMNIST collection. Additionally, you can modify the data_path to change the location where the dataset is stored. If you have a specific directory structure in mind, ensure that the paths for train_path, val_path, and test_path correctly reflect your desired organization. This flexibility allows you to adapt the code to various datasets and storage requirements.\n\ndata_flag = 'synapsemnist3d'\ndata_path = Path('../_data/medmnist_data/')\n\ninfo = download_medmnist(data_flag, data_path, download_only=True)\n\ndata_path = data_path/'synapsemnist3d'\ntrain_path = data_path/'train'\nval_path = data_path/'val'\ntest_path = data_path/'test'\n\nDownloading https://zenodo.org/records/10519652/files/synapsemnist3d.npz?download=1 to ../_data/medmnist_data/synapsemnist3d/synapsemnist3d.npz\nUsing downloaded and verified file: ../_data/medmnist_data/synapsemnist3d/synapsemnist3d.npz\nUsing downloaded and verified file: ../_data/medmnist_data/synapsemnist3d/synapsemnist3d.npz\nSaving training images to ../_data/medmnist_data/synapsemnist3d...\nSaving validation images to ../_data/medmnist_data/synapsemnist3d...\nSaving test images to ../_data/medmnist_data/synapsemnist3d...\nRemoved synapsemnist3d.npz\nDatasets downloaded to ../_data/medmnist_data/synapsemnist3d\nDataset info for 'synapsemnist3d': {'python_class': 'SynapseMNIST3D', 'description': 'The SynapseMNIST3D is a new 3D volume dataset to classify whether a synapse is excitatory or inhibitory. It uses a 3D image volume of an adult rat acquired by a multi-beam scanning electron microscope. The original data is of the size 100×100×100um^3 and the resolution 8×8×30nm^3, where a (30um)^3 sub-volume was used in the MitoEM dataset with dense 3D mitochondria instance segmentation labels. Three neuroscience experts segment a pyramidal neuron within the whole volume and proofread all the synapses on this neuron with excitatory/inhibitory labels. For each labeled synaptic location, we crop a 3D volume of 1024×1024×1024nm^3 and resize it into 28×28×28 voxels. Finally, the dataset is randomly split with a ratio of 7:1:2 into training, validation and test set.', 'url': 'https://zenodo.org/records/10519652/files/synapsemnist3d.npz?download=1', 'MD5': '1235b78a3cd6280881dd7850a78eadb6', 'url_64': 'https://zenodo.org/records/10519652/files/synapsemnist3d_64.npz?download=1', 'MD5_64': '43bd14ebf3af9d3dd072446fedc14d5e', 'task': 'binary-class', 'label': {'0': 'inhibitory synapse', '1': 'excitatory synapse'}, 'n_channels': 1, 'n_samples': {'train': 1230, 'val': 177, 'test': 352}, 'license': 'CC BY 4.0'}\n\n\n100%|██████████| 38034583/38034583 [00:03&lt;00:00, 11376652.92it/s]\n100%|██████████| 1230/1230 [00:00&lt;00:00, 2872.15it/s]\n100%|██████████| 177/177 [00:00&lt;00:00, 2822.73it/s]\n100%|██████████| 352/352 [00:00&lt;00:00, 2847.26it/s]\n\n\n\n\nCreate Dataloader\n\n\nCustomize DataLoader\nIn the next cell, we will create a DataLoader for the SynapseMNIST3D dataset. The BioDataLoaders.class_from_folder function is used to load the dataset from the specified paths and apply transformations to the images.\nWe will set the batch size to 8 and apply the following transformations: - ScaleIntensity(): Scales the intensity of the images. - RandRot90(prob=0.5, spatial_axes=(1,2)): Randomly rotates the images by 90 degrees with a probability of 0.5 along the specified spatial axes. - Resize(32): Resizes the images to 32x32x32.\nYou can customize the DataLoader by changing the batch size, adding or removing transformations, or modifying the paths to the dataset. For example, you can increase the batch size for faster training or add more complex transformations to augment the dataset.\nThe show_summary parameter is set to True to display a summary of the dataset and transformations applied.\nAfter creating the DataLoader, we will print the number of training and validation images to verify that the dataset has been loaded correctly.\n\nParameters:\n\ndata_path: The root directory where the dataset is stored.\ntrain: The subdirectory containing the training images.\nvalid: The subdirectory containing the validation images.\nvocab: The vocabulary or labels for the dataset.\nitem_tfms: A list of transformations to apply to each image individually. Examples include ScaleIntensity(), RandRot90(), and Resize().\nbatch_tfms: A list of transformations to apply to a batch of images. This can be set to None if no batch transformations are needed.\nimg_cls: The class to use for loading images. For 3D images, this is typically BioImageStack.\nbs: The batch size, which determines how many images are processed together in each batch.\nshow_summary: A boolean flag to display a summary of the dataset and the transformations applied.\n\n\n\nExample Usage:\n\nbatch_size = 4\n\ndata = BioDataLoaders.class_from_folder(\n    data_path,                              # root directory for data\n    train='train',                          # folder for training data    \n    valid='val',                            # folder for validation data\n    vocab=info['label'],                    # list of class labels\n    item_tfms=[ScaleIntensity(),\n               RandRot90(prob=0.75, spatial_axes=(1,2)), \n               Resize(32)],                 # transformations to apply to each image\n    batch_tfms=None,                        # transformations to apply to each batch\n    img_cls=BioImageStack,                  # class to use for images\n    bs=batch_size,                          # batch size\n    show_summary=False,                      # print summary of the data\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 1230 \nvalidation images: 177\n\n\n\n\n\nDisplay a Batch of Images\nIn the next cell, we will display a batch of images from the training dataset using the show_batch method of the BioDataLoaders class. This method helps visualize the images and their corresponding labels, providing an overview of the dataset.\nYou can customize the display by modifying the following parameters: - max_n: The maximum number of images to display in the batch. By default, it shows all images in the batch. - nrows: The number of rows to use for displaying the images. This can be adjusted to control the layout of the images. - ncols: The number of columns to use for displaying the images. This can be adjusted to control the layout of the images. - figsize: The size of the figure used to display the images. This can be adjusted to make the images larger or smaller.\nFor example, you can set max_n=4 to display only 4 images from the batch, or set figsize=(10, 10) to increase the size of the displayed images.\nThis visualization step is useful for verifying that the images have been loaded and transformed correctly before proceeding with model training.\n\ndata.show_batch()\n\n\n\n\n\n\nLoad and train a 3D model\n\n\nTrain the Model\nIn the next cell, we will initialize and train a 3D model using the fastTrainer class. The model architecture used is SEResNet50, which is a 3D version of the SE-ResNet50 model. This model is well-suited for 3D image classification tasks.\nWe will use the following components: - SEResNet50: The model architecture with 3D spatial dimensions, 1 input channel, and 2 output classes. - CrossEntropyLossFlat: The loss function used for training the model. - BalancedAccuracy: The metric used to evaluate the model’s performance. - fastTrainer: A custom trainer class to handle the training process.\nThe trainer.fit(20) method will train the model for 20 epochs.\nYou can customize the training process by modifying the following parameters: - model: Change the model architecture to another 3D model, such as DenseNet169. - loss_fn: Use a different loss function, such as FocalLoss. - metrics: Add more metrics to evaluate the model, such as Precision or Recall. - show_summary: Set to False if you do not want to display the model summary. - find_lr: Set to False if you do not want to find the optimal learning rate.\nFor example, you can add more metrics to the metrics list to get a more comprehensive evaluation of the model’s performance.\n\nfrom monai.networks.nets import SEResNet50\nfrom fastai.vision.all import BalancedAccuracy, CrossEntropyLossFlat\n\nmodel = SEResNet50(spatial_dims=3, in_channels=1, num_classes=2)\n\nloss = CrossEntropyLossFlat()\nmetric = BalancedAccuracy()\n\ntrainer = fastTrainer(data, model, loss_fn=loss, metrics=metric, show_summary=True, find_lr=True)\n\n\n\n\n\n\n\n\nSEResNet50 (Input shape: 4 x 1 x 32 x 32 x 32)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     4 x 64 x 16 x 16 x  \nConv3d                                    21952      True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 64 x 8 x 8 x 8  \nMaxPool3d                                                      \nConv3d                                    4096       True      \nBatchNorm3d                               128        True      \nReLU                                                           \nConv3d                                    110592     True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 8 x 8 x 8 \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \n____________________________________________________________________________\n                     4 x 256 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 16              \nLinear                                    4112       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256             \nLinear                                    4352       True      \nSigmoid                                                        \n____________________________________________________________________________\n                     4 x 256 x 8 x 8 x 8 \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 64 x 8 x 8 x 8  \nConv3d                                    16384      True      \nBatchNorm3d                               128        True      \nReLU                                                           \nConv3d                                    110592     True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 8 x 8 x 8 \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \n____________________________________________________________________________\n                     4 x 256 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 16              \nLinear                                    4112       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256             \nLinear                                    4352       True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 64 x 8 x 8 x 8  \nConv3d                                    16384      True      \nBatchNorm3d                               128        True      \nReLU                                                           \nConv3d                                    110592     True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 8 x 8 x 8 \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \n____________________________________________________________________________\n                     4 x 256 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 16              \nLinear                                    4112       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256             \nLinear                                    4352       True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 128 x 4 x 4 x 4 \nConv3d                                    32768      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 32              \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512             \nLinear                                    16896      True      \nSigmoid                                                        \n____________________________________________________________________________\n                     4 x 512 x 4 x 4 x 4 \nConv3d                                    131072     True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 128 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 32              \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512             \nLinear                                    16896      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 128 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 32              \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512             \nLinear                                    16896      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 128 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 4 x 4 x 4 \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 32              \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512             \nLinear                                    16896      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 2 x 2 x 2 \nConv3d                                    131072     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     4 x 1024 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 64              \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024            \nLinear                                    66560      True      \nSigmoid                                                        \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    524288     True      \nBatchNorm3d                               2048       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 2 x 2 x 2 \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     4 x 1024 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 64              \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024            \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 2 x 2 x 2 \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     4 x 1024 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 64              \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024            \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 2 x 2 x 2 \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     4 x 1024 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 64              \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024            \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 2 x 2 x 2 \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     4 x 1024 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 64              \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024            \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 256 x 2 x 2 x 2 \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     4 x 1024 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 64              \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 1024            \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nConv3d                                    524288     True      \nBatchNorm3d                               1024       True      \nReLU                                                           \nConv3d                                    7077888    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 2048 x 1 x 1 x  \nConv3d                                    1048576    True      \nBatchNorm3d                               4096       True      \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 128             \nLinear                                    262272     True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 2048            \nLinear                                    264192     True      \nSigmoid                                                        \n____________________________________________________________________________\n                     4 x 2048 x 1 x 1 x  \nConv3d                                    2097152    True      \nBatchNorm3d                               4096       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nConv3d                                    1048576    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \nConv3d                                    7077888    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 2048 x 1 x 1 x  \nConv3d                                    1048576    True      \nBatchNorm3d                               4096       True      \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 128             \nLinear                                    262272     True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 2048            \nLinear                                    264192     True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     4 x 512 x 1 x 1 x 1 \nConv3d                                    1048576    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \nConv3d                                    7077888    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 2048 x 1 x 1 x  \nConv3d                                    1048576    True      \nBatchNorm3d                               4096       True      \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 128             \nLinear                                    262272     True      \nReLU                                                           \n____________________________________________________________________________\n                     4 x 2048            \nLinear                                    264192     True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     4 x 2               \nLinear                                    4098       True      \n____________________________________________________________________________\n\nTotal params: 48,690,162\nTotal trainable params: 48,690,162\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n  - ShowGraphCallback\nInferred learning rate:  0.0003\n\n\n\n\n\n\n\n\n\n\n\n\nYou can customize the training process by modifying the following parameters: - epochs: Change the number of epochs to train the model for a different duration. For example, you can set it to 50 or 100 epochs. - lr: Adjust the learning rate to control the speed at which the model learns. A lower learning rate can lead to more stable training, while a higher learning rate can speed up the process but may cause instability. - callbacks: Add custom callbacks to monitor the training process, such as early stopping or learning rate schedulers.\nFor example, you can add an early stopping callback to stop training if the validation loss does not improve for a certain number of epochs. This can help prevent overfitting and save training time.\nAdditionally, you can experiment with different learning rates to find the optimal value for your dataset. The find_lr parameter in the fastTrainer class can help you automatically find a suitable learning rate.\nBy customizing these parameters, you can fine-tune the training process to achieve better performance and adapt the model to your specific dataset and requirements.\n\ntrainer.fit(20)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbalanced_accuracy_score\ntime\n\n\n\n\n0\n0.737844\n0.884683\n0.522529\n00:09\n\n\n1\n0.668565\n0.655822\n0.537791\n00:10\n\n\n2\n0.661785\n0.654336\n0.492490\n00:09\n\n\n3\n0.691415\n0.677920\n0.579700\n00:09\n\n\n4\n0.656948\n0.604279\n0.537064\n00:09\n\n\n5\n0.592294\n0.580940\n0.574612\n00:09\n\n\n6\n0.622304\n0.540016\n0.583818\n00:09\n\n\n7\n0.567228\n0.543166\n0.578246\n00:09\n\n\n8\n0.611193\n0.579035\n0.514535\n00:09\n\n\n9\n0.592783\n0.724594\n0.500000\n00:09\n\n\n10\n0.547133\n0.560082\n0.500000\n00:09\n\n\n11\n0.574452\n0.585088\n0.500000\n00:09\n\n\n12\n0.566623\n0.580101\n0.564196\n00:09\n\n\n13\n0.619115\n0.552885\n0.492248\n00:09\n\n\n14\n0.554509\n0.579128\n0.500000\n00:09\n\n\n15\n0.575963\n0.636180\n0.492248\n00:09\n\n\n16\n0.539520\n0.724575\n0.596657\n00:09\n\n\n17\n0.574317\n0.690074\n0.538033\n00:09\n\n\n18\n0.524487\n0.503385\n0.642200\n00:09\n\n\n19\n0.538340\n0.576844\n0.571705\n00:09\n\n\n\n\n\n\n\n\n\n\nSave the Trained Model\nIn the next cell, we will save the trained model to a file using the save method of the fastTrainer class. This step is important to preserve the trained model so that it can be loaded and used later without retraining.\nYou can customize the saving process by modifying the following parameters: - file_name: Change the name of the file to save the model with a different name. For example, you can set it to ‘final_model’ or ‘best_model’. - path: Specify a different path to save the model in a different directory. This is useful if you want to organize your saved models in a specific folder.\nFor example, you can set file_name='final_model' to save the model with the name ‘final_model.pth’, or set path='../models/' to save the model in the ‘models’ directory.\nBy customizing these parameters, you can ensure that the model is saved with a meaningful name and in an organized manner, making it easier to manage and retrieve the model for future use.\n\n# trainer.save('tmp-model')\n\n\n\nEvaluate the Model on Test Data\nHere, we will evaluate the performance of the trained model on the test dataset. This step is crucial to understand how well the model generalizes to unseen data.\nWe will use the data.test_dl method to create a DataLoader for the test dataset. The get_image_files function is used to retrieve the test images from the specified path. The with_labels=True parameter ensures that the test images are loaded with their corresponding labels.\nAfter creating the test DataLoader, we will print the number of test images to verify that the dataset has been loaded correctly.\nYou can customize the evaluation process by modifying the following parameters: - test_path: Change the path to the test dataset if it is stored in a different location. - with_labels: Set to False if the test dataset does not have labels. This is useful for evaluating the model on unlabeled data. - batch_size: Adjust the batch size for the test DataLoader. A larger batch size can speed up the evaluation process but may require more memory.\nBy customizing these parameters, you can adapt the evaluation process to different datasets and requirements, ensuring that the model’s performance is accurately assessed.\n\ntest_data = data.test_dl(get_image_files(test_path), with_labels=True)\n# print length of test dataset\nprint('test images:', len(test_data))\n\ntest images: 88\n\n\nIn the next cell, we will evaluate the performance of the trained model on the test dataset using the evaluate_classification_model function. This function will compute various evaluation metrics and display the results.\nYou can customize the evaluation process by modifying the following parameters: - show_graph: Set to True to display a graph of the evaluation metrics. This can help visualize the model’s performance. - show_results: Set to True to display the results of the evaluation, including the predicted labels and ground truth labels.\nFor example, you can set show_graph=True to visualize the evaluation metrics, or set show_results=True to see the detailed results of the evaluation.\nBy customizing these parameters, you can gain a deeper understanding of the model’s performance and identify areas for improvement. This step is crucial for fine-tuning the model and ensuring that it generalizes well to unseen data.\n\nevaluate_classification_model(trainer, test_data, show_graph=False, show_results=False);\n\n\n\n\n\n\n    \n      \n      0.00% [0/88 00:00&lt;?]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.69      0.12      0.20        95\n           1       0.75      0.98      0.85       257\n\n    accuracy                           0.75       352\n   macro avg       0.72      0.55      0.52       352\nweighted avg       0.73      0.75      0.67       352\n\n\nMost Confused Classes:\n[('0', '1', 84), ('1', '0', 5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n0.581597\n\n\nMedian\n0.453715\n\n\nStandard Deviation\n0.254455\n\n\nMin\n0.314898\n\n\nMax\n1.300359\n\n\nQ1\n0.398424\n\n\nQ3\n0.711500"
  },
  {
    "objectID": "Tutorials/tutorial_denoising.html",
    "href": "Tutorials/tutorial_denoising.html",
    "title": "Denoising",
    "section": "",
    "text": "from bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.data import get_image_files, get_target, RandomSplitter\nfrom bioMONAI.losses import *\nfrom bioMONAI.losses import SSIMLoss\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_file\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\nDownload Data\nIn the next cell, we will download the dataset required for this tutorial. The dataset is hosted online, and we will use the download_file function from the bioMONAI library to download and extract the files.\n\n\nYou can change the output_directory variable to specify a different directory where you want to save the downloaded files.\nThe url variable contains the link to the dataset. If you have a different dataset, you can replace this URL with the link to your dataset.\nBy default, we are downloading only the first two images. You can modify the code to download more images if needed.\n\n\nMake sure you have enough storage space in the specified directory before downloading the dataset.\n\n# Specify the directory where you want to save the downloaded files\noutput_directory = \"../_data/U2OS\"\n# Define the base URL for the dataset\nurl = 'http://csbdeep.bioimagecomputing.com/example_data/snr_7_binning_2.zip'\n\n# Download only the first two images\ndownload_file(url, output_directory, extract=True)\n\nThe file has been downloaded and saved to: ../_data/U2OS\nDecompression (if needed) has been handled automatically.\n\n\n\n\nPrepare Data for Training\nIn the next cell, we will prepare the data for training. We will specify the path to the training images and define the batch size and patch size. Additionally, we will apply several transformations to the images to augment the dataset and improve the model’s robustness.\n\nX_path: The path to the directory containing the low-resolution training images.\nbs: The batch size, which determines the number of images processed together in one iteration.\npatch_size: The size of the patches to be extracted from the images.\nitemTfms: A list of item-level transformations applied to each image, including random cropping, rotation, and flipping.\nbatchTfms: A list of batch-level transformations applied to each batch of images, including intensity scaling.\nget_target_fn: A function to get the corresponding ground truth images for the low-resolution images.\n\n\nYou can customize the following parameters to suit your needs: - Change the X_path variable to point to a different dataset. - Adjust the bs and patch_size variables to match your hardware capabilities and model requirements. - Modify the transformations in itemTfms and batchTfms to include other augmentations or preprocessing steps.\n\nAfter defining these parameters and transformations, we will create a BioDataLoaders object to load the training and validation datasets.\n\nX_path = '../_data/U2OS/128a57f165e1044e34d9a6ef46e66b3c-snr_7_binning_2.zip.unzip/train/low/'\n\nbs = 32\npatch_size = 96\n\nitemTfms = [RandCropND(patch_size), RandRot90(prob=.75), RandFlip(prob=0.75)]\nbatchTfms = [ScaleIntensityPercentiles()]\n\nget_target_fn = get_target('GT', same_filename=True, relative_path=True)\n\ndata = BioDataLoaders.from_folder(\n    X_path,                 # input images\n    get_target_fn,          # target images\n    valid_pct=0.05,         # percentage of data for the validation set\n    seed=42,                # seed for random number generator  \n    item_tfms=itemTfms,     # item transformations\n    batch_tfms=batchTfms,   # batch transformations\n    show_summary=False,     # print summary of the dataset\n    bs = bs,                # batch size\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 2335 \nvalidation images: 122\n\n\n\n\nVisualize a Batch of Training Data\nIn the next cell, we will visualize a batch of training data to get an idea of what the images look like after applying the transformations. This step is crucial to ensure that the data augmentation and preprocessing steps are working as expected.\n\ndata.show_batch(cmap='magma'): This function will display a batch of images from the training dataset using the ‘magma’ colormap.\n\n\nChange the cmap parameter to use a different colormap (e.g., ‘gray’, ‘viridis’, ‘plasma’) based on your preference.\n\nVisualizing the data helps in understanding the dataset better and ensures that the transformations are applied correctly.\n\ndata.show_batch(cmap='magma')\n\n\n\n\n\n\nDefine and Train the Model\nIn the next cell, we will define a 2D U-Net model using the create_unet_model function from the bioMONAI library. The U-Net model is a popular architecture for image segmentation tasks, and it can be customized to suit various applications.\n\nresnet34: The backbone of the U-Net model. You can replace this with other backbones like resnet18, resnet50, etc., depending on your requirements.\n1: The number of output channels. For grayscale images, this should be set to 1. For RGB images, set it to 3.\n(128,128): The input size of the images. Adjust this based on the size of your input images.\nTrue: Whether to use pre-trained weights for the backbone. Set this to False if you want to train the model from scratch.\nn_in=1: The number of input channels. For grayscale images, this should be set to 1. For RGB images, set it to 3.\ncut=7: The layer at which to cut the backbone. Adjust this based on the architecture of the backbone.\n\n\nYou can customize the following parameters to suit your needs: - Change the backbone to a different architecture. - Adjust the input and output channels based on your dataset. - Modify the input size to match the dimensions of your images. - Set pretrained to False if you want to train the model from scratch.\n\nAfter defining the model, we will proceed to train it using the fastTrainer class. The training process involves fine-tuning the model for a specified number of epochs and evaluating its performance on the validation dataset.\n\nfrom bioMONAI.nets import create_unet_model, resnet34\n\nmodel = create_unet_model(resnet34, 1, (128,128), True, n_in=1, cut=7)\n\n\nloss = CombinedLoss(mse_weight=0.8, mae_weight=0.1)\n\nmetrics = [MSEMetric(), MAEMetric(), SSIMMetric(2)]\n\ntrainer = fastTrainer(data, model, loss_fn=loss, metrics=metrics, show_summary=False)\n\n\ntrainer.fine_tune(50, freeze_epochs=2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nMSE\nMAE\nSSIM\ntime\n\n\n\n\n0\n0.059263\n0.031750\n0.003062\n0.030812\n0.737810\n00:05\n\n\n1\n0.031509\n0.021351\n0.002592\n0.025890\n0.833123\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nMSE\nMAE\nSSIM\ntime\n\n\n\n\n0\n0.018390\n0.017519\n0.001561\n0.020792\n0.858087\n00:04\n\n\n1\n0.016879\n0.015520\n0.001512\n0.020095\n0.876995\n00:04\n\n\n2\n0.015920\n0.016230\n0.001825\n0.021673\n0.873977\n00:04\n\n\n3\n0.015772\n0.015743\n0.001493\n0.019981\n0.874495\n00:04\n\n\n4\n0.015445\n0.018726\n0.002763\n0.026642\n0.861489\n00:04\n\n\n5\n0.015966\n0.015048\n0.001492\n0.019624\n0.881081\n00:04\n\n\n6\n0.016098\n0.016459\n0.001995\n0.022954\n0.874330\n00:04\n\n\n7\n0.015651\n0.015040\n0.001728\n0.020668\n0.884089\n00:04\n\n\n8\n0.015609\n0.016344\n0.002146\n0.022793\n0.876518\n00:04\n\n\n9\n0.015790\n0.016388\n0.001928\n0.021907\n0.873452\n00:04\n\n\n10\n0.016008\n0.016197\n0.001503\n0.020361\n0.870417\n00:05\n\n\n11\n0.015512\n0.015503\n0.001408\n0.019778\n0.876012\n00:05\n\n\n12\n0.015767\n0.015337\n0.001368\n0.019256\n0.876830\n00:05\n\n\n13\n0.015257\n0.014446\n0.001362\n0.019063\n0.885507\n00:04\n\n\n14\n0.015756\n0.014684\n0.001553\n0.020141\n0.885727\n00:04\n\n\n15\n0.015227\n0.015024\n0.001379\n0.019199\n0.879995\n00:04\n\n\n16\n0.015261\n0.014982\n0.001360\n0.019180\n0.880241\n00:04\n\n\n17\n0.014247\n0.014851\n0.002221\n0.022847\n0.892102\n00:04\n\n\n18\n0.014091\n0.014845\n0.001407\n0.019615\n0.882418\n00:04\n\n\n19\n0.014043\n0.013648\n0.001350\n0.018536\n0.892850\n00:04\n\n\n20\n0.013710\n0.014114\n0.001372\n0.018851\n0.888687\n00:04\n\n\n21\n0.013437\n0.016465\n0.001930\n0.022347\n0.873133\n00:04\n\n\n22\n0.013622\n0.014885\n0.001425\n0.019470\n0.882027\n00:04\n\n\n23\n0.013829\n0.014458\n0.001389\n0.019248\n0.885781\n00:04\n\n\n24\n0.014047\n0.013161\n0.001478\n0.018917\n0.899137\n00:04\n\n\n25\n0.013911\n0.012902\n0.001281\n0.018061\n0.899292\n00:04\n\n\n26\n0.013629\n0.013000\n0.001469\n0.018982\n0.900730\n00:04\n\n\n27\n0.013198\n0.014055\n0.001335\n0.018980\n0.889103\n00:04\n\n\n28\n0.013180\n0.012791\n0.001315\n0.018064\n0.900674\n00:04\n\n\n29\n0.013422\n0.012379\n0.001298\n0.017815\n0.904412\n00:04\n\n\n30\n0.013412\n0.013191\n0.001374\n0.018449\n0.897529\n00:04\n\n\n31\n0.013132\n0.012420\n0.001276\n0.017726\n0.903737\n00:04\n\n\n32\n0.012796\n0.012543\n0.001240\n0.017730\n0.902221\n00:04\n\n\n33\n0.012817\n0.012291\n0.001316\n0.018162\n0.905779\n00:04\n\n\n34\n0.012658\n0.012130\n0.001254\n0.017662\n0.906392\n00:04\n\n\n35\n0.012476\n0.012438\n0.001278\n0.018083\n0.903923\n00:04\n\n\n36\n0.012751\n0.012415\n0.001341\n0.018224\n0.904799\n00:04\n\n\n37\n0.012631\n0.014831\n0.001759\n0.020762\n0.886519\n00:04\n\n\n38\n0.012579\n0.012348\n0.001227\n0.017694\n0.904028\n00:04\n\n\n39\n0.012365\n0.012506\n0.001264\n0.017780\n0.902831\n00:04\n\n\n40\n0.012290\n0.012359\n0.001237\n0.017511\n0.903814\n00:04\n\n\n41\n0.012234\n0.011951\n0.001238\n0.017431\n0.907826\n00:04\n\n\n42\n0.012528\n0.012128\n0.001226\n0.017376\n0.905901\n00:04\n\n\n43\n0.012046\n0.011911\n0.001287\n0.017585\n0.908764\n00:05\n\n\n44\n0.012103\n0.012053\n0.001222\n0.017326\n0.906574\n00:04\n\n\n45\n0.012201\n0.011962\n0.001235\n0.017346\n0.907602\n00:04\n\n\n46\n0.012174\n0.011858\n0.001234\n0.017341\n0.908638\n00:04\n\n\n47\n0.011766\n0.013627\n0.001352\n0.018609\n0.893159\n00:04\n\n\n48\n0.011853\n0.012117\n0.001251\n0.017631\n0.906461\n00:04\n\n\n49\n0.011875\n0.012058\n0.001255\n0.017596\n0.907052\n00:04\n\n\n\n\n\n\n\n\n\n\nShow Results\nIn the next cell, we will visualize the results of the trained model on a batch of validation data. This step helps in understanding how well the model has learned to denoise the images.\n\ntrainer.show_results(cmap='magma'): This function will display a batch of images from the validation dataset along with their corresponding denoised outputs using the ‘magma’ colormap.\n\nVisualizing the results helps in assessing the performance of the model and identifying any areas that may need further improvement.m\n\ntrainer.show_results(cmap='magma')\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave the Trained Model\nIn the next cell, we will save the trained model to a file. This step is crucial to preserve the model’s weights and architecture, allowing you to load and use the model later without retraining it.\n\ntrainer.save('tmp-model'): This function saves the model to a file named ‘tmp-model’. You can change the filename to something more descriptive based on your project.\n\n\nSuggestions for customization: - Change the filename to include details like the model architecture, dataset, or date (e.g., ‘unet_resnet34_U2OS_2023’). - Save the model in a specific directory by providing the full path (e.g., ‘models/unet_resnet34_U2OS_2023’). - Save additional information like training history, metrics, or configuration settings in a separate file for better reproducibility.\n\nSaving the model ensures that you can easily share it with others or deploy it in a production environment without needing to retrain it.\n\n# trainer.save('tmp-model')\n\n\n\nEvaluate the Model on Test Data\nIn the next cell, we will evaluate the performance of the trained model on unseen test data. This step is crucial to get an unbiased evaluation of the model’s performance and understand how well it generalizes to new data.\n\ntest_X_path: The path to the directory containing the low-resolution test images.\ntest_data: A DataLoader object created from the test images.\nevaluate_model(trainer, test_data, metrics=SSIMMetric(2)): This function evaluates the model on the test dataset using the specified metrics (in this case, SSIM).\n\n\nSuggestions for customization: - Change the test_X_path variable to point to a different test dataset. - Add more metrics to the metrics parameter to get a comprehensive evaluation (e.g., MSEMetric(), MAEMetric()). - Save the evaluation results to a file for further analysis or reporting.\n\nEvaluating the model on test data helps in understanding its performance in real-world scenarios and identifying any areas that may need further improvement.\n\ntest_X_path = '../_data/U2OS/128a57f165e1044e34d9a6ef46e66b3c-snr_7_binning_2.zip.unzip/test/low/'\n\ntest_data = data.test_dl(get_image_files(test_X_path), with_labels=True)\n\nevaluate_model(trainer, test_data, metrics=SSIMMetric(2));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCombinedLoss\n\n\n\n\n\nMean\n0.016833\n\n\nMedian\n0.013806\n\n\nStandard Deviation\n0.012330\n\n\nMin\n0.000871\n\n\nMax\n0.094672\n\n\nQ1\n0.010612\n\n\nQ3\n0.018312\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nSSIM\n\n\n\n\n\nMean\n0.860965\n\n\nMedian\n0.893404\n\n\nStandard Deviation\n0.118673\n\n\nMin\n0.104494\n\n\nMax\n0.992975\n\n\nQ1\n0.852688\n\n\nQ3\n0.918993"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\nPSNRMetric\n\n PSNRMetric (max_val, **kwargs)\n\n\nsource\n\n\nRMSEMetric\n\n RMSEMetric (**kwargs)\n\n\nsource\n\n\nMAEMetric\n\n MAEMetric (**kwargs)\n\n\nsource\n\n\nMSEMetric\n\n MSEMetric (**kwargs)\n\n\nsource\n\n\nSSIMMetric\n\n SSIMMetric (spatial_dims=3, **kwargs)\n\n\n\nFourier Ring Correlation\n\nRadial mask\n\nsource\n\n\n\nradial_mask\n\n radial_mask (r, cx=128, cy=128, sx=256, sy=256, delta=1)\n\n*Generate a radial mask.\nReturns: - numpy.ndarray: Radial mask.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nr\n\n\nRadius of the radial mask\n\n\ncx\nint\n128\nX coordinate mask center\n\n\ncy\nint\n128\nY coordinate maske center\n\n\nsx\nint\n256\nSize of the x-axis\n\n\nsy\nint\n256\nSize of the y-axis\n\n\ndelta\nint\n1\nThickness adjustment for the circular mask\n\n\n\n\nsource\n\n\nget_radial_masks\n\n get_radial_masks (width, height)\n\n*Generates a set of radial masks and corresponding to spatial frequencies.\nReturns: tuple: A tuple containing: - numpy.ndarray: Array of radial masks. - numpy.ndarray: Array of spatial frequencies corresponding to the masks.*\n\n\n\n\nDetails\n\n\n\n\nwidth\nWidth of the image\n\n\nheight\nHeight of the image\n\n\n\n\nFourier ring correlation\n\nsource\n\n\n\nget_fourier_ring_correlations\n\n get_fourier_ring_correlations (image1, image2)\n\n*Compute Fourier Ring Correlation (FRC) between two images.\nReturns: tuple: A tuple containing: - torch.Tensor: Fourier Ring Correlation values. - torch.Tensor: Array of spatial frequencies.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nFirst input image\n\n\nimage2\nSecond input image\n\n\n\n\nsource\n\n\nFRCMetric\n\n FRCMetric (image1, image2)\n\n*Compute the area under the Fourier Ring Correlation (FRC) curve between two images.\nReturns: - float: The area under the FRC curve.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nFirst input image\n\n\nimage2\nSecond input image"
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "Loss functions",
    "section": "",
    "text": "source\n\nMSELoss\n\n MSELoss (inp:Any, targ:Any)\n\n\nsource\n\n\nL1Loss\n\n L1Loss (inp:Any, targ:Any)\n\n\n\n\nSSIMLoss\n\n SSIMLoss (spatial_dims:int, data_range:float=1.0,\n           kernel_type:monai.metrics.regression.KernelType|str=gaussian,\n           win_size:int|collections.abc.Sequence[int]=11,\n           kernel_sigma:float|collections.abc.Sequence[float]=1.5,\n           k1:float=0.01, k2:float=0.03,\n           reduction:monai.utils.enums.LossReduction|str=mean)\n\n*Compute the loss function based on the Structural Similarity Index Measure (SSIM) Metric.\nFor more info, visit https://vicuesoft.com/glossary/term/ssim-ms-ssim/\nSSIM reference paper: Wang, Zhou, et al. “Image quality assessment: from error visibility to structural similarity.” IEEE transactions on image processing 13.4 (2004): 600-612.*\n\n\nCombined Losses\n\nsource\n\n\nCombinedLoss\n\n CombinedLoss (spatial_dims=2, mse_weight=0.33, mae_weight=0.33)\n\nlosses combined\n\nsource\n\n\nMSSSIMLoss\n\n MSSSIMLoss (spatial_dims=2, window_size:int=8, sigma:float=1.5,\n             reduction:str='mean', levels:int=3, weights=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions.\n\n\nwindow_size\nint\n8\nSize of the Gaussian filter for SSIM.\n\n\nsigma\nfloat\n1.5\nStandard deviation of the Gaussian filter.\n\n\nreduction\nstr\nmean\nSpecifies the reduction to apply to the output (‘mean’, ‘sum’, or ‘none’).\n\n\nlevels\nint\n3\nNumber of scales to use for MS-SSIM.\n\n\nweights\nNoneType\nNone\nWeights to apply to each scale. If None, default values are used.\n\n\n\n\nmsssim_loss = MSSSIMLoss(levels=3)\nssim_loss = SSIMLoss(2)\noutput = torch.rand(10, 3, 64, 64).cuda()  # Example output\ntarget = torch.rand(10, 3, 64, 64).cuda()  # Example target\nloss = msssim_loss(output, target)\nloss2 = ssim_loss(output,target)\nprint(\"ms-ssim: \",loss, '\\nssim: ', loss2)\n\nms-ssim:  tensor(0.9686, device='cuda:0') \nssim:  tensor(0.9949, device='cuda:0')\n\n\n\nsource\n\n\nMSSSIML1Loss\n\n MSSSIML1Loss (spatial_dims=2, alpha:float=0.025, window_size:int=8,\n               sigma:float=1.5, reduction:str='mean', levels:int=3,\n               weights=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions.\n\n\nalpha\nfloat\n0.025\nWeighting factor between MS-SSIM and L1 loss.\n\n\nwindow_size\nint\n8\nSize of the Gaussian filter for SSIM.\n\n\nsigma\nfloat\n1.5\nStandard deviation of the Gaussian filter.\n\n\nreduction\nstr\nmean\nSpecifies the reduction to apply to the output (‘mean’, ‘sum’, or ‘none’).\n\n\nlevels\nint\n3\nNumber of scales to use for MS-SSIM.\n\n\nweights\nNoneType\nNone\nWeights to apply to each scale. If None, default values are used.\n\n\n\n\nmsssiml1_loss = MSSSIML1Loss(alpha=0.025, window_size=11, sigma=1.5, levels=3)\ninput_image = torch.randn(4, 1, 128, 128)  # Batch of 4 grayscale images (1 channel)\ntarget_image = torch.randn(4, 1, 128, 128)\n\n# Compute MSSSIM + Gaussian-weighted L1 loss\nloss = msssiml1_loss(input_image, target_image)\nloss2 = ssim_loss(input_image, target_image)\nprint(\"ms-ssim: \", loss, '\\nssim: ', loss2)\n\nms-ssim:  tensor(0.0250) \nssim:  tensor(0.9955)\n\n\n\nsource\n\n\nMSSSIML2Loss\n\n MSSSIML2Loss (spatial_dims=2, alpha:float=0.1, window_size:int=11,\n               sigma:float=1.5, reduction:str='mean', levels:int=3,\n               weights=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions.\n\n\nalpha\nfloat\n0.1\nWeighting factor between MS-SSIM and L2 loss.\n\n\nwindow_size\nint\n11\nSize of the Gaussian window for SSIM.\n\n\nsigma\nfloat\n1.5\nStandard deviation of the Gaussian.\n\n\nreduction\nstr\nmean\nSpecifies the reduction to apply to the output (‘mean’, ‘sum’, or ‘none’).\n\n\nlevels\nint\n3\nNumber of scales to use for MS-SSIM.\n\n\nweights\nNoneType\nNone\nWeights to apply to each scale. If None, default values are used.\n\n\n\n\nmsssim_l2_loss = MSSSIML2Loss()\noutput = torch.rand(10, 3, 64, 64).cuda()  # Example output with even dimensions\ntarget = torch.rand(10, 3, 64, 64).cuda()  # Example target with even dimensions\nloss = msssim_l2_loss(output, target)\nprint(loss)\n\ntensor(0.0956, device='cuda:0')\n\n\n\n\nCrossEntropy and Dice Loss\n\nsource\n\n\nCrossEntropyLossFlat3D\n\n CrossEntropyLossFlat3D (*args, axis:int=-1, weight=None,\n                         ignore_index=-100, reduction='mean',\n                         flatten:bool=True, floatify:bool=False,\n                         is_2d:bool=True)\n\nSame as nn.CrossEntropyLoss, but flattens input and target for 3D inputs.\n\nsource\n\n\nDiceLoss\n\n DiceLoss (smooth=1)\n\n*DiceLoss computes the Sørensen–Dice coefficient loss, which is often used for evaluating the performance of image segmentation algorithms.\nThe Dice coefficient is a measure of overlap between two samples. It ranges from 0 (no overlap) to 1 (perfect overlap). The Dice loss is computed as 1 - Dice coefficient, so it ranges from 1 (no overlap) to 0 (perfect overlap).\nAttributes: smooth (float): A smoothing factor to avoid division by zero and ensure numerical stability.\nMethods: forward(inputs, targets): Computes the Dice loss between the predicted probabilities (inputs) and the ground truth (targets).*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsmooth\nint\n1\nSmoothing factor to avoid division by zero\n\n\n\n\n# inputs and targets must be equally dimensional tensors\nfrom torch import randn, randint\n\n\ninputs = randn((1, 1, 256, 256))  # Input\ntargets = randint(0, 2, (1, 1, 256, 256)).float()  # Ground Truth\n\n# Initialize\ndice_loss = DiceLoss()\n\n# Compute loss\nloss = dice_loss(inputs, targets)\nprint('Dice Loss:', loss.item())\n\nDice Loss: 0.4982335567474365\n\n\n\n\nFourier Ring Correlation\n\nsource\n\n\nFRCLoss\n\n FRCLoss (image1, image2)\n\n*Compute the Fourier Ring Correlation (FRC) loss between two images.\nReturns: - torch.Tensor: The FRC loss.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nThe first input image.\n\n\nimage2\nThe second input image.\n\n\n\n\nsource\n\n\nFCRCutoff\n\n FCRCutoff (image1, image2)\n\n*Calculate the cutoff frequency at when Fourier ring correlation drops to 1/7.\nReturns: - float: The cutoff frequency.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nThe first input image.\n\n\nimage2\nThe second input image."
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Transforms",
    "section": "",
    "text": "source\n\n\n\n Resample (sampling, **kwargs)\n\n*A subclass of Spacing that handles image resampling based on specified sampling factors or voxel dimensions.\nThe Resample class inherits from Spacing and provides a flexible way to adjust the spacing (voxel size) of images by specifying either a sampling factor or explicitly providing new voxel dimensions.*\n\n\n\n\nDetails\n\n\n\n\nsampling\nSampling factor for isotropic resampling\n\n\nkwargs\n\n\n\n\n\nfrom bioMONAI.core import cells3d, img2Tensor\nfrom bioMONAI.visualize import visualize_slices\n\n\nimg = BioImageStack(img2Tensor(cells3d()[:,0]))\nvisualize_slices(img, showlines=False)\n\nimg2 = Resample(4)(img)\nvisualize_slices(img2, showlines=False)\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n Resize (size=None, **kwargs)\n\n*A subclass of Reshape that handles image resizing based on specified target dimensions.\nThe Resize class inherits from Reshape and provides a flexible way to adjust the size of images by specifying either a target size or scaling factors.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nNoneType\nNone\nTarget dimensions for resizing (height, width). If its length is smaller than the spatial dimensions, values will be repeated. If an int is provided, it will be broadcast to all spatial dimensions.\n\n\nkwargs\n\n\n\n\n\n\n\nprint(img.size())\nvisualize_slices(img, showlines=False)\n\nimg2 = Resize(50)(img)\nprint(img2.size())\nvisualize_slices(img2, showlines=False)\n\ntorch.Size([60, 256, 256])\ntorch.Size([60, 50, 50])"
  },
  {
    "objectID": "transforms.html#size-sampling",
    "href": "transforms.html#size-sampling",
    "title": "Transforms",
    "section": "",
    "text": "source\n\n\n\n Resample (sampling, **kwargs)\n\n*A subclass of Spacing that handles image resampling based on specified sampling factors or voxel dimensions.\nThe Resample class inherits from Spacing and provides a flexible way to adjust the spacing (voxel size) of images by specifying either a sampling factor or explicitly providing new voxel dimensions.*\n\n\n\n\nDetails\n\n\n\n\nsampling\nSampling factor for isotropic resampling\n\n\nkwargs\n\n\n\n\n\nfrom bioMONAI.core import cells3d, img2Tensor\nfrom bioMONAI.visualize import visualize_slices\n\n\nimg = BioImageStack(img2Tensor(cells3d()[:,0]))\nvisualize_slices(img, showlines=False)\n\nimg2 = Resample(4)(img)\nvisualize_slices(img2, showlines=False)\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n Resize (size=None, **kwargs)\n\n*A subclass of Reshape that handles image resizing based on specified target dimensions.\nThe Resize class inherits from Reshape and provides a flexible way to adjust the size of images by specifying either a target size or scaling factors.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nNoneType\nNone\nTarget dimensions for resizing (height, width). If its length is smaller than the spatial dimensions, values will be repeated. If an int is provided, it will be broadcast to all spatial dimensions.\n\n\nkwargs\n\n\n\n\n\n\n\nprint(img.size())\nvisualize_slices(img, showlines=False)\n\nimg2 = Resize(50)(img)\nprint(img2.size())\nvisualize_slices(img2, showlines=False)\n\ntorch.Size([60, 256, 256])\ntorch.Size([60, 50, 50])"
  },
  {
    "objectID": "transforms.html#noise",
    "href": "transforms.html#noise",
    "title": "Transforms",
    "section": "Noise",
    "text": "Noise\n\nsource\n\nRandCameraNoise\n\n RandCameraNoise (p:float=1.0, damp=0.01, qe=0.7, gain=2, offset=100,\n                  exp_time=0.1, dark_current=0.6, readout=1.5,\n                  bitdepth=16, seed=42, simulation=False, camera='cmos',\n                  gain_variance=0.1, offset_variance=5)\n\n*Simulates camera noise by adding Poisson shot noise, dark current noise, and optionally CMOS fixed pattern noise.\nReturns: numpy.ndarray: The noisy image as a NumPy array with dimensions of input_image.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n1.0\nProbability of applying Transform\n\n\ndamp\nfloat\n0.01\nDampening factor to prevent saturation when adding noise\n\n\nqe\nfloat\n0.7\nQuantum efficiency of the camera (0 to 1).\n\n\ngain\nint\n2\nCamera gain factor. If an array, it should be broadcastable with input_image shape.\n\n\noffset\nint\n100\nCamera offset in ADU. If an array, it should be broadcastable with input_image shape.\n\n\nexp_time\nfloat\n0.1\nExposure time in seconds.\n\n\ndark_current\nfloat\n0.6\nDark current per pixel in electrons/second.\n\n\nreadout\nfloat\n1.5\nReadout noise standard deviation in electrons.\n\n\nbitdepth\nint\n16\nBit depth of the camera output.\n\n\nseed\nint\n42\nSeed for random number generator for reproducibility.\n\n\nsimulation\nbool\nFalse\nIf True, assumes input_image is already in units of photons and does not convert from electrons.\n\n\ncamera\nstr\ncmos\nSpecifies the type of camera (‘cmos’ or any other). Used to add CMOS fixed pattern noise if ‘cmos’ is specified.\n\n\ngain_variance\nfloat\n0.1\nVariance for the gain noise in CMOS cameras. Only applicable if camera type is ‘cmos’.\n\n\noffset_variance\nint\n5\nVariance for the offset noise in CMOS cameras. Only applicable if camera type is ‘cmos’.\n\n\n\n\nfrom bioMONAI.visualize import plot_image\n\n\nimg3 = img[30]\n\n\n# Original clean image\nplot_image(img3)\n\n\n\n\n\n# Noisy image simulating a CMOS camera\nplot_image(RandCameraNoise(camera = 'cmos').encodes(img3))\n\n\n\n\n\n# Noisy image simulating a CCD camera\nplot_image(RandCameraNoise(camera = 'ccd', readout=2).encodes(img3))"
  },
  {
    "objectID": "transforms.html#normalization",
    "href": "transforms.html#normalization",
    "title": "Transforms",
    "section": "Normalization",
    "text": "Normalization\n\nsource\n\nScaleIntensity\n\n ScaleIntensity (x, min=0.0, max=1.0, axis=None, eps=1e-20, dtype=&lt;class\n                 'numpy.float32'&gt;)\n\nImage normalization.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\nThe input image to scale.\n\n\nmin\nfloat\n0.0\nThe minimum intensity value.\n\n\nmax\nfloat\n1.0\nThe maximum intensity value.\n\n\naxis\nNoneType\nNone\nThe axis or axes along which to compute the minimum and maximum values.\n\n\neps\nfloat\n1e-20\nA small value to prevent division by zero.\n\n\ndtype\ntype\nfloat32\nThe data type to use for the output image.\n\n\n\n\nsource\n\n\nScaleIntensityPercentiles\n\n ScaleIntensityPercentiles (x, pmin=3, pmax=99.8, axis=None, clip=True,\n                            b_min=0.0, b_max=1.0, eps=1e-20, dtype=&lt;class\n                            'numpy.float32'&gt;)\n\nPercentile-based image normalization.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\nThe input image to scale.\n\n\npmin\nint\n3\nThe minimum percentile value.\n\n\npmax\nfloat\n99.8\nThe maximum percentile value.\n\n\naxis\nNoneType\nNone\nThe axis or axes along which to compute the minimum and maximum values.\n\n\nclip\nbool\nTrue\nIf True, clips the output values to the specified range.\n\n\nb_min\nfloat\n0.0\nThe minimum intensity value.\n\n\nb_max\nfloat\n1.0\nThe maximum intensity value.\n\n\neps\nfloat\n1e-20\nA small value to prevent division by zero.\n\n\ndtype\ntype\nfloat32\nThe data type to use for the output image.\n\n\n\n\nsource\n\n\nScaleIntensityVariance\n\n ScaleIntensityVariance (target_variance=1.0, ndim=2)\n\nScales the intensity variance of an ND image to a target value.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_variance\nfloat\n1.0\nThe desired variance for the scaled intensities.\n\n\nndim\nint\n2\nNumber of spatial dimensions in the image.\n\n\n\n\n# Example usage with a random tensor of shape (1, 3, 256, 256)\nrand_tensor = BioImageBase(torch.rand(1, 3, 256, 256))\n\ntransform = ScaleIntensityVariance(ndim=4)\n\n# Apply the transform to the tensor\nscaled_tensor = transform(rand_tensor)\n\nprint('Original Tensor Variance:', rand_tensor.var().item())\nprint('Scaled Tensor Variance:', scaled_tensor.var().item())\n\nOriginal Tensor Variance: 0.08352072536945343\nScaled Tensor Variance: 0.9999999403953552"
  },
  {
    "objectID": "transforms.html#data-augmentation",
    "href": "transforms.html#data-augmentation",
    "title": "Transforms",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nsource\n\nRandCrop2D\n\n RandCrop2D (size:int|tuple, lazy=False, **kwargs)\n\nRandomly crop an image to size\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint | tuple\n\nSize to crop to, duplicated if one value is specified\n\n\nlazy\nbool\nFalse\na flag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nRandCropND\n\n RandCropND (size:int|tuple, lazy=False, **kwargs)\n\n*Randomly crops an ND image to a specified size.\nThis transform randomly crops an ND image to a specified size during training and performs a center crop during validation. It supports both 2D and 3D images and videos, assuming the first dimension is the batch dimension.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint | tuple\n\nSize to crop to, duplicated if one value is specified\n\n\nlazy\nbool\nFalse\na flag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\n# Define a random tensor\norig_size = (65, 65)\nrand_tensor = BioImageBase(torch.rand(8, *orig_size))\n\nfor i in range(100):\n    test_eq((8,64,64),RandCropND((64,64))(rand_tensor).shape)\n\n\nsource\n\n\nRandFlip\n\n RandFlip (prob=0.1, spatial_axis=None, ndim=2, lazy=False, **kwargs)\n\nRandomly flips an ND image over a specified axis.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.1\nProbability of flipping\n\n\nspatial_axis\nNoneType\nNone\nSpatial axes along which to flip over. Default is None. The default axis=None will flip over all of the axes of the input array.\n\n\nndim\nint\n2\nNumber of spatial dimensions in the image\n\n\nlazy\nbool\nFalse\nFlag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\n# Define a random tensor\norig_size = (1,4,4)\nrand_tensor = BioImageBase(torch.rand(*orig_size))\n\nprint('orig tensor: ', rand_tensor, '\\n')\n\nfor i in range(3):\n    print(RandFlip(prob=.75, spatial_axis=None)(rand_tensor))\n\norig tensor:  metatensor([[[0.4648, 0.4394, 0.0145, 0.4412],\n         [0.2723, 0.8205, 0.4727, 0.9594],\n         [0.6661, 0.9815, 0.9765, 0.4414],\n         [0.2815, 0.6184, 0.0845, 0.0834]]]) \n\nmetatensor([[[0.2815, 0.6184, 0.0845, 0.0834],\n         [0.6661, 0.9815, 0.9765, 0.4414],\n         [0.2723, 0.8205, 0.4727, 0.9594],\n         [0.4648, 0.4394, 0.0145, 0.4412]]])\nmetatensor([[[0.0834, 0.0845, 0.6184, 0.2815],\n         [0.4414, 0.9765, 0.9815, 0.6661],\n         [0.9594, 0.4727, 0.8205, 0.2723],\n         [0.4412, 0.0145, 0.4394, 0.4648]]])\nmetatensor([[[0.4648, 0.4394, 0.0145, 0.4412],\n         [0.2723, 0.8205, 0.4727, 0.9594],\n         [0.6661, 0.9815, 0.9765, 0.4414],\n         [0.2815, 0.6184, 0.0845, 0.0834]]])\n\n\n\nsource\n\n\nRandRot90\n\n RandRot90 (prob=0.1, max_k=3, spatial_axes=(0, 1), ndim=2, lazy=False,\n            **kwargs)\n\nRandomly rotate an ND image by 90 degrees in the plane specified by axes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.1\nProbability of rotating\n\n\nmax_k\nint\n3\nMax number of times to rotate by 90 degrees\n\n\nspatial_axes\ntuple\n(0, 1)\nSpatial axes along which to rotate. Default: (0, 1), this is the first two axis in spatial dimensions.\n\n\nndim\nint\n2\n\n\n\nlazy\nbool\nFalse\nFlag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\n# Define a random tensor\norig_size = (1,4,4)\nrand_tensor = BioImageBase(torch.rand(*orig_size))\n\nprint('orig tensor: ', rand_tensor, '\\n')\n\nfor i in range(3):\n    print(RandRot90(prob=.75)(rand_tensor))\n\norig tensor:  metatensor([[[0.6143, 0.4423, 0.1837, 0.9712],\n         [0.5201, 0.4463, 0.2429, 0.4175],\n         [0.7642, 0.9752, 0.8923, 0.8719],\n         [0.0691, 0.7557, 0.9410, 0.0721]]]) \n\nmetatensor([[[0.9712, 0.4175, 0.8719, 0.0721],\n         [0.1837, 0.2429, 0.8923, 0.9410],\n         [0.4423, 0.4463, 0.9752, 0.7557],\n         [0.6143, 0.5201, 0.7642, 0.0691]]])\nmetatensor([[[0.9712, 0.4175, 0.8719, 0.0721],\n         [0.1837, 0.2429, 0.8923, 0.9410],\n         [0.4423, 0.4463, 0.9752, 0.7557],\n         [0.6143, 0.5201, 0.7642, 0.0691]]])\nmetatensor([[[0.0721, 0.9410, 0.7557, 0.0691],\n         [0.8719, 0.8923, 0.9752, 0.7642],\n         [0.4175, 0.2429, 0.4463, 0.5201],\n         [0.9712, 0.1837, 0.4423, 0.6143]]])"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "The module introduces specialized classes to represent various bioimaging data structures, facilitating seamless integration with machine learning workflows.\n\nsource\n\n\n\n MetaResolver (*args, **kwargs)\n\nThe MetaResolver class addresses metaclass conflicts, ensuring compatibility across different data structures. This is particularly useful when integrating with libraries that have specific metaclass requirements.\n\nsource\n\n\n\n\n BioImageBase (*args, **kwargs)\n\n*Serving as the foundational class for bioimaging data, BioImageBase provides core functionalities for image handling. It ensures that instances of specified types are appropriately cast to this class, maintaining consistency in data representation.\nMetaclass casts x to this class if it is of type cls._bypass_type.*\n\nsource\n\n\n\n\n BioImage (*args, **kwargs)\n\nA subclass of BioImageBase, the BioImage class is tailored for handling both 2D and 3D image objects. It offers methods to load images from various formats and provides access to image properties such as shape and dimensions.\n\na = BioImage.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageStack (*args, **kwargs)\n\nDesigned for 3D image data, BioImageStack extends BioImageBase to manage volumetric images effectively. It includes functionalities for slicing, visualization, and manipulation of 3D data.\n\na = BioImageStack.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageProject (*args, **kwargs)\n\nThe BioImageProject class represents a 3D image stack as a 2D image using maximum intensity projection. This is particularly useful for visualizing volumetric data in a 2D format, aiding in quick assessments and presentations.\n\na = BioImageProject.create('data_examples/example_tiff.tiff')\na.shape\n\ntorch.Size([1, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageMulti (*args, **kwargs)\n\nFor multi-channel 2D images, BioImageMulti extends BioImageBase to handle data with multiple channels, such as different fluorescence markers in microscopy images.\n\n# Load a 3D image stack as a multichannel image\na = BioImageMulti.create('data_examples/example_tiff.tiff')\n# Differently from BioImageStack, here the third dimension is encoded as channels.\nprint(a.shape)\n\ntorch.Size([96, 512, 512])"
  },
  {
    "objectID": "data.html#data-types",
    "href": "data.html#data-types",
    "title": "Data",
    "section": "",
    "text": "The module introduces specialized classes to represent various bioimaging data structures, facilitating seamless integration with machine learning workflows.\n\nsource\n\n\n\n MetaResolver (*args, **kwargs)\n\nThe MetaResolver class addresses metaclass conflicts, ensuring compatibility across different data structures. This is particularly useful when integrating with libraries that have specific metaclass requirements.\n\nsource\n\n\n\n\n BioImageBase (*args, **kwargs)\n\n*Serving as the foundational class for bioimaging data, BioImageBase provides core functionalities for image handling. It ensures that instances of specified types are appropriately cast to this class, maintaining consistency in data representation.\nMetaclass casts x to this class if it is of type cls._bypass_type.*\n\nsource\n\n\n\n\n BioImage (*args, **kwargs)\n\nA subclass of BioImageBase, the BioImage class is tailored for handling both 2D and 3D image objects. It offers methods to load images from various formats and provides access to image properties such as shape and dimensions.\n\na = BioImage.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageStack (*args, **kwargs)\n\nDesigned for 3D image data, BioImageStack extends BioImageBase to manage volumetric images effectively. It includes functionalities for slicing, visualization, and manipulation of 3D data.\n\na = BioImageStack.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageProject (*args, **kwargs)\n\nThe BioImageProject class represents a 3D image stack as a 2D image using maximum intensity projection. This is particularly useful for visualizing volumetric data in a 2D format, aiding in quick assessments and presentations.\n\na = BioImageProject.create('data_examples/example_tiff.tiff')\na.shape\n\ntorch.Size([1, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageMulti (*args, **kwargs)\n\nFor multi-channel 2D images, BioImageMulti extends BioImageBase to handle data with multiple channels, such as different fluorescence markers in microscopy images.\n\n# Load a 3D image stack as a multichannel image\na = BioImageMulti.create('data_examples/example_tiff.tiff')\n# Differently from BioImageStack, here the third dimension is encoded as channels.\nprint(a.shape)\n\ntorch.Size([96, 512, 512])"
  },
  {
    "objectID": "data.html#data-conversion",
    "href": "data.html#data-conversion",
    "title": "Data",
    "section": "Data conversion",
    "text": "Data conversion\nTo facilitate seamless integration between tensors and bioimaging data structures, the module provides conversion utilities.\n\nsource\n\nTensor2BioImage\n\n Tensor2BioImage (cls:__main__.BioImageBase=&lt;class\n                  '__main__.BioImageStack'&gt;)\n\nThe Tensor2BioImage transform converts tensors into BioImageBase instances, enabling the application of bioimaging-specific methods to tensor data. This is essential for integrating deep learning models with bioimaging workflows."
  },
  {
    "objectID": "data.html#data-blocks-and-dataloader",
    "href": "data.html#data-blocks-and-dataloader",
    "title": "Data",
    "section": "Data Blocks and Dataloader",
    "text": "Data Blocks and Dataloader\nThe module offers classes to construct data blocks and data loaders, streamlining the preparation of datasets for machine learning models.\n\nsource\n\nBioImageBlock\n\n BioImageBlock (cls:__main__.BioImageBase=&lt;class '__main__.BioImage'&gt;)\n\nA TransformBlock tailored for bioimaging data, BioImageBlock facilitates the creation of data processing pipelines, including transformations and augmentations specific to bioimaging.\n\nsource\n\n\nBioDataBlock\n\n BioDataBlock (blocks:list=(&lt;fastai.data.block.TransformBlock object at\n               0x7fa8e2496e90&gt;, &lt;fastai.data.block.TransformBlock object\n               at 0x7fa8d12a48d0&gt;), dl_type:fastai.data.core.TfmdDL=None,\n               get_items=&lt;function get_image_files&gt;, get_y=None,\n               get_x=None, getters:list=None, n_inp:int=None,\n               item_tfms:list=None, batch_tfms:list=None, **kwargs)\n\nThe BioDataBlock class serves as a generic container to build Datasets and DataLoaders efficiently. It integrates item and batch transformations, getters, and splitters, simplifying the setup of data pipelines for training and validation.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nblocks\nlist\n(&lt;fastai.data.block.TransformBlock object at 0x7fa8e2496e90&gt;, &lt;fastai.data.block.TransformBlock object at 0x7fa8d12a48d0&gt;)\nOne or more TransformBlocks\n\n\ndl_type\nTfmdDL\nNone\nTask specific TfmdDL, defaults to block’s dl_type orTfmdDL\n\n\nget_items\nfunction\nget_image_files\n\n\n\nget_y\nNoneType\nNone\n\n\n\nget_x\nNoneType\nNone\n\n\n\ngetters\nlist\nNone\nGetter functions applied to results of get_items\n\n\nn_inp\nint\nNone\nNumber of inputs\n\n\nitem_tfms\nlist\nNone\nItemTransforms, applied on an item\n\n\nbatch_tfms\nlist\nNone\nTransforms or RandTransforms, applied by batch\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nBioDataLoaders\n\n BioDataLoaders (*loaders, path:str|pathlib.Path='.', device=None)\n\nBasic wrapper around several DataLoaders with factory methods for biomedical imaging problems. Managing multiple DataLoader instances, BioDataLoaders handles data loading for different phases of model training, such as training, validation, and testing. It ensures efficient data handling and supports various batch processing strategies.\n\nsource\n\n\nBioDataLoaders.from_source\n\n BioDataLoaders.from_source (data_source, show_summary:bool=False,\n                             path:str|Path='.', bs:int=64,\n                             val_bs:int=None, shuffle:bool=True,\n                             device=None)\n\n*Create and return a DataLoader from a BioDataBlock using provided keyword arguments.\nReturns a DataLoader: A PyTorch DataLoader object populated with the data from the BioDataBlock. If show_summary is True, it also prints a summary of the datablock after creation.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\n\n\nThe source of the data to be loaded by the dataloader. This can be any type that is compatible with the dataloading method specified in kwargs (e.g., paths, datasets).\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.from_folder\n\n BioDataLoaders.from_folder (path, get_target_fn, train='train',\n                             valid='valid', valid_pct=None, seed=None,\n                             item_tfms=None, batch_tfms=None,\n                             img_cls=&lt;class '__main__.BioImage'&gt;,\n                             target_img_cls=&lt;class '__main__.BioImage'&gt;,\n                             show_summary:bool=False, bs:int=64,\n                             val_bs:int=None, shuffle:bool=True,\n                             device=None)\n\nCreate from dataset in path with train and valid subfolders (or provide valid_pct)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nget_target_fn\n\n\n\n\n\ntrain\nstr\ntrain\n\n\n\nvalid\nstr\nvalid\n\n\n\nvalid_pct\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\ntarget_img_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.from_df\n\n BioDataLoaders.from_df (df, path='.', valid_pct=0.2, seed=None, fn_col=0,\n                         folder=None, pref=None, suff='', target_col=1,\n                         target_folder=None, target_suff='',\n                         valid_col=None, item_tfms=None, batch_tfms=None,\n                         img_cls=&lt;class '__main__.BioImage'&gt;,\n                         target_img_cls=&lt;class '__main__.BioImage'&gt;,\n                         show_summary:bool=False, bs:int=64,\n                         val_bs:int=None, shuffle:bool=True, device=None)\n\nCreate from df using fn_col and target_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\npref\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\ntarget_col\nint\n1\n\n\n\ntarget_folder\nNoneType\nNone\n\n\n\ntarget_suff\nstr\n\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\ntarget_img_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.from_csv\n\n BioDataLoaders.from_csv (path, csv_fname='train.csv', header='path',\n                          delimiter=None, quoting=0, valid_pct=0.2,\n                          seed=None, fn_col=0, folder=None, pref=None,\n                          suff='', target_col=1, target_folder=None,\n                          target_suff='', valid_col=None, item_tfms=None,\n                          batch_tfms=None, img_cls=&lt;class\n                          '__main__.BioImage'&gt;, target_img_cls=&lt;class\n                          '__main__.BioImage'&gt;, show_summary:bool=False,\n                          bs:int=64, val_bs:int=None, shuffle:bool=True,\n                          device=None)\n\nCreate from path/csv_fname using fn_col and target_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\ncsv_fname\nstr\ntrain.csv\n\n\n\nheader\nstr\npath\n\n\n\ndelimiter\nNoneType\nNone\n\n\n\nquoting\nint\n0\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\npref\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\ntarget_col\nint\n1\n\n\n\ntarget_folder\nNoneType\nNone\n\n\n\ntarget_suff\nstr\n\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\ntarget_img_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_folder\n\n BioDataLoaders.class_from_folder (path, train='train', valid='valid',\n                                   valid_pct=None, seed=None, vocab=None,\n                                   item_tfms=None, batch_tfms=None,\n                                   img_cls=&lt;class '__main__.BioImage'&gt;,\n                                   show_summary:bool=False, bs:int=64,\n                                   val_bs:int=None, shuffle:bool=True,\n                                   device=None)\n\nCreate from dataset in path with train and valid subfolders (or provide valid_pct)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\ntrain\nstr\ntrain\n\n\n\nvalid\nstr\nvalid\n\n\n\nvalid_pct\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\n\n\n\nvocab\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_df\n\n BioDataLoaders.class_from_df (df, path='.', valid_pct=0.2, seed=None,\n                               fn_col=0, folder=None, suff='',\n                               label_col=1, label_delim=None,\n                               y_block=None, valid_col=None,\n                               item_tfms=None, batch_tfms=None,\n                               img_cls=&lt;class '__main__.BioImage'&gt;,\n                               show_summary:bool=False, bs:int=64,\n                               val_bs:int=None, shuffle:bool=True,\n                               device=None)\n\nCreate from df using fn_col and label_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\nlabel_col\nint\n1\n\n\n\nlabel_delim\nNoneType\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_csv\n\n BioDataLoaders.class_from_csv (path, csv_fname='labels.csv',\n                                header='infer', delimiter=None, quoting=0,\n                                valid_pct=0.2, seed=None, fn_col=0,\n                                folder=None, suff='', label_col=1,\n                                label_delim=None, y_block=None,\n                                valid_col=None, item_tfms=None,\n                                batch_tfms=None, img_cls=&lt;class\n                                '__main__.BioImage'&gt;,\n                                show_summary:bool=False, bs:int=64,\n                                val_bs:int=None, shuffle:bool=True,\n                                device=None)\n\nCreate from path/csv_fname using fn_col and label_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\ncsv_fname\nstr\nlabels.csv\n\n\n\nheader\nstr\ninfer\n\n\n\ndelimiter\nNoneType\nNone\n\n\n\nquoting\nint\n0\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\nlabel_col\nint\n1\n\n\n\nlabel_delim\nNoneType\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_lists\n\n BioDataLoaders.class_from_lists (path, fnames, labels, valid_pct=0.2,\n                                  seed:int=None, y_block=None,\n                                  item_tfms=None, batch_tfms=None,\n                                  img_cls=&lt;class '__main__.BioImage'&gt;,\n                                  show_summary:bool=False, bs:int=64,\n                                  val_bs:int=None, shuffle:bool=True,\n                                  device=None)\n\nCreate from list of fnames and labels in path\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nfnames\n\n\n\n\n\nlabels\n\n\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nint\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders"
  },
  {
    "objectID": "data.html#data-getters",
    "href": "data.html#data-getters",
    "title": "Data",
    "section": "Data getters",
    "text": "Data getters\nFunctions to retrieve specific data components are provided, aiding in the organization and preprocessing of datasets.\n\nsource\n\nget_gt\n\n get_gt (path_gt, gt_file_name='avg50.png')\n\n*The get_gt function retrieves ground truth data, essential for supervised learning tasks. It ensures that the correct labels or annotations are associated with each data sample.\nThis function constructs a path to a ground truth file based on the given path_gt and gt_file_name.\nIt uses a lambda function to create a new path by appending gt_file_name to the parent directory of the input file, as specified by path_gt.\nReturns a callable: A function that takes a single argument (a filename) and returns a Path object representing the full path to the ground truth file. When called with a filename, this function constructs the path by combining path_gt or the parent directory of the filename with gt_file_name.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath_gt\n\n\nThe base directory where the ground truth files are stored, or a file path from which to derive the parent directory.\n\n\ngt_file_name\nstr\navg50.png\nThe name of the ground truth file.\n\n\n\nSimilar to get_gt, the get_target function fetches target data for training, validation, or testing, facilitating the preparation of datasets for machine learning models.\n\nsource\n\n\nget_target\n\n get_target (path:str, same_filename=True, target_file_prefix='target',\n             signal_file_prefix='signal', relative_path=False)\n\n*Constructs and returns functions for generating file paths to “target” files based on given input parameters.\nThis function defines two nested helper functions within its scope:\n- `construct_target_filename(file_name)`: Constructs a target file name by inserting the specified prefix into the original file name.\n- `generate_target_path(file_name)`: Generates a path to the target file based on whether `same_filename` is set to True or False.\nThe main function returns the appropriate helper function based on the value of same_filename.\nReturns a callable: A function that takes a file name as input and returns its corresponding target file path based on the specified parameters.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nThe base directory where the files are located. This should be a string representing an absolute or relative path.\n\n\nsame_filename\nbool\nTrue\nIf True, the target file name will match the original file name; otherwise, it will use the specified prefix.\n\n\ntarget_file_prefix\nstr\ntarget\nThe prefix to insert into the target file name if same_filename is False.\n\n\nsignal_file_prefix\nstr\nsignal\nThe prefix used in the original file names that should be replaced by the target prefix.\n\n\nrelative_path\nbool\nFalse\nIf True, it indicates that the path is relative to the parent folder in the path where the input files are located.\n\n\n\n\nprint(get_target('train_folder/target', same_filename=False)('../signal/signal01.tif'))\nprint(get_target('target', relative_path=True)('../train_folder/signal/image01.tif'))\n\ntrain_folder/target/target01.tif\n../train_folder/target/image01.tif\n\n\n\nprint(get_target('GT', relative_path=True, same_filename=False, target_file_prefix=\"image_clean\", signal_file_prefix=\"image_noisy\")('train_folder/signal/image_noisy.tif'))\n\ntrain_folder/GT/image_clean.tif\n\n\nFor tasks involving denoising or noise analysis, get_noisy_pair retrieves pairs of clean and noisy data, enabling the training of models to learn noise reduction.\n\nsource\n\n\nget_noisy_pair\n\n get_noisy_pair (fn)\n\n*Get another “noisy” version of the input file by selecting a file from the same directory.\nThis function first retrieves all image files in the directory of the input file fn (excluding subdirectories). It then selects one of these files at random, ensuring that it is not the original file itself to avoid creating a trivial “noisy” pair.\nParameters:\nfn (Path or str): The path to the original image file. This should be a Path object but accepts string inputs for convenience.\nReturns:\nPath: A Path object pointing to the selected noisy file.*"
  },
  {
    "objectID": "data.html#data-display",
    "href": "data.html#data-display",
    "title": "Data",
    "section": "Data Display",
    "text": "Data Display\nVisualization functions are included to display batches of data and model results, aiding in qualitative assessments and debugging.\n\nshow_batch\n\nshow_batch (x:BioImageBase, y:BioImageBase, samples,\n            ctxs=None, max_n:int=10, nrows:int=None, ncols:int=None,\n            figsize:tuple=None, **kwargs)\n\nThe show_batch function visualizes a batch of data samples, allowing users to inspect the input data and verify preprocessing steps.\nReturns: List[Context]: A list of contexts after displaying the images and labels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nBioImageBase\n\nThe input image data.\n\n\ny\nBioImageBase\n\nThe target label data.\n\n\nsamples\n\n\nList of sample indices to display.\n\n\nctxs\nNoneType\n\nList of contexts for displaying images. If None, create new ones using get_grid().\n\n\nmax_n\nint\n10\nMaximum number of samples to display.\n\n\nnrows\nint\nNone\nNumber of rows in the grid if ctxs are not provided.\n\n\nncols\nint\nNone\nNumber of columns in the grid if ctxs are not provided.\n\n\nfigsize\ntuple\nNone\nFigure size for the image display.\n\n\nkwargs\n\n\nAdditional keyword arguments.\n\n\n\n\n\nshow_results\n\nshow_results (x: BioImageBase, y: BioImageBase, samples,\n              outs, ctxs=None, max_n=10, figsize=None, **kwargs)\n\nAfter model inference, show_results displays the model’s predictions alongside the ground truth, facilitating the evaluation of model performance.\nReturns:\nList[Context]: A list of contexts after displaying the images and labels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nBioImageBase\n\nThe input image data.\n\n\ny\nBioImageBase\n\nThe target label data.\n\n\nsamples\n\n\nList of sample indices to display.\n\n\nouts\n\n\nList of output predictions corresponding to the samples.\n\n\nctxs\nNoneType\n\nList of contexts for displaying images. If None, create new ones using get_grid().\n\n\nmax_n\nint\n10\n\n\n\nfigsize\ntuple\nNone\nFigure size for the image display.\n\n\nkwargs\n\n\nAdditional keyword arguments."
  },
  {
    "objectID": "data.html#preprocessing",
    "href": "data.html#preprocessing",
    "title": "Data",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe module provides functions for data preprocessing, including patch extraction and dimensionality reduction, essential for preparing data for machine learning models.\n\nsource\n\nextract_patches\n\n extract_patches (data, patch_size, overlap)\n\n*Extracts n-dimensional patches from the input data.\nReturns: - A list of patches as numpy arrays.*\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ndata\nnumpy array of the input data (n-dimensional).\n\n\npatch_size\ntuple of integers defining the size of the patches in each dimension.\n\n\noverlap\nfloat (between 0 and 1) indicating overlap between patches.\n\n\n\nThe extract_patches function divides images into smaller patches, which is useful for training models on localized regions of interest, especially when dealing with high-resolution images.\n\ndata = np.random.rand(100, 100, 3)  # Example 3D data\npatch_size = (64,64,2)\noverlap = 0.5\npatches = extract_patches(data, patch_size, overlap)\nprint(\"Number of generated patches:\", len(patches))\npatches[0].shape\n\nNumber of generated patches: 8\n\n\n(64, 64, 2)\n\n\n\nsource\n\n\nsave_patches_grid\n\n save_patches_grid (data_folder, gt_folder, output_folder, patch_size,\n                    overlap, threshold=None, squeeze_input=True,\n                    squeeze_patches=False, csv_output=True,\n                    train_test_split_ratio=0.8)\n\nLoads n-dimensional data from data_folder and gt_folder, generates patches, and saves them into individual HDF5 files. Each HDF5 file will have datasets with the structure X/patch_idx and y/patch_idx.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_folder\n\n\nPath to the folder containing data files (n-dimensional data).\n\n\ngt_folder\n\n\nPath to the folder containing ground truth (gt) files (n-dimensional data).\n\n\noutput_folder\n\n\nPath to the folder where the HDF5 files will be saved.\n\n\npatch_size\n\n\ntuple of integers defining the size of the patches.\n\n\noverlap\n\n\nfloat (between 0 and 1) defining the overlap between patches.\n\n\nthreshold\nNoneType\nNone\nIf provided, patches with a mean value below this threshold will be discarded.\n\n\nsqueeze_input\nbool\nTrue\n\n\n\nsqueeze_patches\nbool\nFalse\n\n\n\ncsv_output\nbool\nTrue\nIf True, a CSV file listing all patch paths is created.\n\n\ntrain_test_split_ratio\nfloat\n0.8\nRatio of data to split into train and test CSV files (e.g., 0.8 for 80% train).\n\n\n\nAfter extracting patches, save_patches_grid saves them in a grid format, facilitating visualization and inspection of the patches.\n\ndata_folder = './data_examples/Confocal_BPAE_B'\n# For the sake of simplicity, in this example we use the same folder for ground truth\ngt_folder = './data_examples/Confocal_BPAE_B' \noutput_folder = './_test'\npatch_size = (64,64)\noverlap = 0\nsave_patches_grid(data_folder, gt_folder, output_folder, patch_size, overlap, squeeze_input=True)\n\nProcessing files: 100%|██████████| 2/2 [00:00&lt;00:00, 35.05it/s]\n\n\nCSV files saved to: ./_test/train_patches.csv and ./_test/test_patches.csv\n\n\n\nfrom bioMONAI.io import hdf5_reader, split_hdf_path\nfrom bioMONAI.visualize import plot_image\n\n\nfile_path = './_test/HV110_P0500510000.h5/X/1'\n\nim , _ = hdf5_reader()(file_path)\nplot_image(im)\n\n\n\n\n\nsource\n\n\nextract_random_patches\n\n extract_random_patches (data, patch_size, num_patches)\n\n*Extracts a specified number of random n-dimensional patches from the input data.\nReturns: - A list of randomly cropped patches as numpy arrays.*\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ndata\nnumpy array of the input data (n-dimensional).\n\n\npatch_size\ntuple of integers defining the size of the patches in each dimension.\n\n\nnum_patches\nnumber of random patches to extract.\n\n\n\n\nsource\n\n\nsave_patches_random\n\n save_patches_random (data_folder, gt_folder, output_folder, patch_size,\n                      num_patches, threshold=None, squeeze_input=True,\n                      squeeze_patches=False, csv_output=True,\n                      train_test_split_ratio=0.8)\n\nLoads n-dimensional data from data_folder and gt_folder, generates random patches, and saves them into individual HDF5 files. Each HDF5 file will have datasets with the structure X/patch_idx and y/patch_idx.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_folder\n\n\nPath to the folder containing data files (n-dimensional data).\n\n\ngt_folder\n\n\nPath to the folder containing ground truth (gt) files (n-dimensional data).\n\n\noutput_folder\n\n\nPath to the folder where the HDF5 files will be saved.\n\n\npatch_size\n\n\ntuple of integers defining the size of the patches.\n\n\nnum_patches\n\n\nnumber of random patches to extract per file.\n\n\nthreshold\nNoneType\nNone\nIf provided, patches with a mean value below this threshold will be discarded.\n\n\nsqueeze_input\nbool\nTrue\nIf True, squeezes singleton dimensions in the input data.\n\n\nsqueeze_patches\nbool\nFalse\nIf True, squeezes singleton dimensions in the patches.\n\n\ncsv_output\nbool\nTrue\nIf True, a CSV file listing all patch paths is created.\n\n\ntrain_test_split_ratio\nfloat\n0.8\nRatio of data to split into train and test CSV files (e.g., 0.8 for 80% train).\n\n\n\n\ndata_folder = './data_examples/Confocal_BPAE_B' \ngt_folder = './data_examples/Confocal_BPAE_B' \noutput_folder = './_test2'\npatch_size = (64,64)\nnum_patches= 2\nsave_patches_random(data_folder, gt_folder, output_folder, patch_size, num_patches)\n\nProcessing files: 100%|██████████| 2/2 [00:00&lt;00:00, 62.06it/s]\n\n\nCSV files saved to: ./_test2/train_patches.csv and ./_test2/test_patches.csv\n\n\n\nfile_path = './_test2/HV110_P0500510000_random_patches.h5/X/1'\n\nim , _ = hdf5_reader()(file_path)\nplot_image(im)\n\n\n\n\n\nsource\n\n\ndict2string\n\n dict2string (d, item_sep='_', key_value_sep='', pad_zeroes=None)\n\n*Transforms a dictionary into a string with customizable separators and optional zero padding for integers.\nReturns the formatted dictionary as a string.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\n\n\nThe dictionary to convert.\n\n\nitem_sep\nstr\n_\nThe separator between dictionary items (default is “,”).\n\n\nkey_value_sep\nstr\n\nThe separator between keys and values (default is “:”).\n\n\npad_zeroes\nNoneType\nNone\nThe minimum width for integer values, padded with zeros. If None, no padding is applied.\n\n\n\n\nmy_dict = {'C': 2, 'Z': 30, 'S': 1}\nresult = dict2string(my_dict, pad_zeroes=3)\nprint(result)\n\nC002_Z030_S001\n\n\n\nsource\n\n\nremove_singleton_dims\n\n remove_singleton_dims (substack, order)\n\n*Remove dimensions with a size of 1 from both the substack and the order string.\nReturns:\nsubstack (np.array): The substack with singleton dimensions removed.\nnew_order (str): The updated dimension order string.*\n\n\n\n\nDetails\n\n\n\n\nsubstack\nThe extracted substack data.\n\n\norder\nThe dimension order string (e.g., ‘CZYX’).\n\n\n\n\nsource\n\n\nextract_substacks\n\n extract_substacks (input_file, output_dir=None, indices=None,\n                    split_dimension=None, squeeze_dims=True, *kwargs)\n\nExtract substacks from a multidimensional OME-TIFF stack using AICSImageIO.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_file\n\n\nPath to the input OME-TIFF file.\n\n\noutput_dir\nNoneType\nNone\nDirectory to save the extracted substacks. If a list, the substacks will be saved in the corresponding subdirectories from the list.\n\n\nindices\nNoneType\nNone\nA dictionary specifying which indices to extract. Keys can include ‘C’ for channel, ‘Z’ for z-slice, ‘T’ for time point, and ‘S’ for scene. If None, all indices are extracted.\n\n\nsplit_dimension\nNoneType\nNone\nDimension to split substacks along. If provided, separate substacks will be generated for each index in the split_dimension. Must be one of the keys in indices.\n\n\nsqueeze_dims\nbool\nTrue\nDimension to squeeze substacks along.\n\n\nkwargs\n\n\n\n\n\n\n\noutput_dir = \"./_test_folder/\"\nsubdirs = [output_dir + folder for folder in [\"channel_0\", \"channel_1\", \"channel_2\"]]\nsubdirs\n\n['./_test_folder/channel_0',\n './_test_folder/channel_1',\n './_test_folder/channel_2']\n\n\n\n[os.path.join([output_dir][0], f\"{subdirs[0]}_{ii}\") for ii in range(2)]\n\n['./_test_folder/./_test_folder/channel_0_0',\n './_test_folder/./_test_folder/channel_0_1']\n\n\n\nfilename = './data_examples/2155a4fe_3500000635_100X_20170227_E08_P21.ome.tiff'\n\n# This extracts a single substack for channel 0, z-slice 5, and time point 0.\nextract_substacks(filename, output_dir=output_dir, indices={\"C\": 0, \"Z\": range(35), \"T\": 0})\n\nExtracted substack saved to: ./_test_folder/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C0_Zrange(0, 35)_T0.ome.tiff\n\n\n\n# This extracts substacks for each channel (`C`) and saves them in separate subfolders named \"C_0\", \"C_1\", \"C_2\", etc.\nextract_substacks(filename, output_dir=[output_dir], indices={\"C\": [0, 1, 2], \"Z\": 5, \"T\": 0}, split_dimension=\"C\")\n\nExtracted substack saved to: ./_test_folder/C_0/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C0_Z5_T0.ome.tiff\nExtracted substack saved to: ./_test_folder/C_1/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C1_Z5_T0.ome.tiff\nExtracted substack saved to: ./_test_folder/C_2/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C2_Z5_T0.ome.tiff\n\n\n\n# This extracts substacks for each channel and saves them in directories \"channel_0\", \"channel_1\", and \"channel_2\".\nextract_substacks(filename, output_dir=subdirs, indices={\"C\": [0, 1, 2], \"Z\": 5}, split_dimension=\"C\")\n\nExtracted substack saved to: ./_test_folder/channel_0/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C0_Z5.ome.tiff\nExtracted substack saved to: ./_test_folder/channel_1/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C1_Z5.ome.tiff\nExtracted substack saved to: ./_test_folder/channel_2/2155a4fe_3500000635_100X_20170227_E08_P21_substack_C2_Z5.ome.tiff"
  },
  {
    "objectID": "Tutorials/tutorial_classification.html",
    "href": "Tutorials/tutorial_classification.html",
    "title": "Image Classification 2D",
    "section": "",
    "text": "Setup imports\n\nfrom bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.data import get_image_files\nfrom bioMONAI.losses import *\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_medmnist\n\nfrom fastai.vision.all import CategoryBlock, GrandparentSplitter, parent_label, resnet34, CrossEntropyLossFlat, accuracy\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\n\nDataset Information and Download\nWe’ll employ the publicly available BloodMNIST dataset. The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes.\n\nIn this step, we will download the BloodMNIST dataset using the download_medmnist function from bioMONAI. This function will download the dataset and provide information about it. The dataset will be stored in the specified path. You can customize the path or dataset name as needed. Additionally, you can explore other datasets available in the MedMNIST collection by changing the dataset name in the download_medmnist function.\n\n\nimage_path = '../_data/medmnist_data/'\ninfo = download_medmnist('bloodmnist', image_path, download_only=True)\n\nDataset 'bloodmnist' is already downloaded and available in '../_data/medmnist_data/bloodmnist'.\n\n\n\n\nCreate DataLoader\nIn this step, we will customize the DataLoader for the BloodMNIST dataset. The DataLoader is responsible for loading the data during training and validation. We will define the data loading strategy using the BioDataLoaders.from_source() method, configured with the arguments specified in data_ops. You can customize the following parameters to suit your needs:\n\nbatch_size: The number of samples per batch. Adjust this based on your GPU memory capacity.\nitem_tfms: List of item-level transformations to apply to the images. You can add or modify transformations to augment your dataset.\nsplitter: The method to split the dataset into training and validation sets. You can customize the split strategy if needed.\n\n\nFeel free to experiment with different configurations to improve model performance or adapt to different datasets.\n\n\nbatch_size = 32\n\npath = Path(image_path)/'bloodmnist'\npath_train = path/'train'\npath_val = path/'val'\n\ndata_ops = {\n    'blocks':       (BioImageBlock(cls=BioImageMulti), CategoryBlock(info['label'])),\n    'get_items':    get_image_files,\n    'get_y':        parent_label,\n    'splitter':     GrandparentSplitter(train_name='train', valid_name='val'),\n    'item_tfms':    [ScaleIntensity(min=0.0, max=1.0), RandRot90(prob=0.5), RandFlip(prob=0.5)],\n    'bs':           batch_size,\n}\n\ndata = BioDataLoaders.from_source(\n    path, \n    show_summary=False,\n    **data_ops,\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 11959 \nvalidation images: 1712\n\n\n\n\nVisualize a Batch of Images\nIn this step, we will visualize a batch of images from the BloodMNIST dataset using the show_batch method. This will help us understand the data distribution and verify the transformations applied to the images. The max_n parameter specifies the number of images to display.\n\n\nYou can adjust the max_n parameter to display more or fewer images.\nExperiment with different transformations in the item_tfms list to see their effects on the images.\nUse the show_batch method at different stages of your data pipeline to ensure the data is being processed correctly.\n\n\n\ndata.show_batch(max_n=4)\n\n\n\n\n\n\nTrain the Model\nIn this step, we will train the model using the visionTrainer class. The fine_tune method will be used to fine-tune the model for a specified number of epochs. The freeze_epochs parameter allows you to freeze the initial layers of the model for a certain number of epochs before unfreezing and training the entire model.\n\n\nYou can adjust the epochs parameter to train the model for more or fewer epochs based on your dataset and computational resources.\nExperiment with different values for freeze_epochs to see how it affects model performance.\nMonitor the training process and adjust the learning rate or other hyperparameters if needed.\nConsider using techniques like early stopping or learning rate scheduling to improve training efficiency and performance.\n\n\n\nVisionTrainer Class\nThe visionTrainer class is a high-level API designed to simplify the training process for vision models. It provides a convenient interface for training, fine-tuning, and evaluating deep learning models. Here are some key features and functionalities of the visionTrainer class:\n\nInitialization: The class is initialized with the data, model architecture, loss function, and metrics. It also provides options to display a summary of the model and data.\nFine-tuning: The fine_tune method allows you to fine-tune the model for a specified number of epochs. You can freeze the initial layers of the model for a certain number of epochs before unfreezing and training the entire model.\nTraining: The class handles the training loop, including forward and backward passes, loss computation, and optimization.\nEvaluation: The class provides methods to evaluate the model on validation and test datasets, compute metrics, and visualize results.\nCustomization: You can customize various aspects of the training process, such as learning rate, batch size, and data augmentations, to suit your specific needs.\n\n\nThe visionTrainer class is designed to streamline the training process, making it easier to experiment with different models and hyperparameters. It is particularly useful for tasks like image classification, where you can leverage pre-trained models and fine-tune them on your dataset.\n\n\nmodel = resnet34\n\nloss = CrossEntropyLossFlat()\nmetrics = accuracy\n\ntrainer = visionTrainer(data, model, loss_fn=loss, metrics=metrics, show_summary=False)\n\n\ntrainer.fine_tune(20, freeze_epochs=2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.708640\n0.683854\n0.777453\n00:10\n\n\n1\n0.576192\n0.405103\n0.858061\n00:10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.299481\n0.193486\n0.936332\n00:11\n\n\n1\n0.293008\n0.228729\n0.918224\n00:10\n\n\n2\n0.272829\n0.214455\n0.926402\n00:09\n\n\n3\n0.305058\n0.591883\n0.805491\n00:10\n\n\n4\n0.300107\n0.269342\n0.910631\n00:09\n\n\n5\n0.284960\n0.268985\n0.907710\n00:10\n\n\n6\n0.324666\n0.301217\n0.892523\n00:10\n\n\n7\n0.464664\n0.377951\n0.877921\n00:10\n\n\n8\n0.248325\n0.231458\n0.924650\n00:09\n\n\n9\n0.221637\n0.188274\n0.936332\n00:10\n\n\n10\n0.202730\n0.250833\n0.921729\n00:10\n\n\n11\n0.167961\n0.246925\n0.910631\n00:10\n\n\n12\n0.147837\n0.146140\n0.944509\n00:10\n\n\n13\n0.164445\n0.146480\n0.943341\n00:09\n\n\n14\n0.126141\n0.103653\n0.963785\n00:10\n\n\n15\n0.112292\n0.123060\n0.960864\n00:10\n\n\n16\n0.091364\n0.090764\n0.966121\n00:10\n\n\n17\n0.096191\n0.093275\n0.964953\n00:09\n\n\n18\n0.073260\n0.085704\n0.966706\n00:10\n\n\n19\n0.080190\n0.087858\n0.966706\n00:10\n\n\n\n\n\n\n\n\n\n\n\nEvaluate the Model on Validation Data\nIn this step, we will evaluate the trained model on the validation dataset using the evaluate_classification_model function. This function computes the specified metrics and provides insights into the model’s performance. Additionally, it can display the most confused classes to help identify areas for improvement.\n\n\nYou can customize the metrics parameter to include other evaluation metrics relevant to your task.\nThe most_confused_n parameter specifies the number of most confused classes to display. Adjust this value to see more or fewer confused classes.\nSet the show_graph parameter to True to visualize the confusion matrix and other evaluation graphs.\nUse this evaluation step to monitor the model’s performance and make necessary adjustments to the training process or data pipeline.\n\n\n\nevaluate_classification_model(trainer, metrics=metrics, most_confused_n=5, show_graph=False);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96       122\n           1       1.00      1.00      1.00       312\n           2       0.97      0.97      0.97       155\n           3       0.94      0.89      0.91       290\n           4       0.97      0.97      0.97       122\n           5       0.91      0.96      0.93       143\n           6       0.98      0.97      0.97       333\n           7       1.00      1.00      1.00       235\n\n    accuracy                           0.97      1712\n   macro avg       0.96      0.97      0.96      1712\nweighted avg       0.97      0.97      0.97      1712\n\n\nMost Confused Classes:\n[('3', '5', 12), ('6', '3', 9), ('3', '0', 8), ('3', '6', 6), ('5', '3', 6)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n1.313160\n\n\nMedian\n1.274517\n\n\nStandard Deviation\n0.144572\n\n\nMin\n1.274009\n\n\nMax\n2.273876\n\n\nQ1\n1.274059\n\n\nQ3\n1.279382\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\naccuracy\n\n\n\n\n\nMean\n0.966706\n\n\nMedian\n1.000000\n\n\nStandard Deviation\n0.179404\n\n\nMin\n0.000000\n\n\nMax\n1.000000\n\n\nQ1\n1.000000\n\n\nQ3\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave the model\n\n\nSave the Model\nIn this step, we will save the trained model using the save method of the visionTrainer class. Saving the model allows us to reuse it later without retraining. This is particularly useful when you want to deploy the model or continue training at a later time.\n\n\nYou can specify the file path and name for the saved model. Ensure the directory exists or create it if necessary.\nConsider saving the model at different checkpoints during training to have backups and the ability to revert to a previous state if needed.\nYou can also save additional information such as the training history, optimizer state, and hyperparameters to facilitate future use or further training.\n\n\n\n# trainer.save('tmp-model')\n\n\n\nEvaluate the Model on Test Data\nIn this step, we will evaluate the trained model on the test dataset to assess its performance on unseen data. This is a crucial step to ensure that the model generalizes well and performs accurately on new, unseen samples. We will use the evaluate_classification_model function to compute the specified metrics and gain insights into the model’s performance.\n\n\nEnsure that the test dataset is completely separate from the training and validation datasets to get an unbiased evaluation.\nYou can customize the metrics parameter to include other evaluation metrics relevant to your task.\nThe show_graph parameter can be set to True to visualize the confusion matrix and other evaluation graphs.\nUse this evaluation step to identify any potential issues with the model and make necessary adjustments to the training process or data pipeline.\nConsider experimenting with different model architectures, hyperparameters, and data augmentations to further improve performance.\n\n\n\npath_test = path/'test'\n\ntest_data = data.test_dl(get_image_files(path_test).shuffle(), with_labels=True)\n# print length of test dataset\nprint('test images:', len(test_data))\n\ntest images: 107\n\n\n\nevaluate_classification_model(trainer, test_data, metrics=metrics, show_graph=False);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95       244\n           1       1.00      1.00      1.00       624\n           2       0.98      0.97      0.98       311\n           3       0.91      0.93      0.92       579\n           4       0.95      0.95      0.95       243\n           5       0.90      0.92      0.91       284\n           6       0.98      0.97      0.97       666\n           7       1.00      1.00      1.00       470\n\n    accuracy                           0.96      3421\n   macro avg       0.96      0.96      0.96      3421\nweighted avg       0.96      0.96      0.96      3421\n\n\nMost Confused Classes:\n[('3', '5', 22), ('6', '3', 20), ('5', '3', 15), ('3', '6', 10), ('0', '3', 9), ('5', '4', 7), ('2', '3', 6), ('3', '0', 5), ('4', '3', 5), ('0', '5', 4), ('3', '4', 4), ('4', '5', 4), ('4', '2', 3), ('5', '0', 2), ('0', '4', 1), ('0', '6', 1), ('1', '0', 1), ('2', '1', 1), ('2', '4', 1), ('2', '6', 1), ('3', '2', 1), ('4', '0', 1), ('6', '1', 1), ('6', '2', 1)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n1.319366\n\n\nMedian\n1.274616\n\n\nStandard Deviation\n0.156896\n\n\nMin\n1.274009\n\n\nMax\n2.274005\n\n\nQ1\n1.274069\n\n\nQ3\n1.280444\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\naccuracy\n\n\n\n\n\nMean\n0.963169\n\n\nMedian\n1.000000\n\n\nStandard Deviation\n0.188348\n\n\nMin\n0.000000\n\n\nMax\n1.000000\n\n\nQ1\n1.000000\n\n\nQ3\n1.000000"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html",
    "href": "Tutorials/tutorial_multispectral_classification.html",
    "title": "Multispectral Classification",
    "section": "",
    "text": "from bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.data import *\nfrom bioMONAI.nets import BasicUNet, DynUNet\nfrom bioMONAI.losses import *\nfrom bioMONAI.losses import SSIMLoss\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_file, split_dataframe, add_columns_to_csv\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndevice = get_device()\nprint(device)\n\ncuda"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#download-dataset",
    "href": "Tutorials/tutorial_multispectral_classification.html#download-dataset",
    "title": "Multispectral Classification",
    "section": "Download dataset",
    "text": "Download dataset\nIn the next cell, we will download a subset of the RXRX1 dataset from the MONAI repository. This dataset contains multispectral images that we will use for our classification task. The download_file function is used to download and extract the dataset to a specified directory.\n\nThe dataset URL is specified, and a hash is provided to ensure data integrity.\nThe extract parameter is set to True to automatically extract the downloaded zip file.\nThe extract_dir parameter is left empty, meaning the contents will be extracted to the specified directory.\n\n\n\nYou can change the url variable to point to a different dataset if needed.\nModify the extract_dir parameter to specify a different extraction directory.\nEnsure that the hash value matches the dataset you are downloading to avoid data corruption issues.\n\n\n\n# Define the base URL for the dataset\nurl = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/rxrx1_subset_monai.zip\"\n\ndownload_file(url, \"../_data\", extract=True, hash='e80db433db641bb390ade991b81f98814a26c7de30e0da6f20e8abddf7a84538', extract_dir='')\n\nDownloading data from 'https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/rxrx1_subset_monai.zip' to file '/home/biagio/Code/bioMONAI/nbs/_data/420070ba36287636b45f69ca979afd7a-rxrx1_subset_monai.zip'.\nUnzipping contents of '/home/biagio/Code/bioMONAI/nbs/_data/420070ba36287636b45f69ca979afd7a-rxrx1_subset_monai.zip' to '/home/biagio/Code/bioMONAI/nbs/_data/'\n\n\nThe file has been downloaded and saved to: ../_data\nExtracted files have been saved to: ../_data"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#prepare-image-paths-and-update-metadata",
    "href": "Tutorials/tutorial_multispectral_classification.html#prepare-image-paths-and-update-metadata",
    "title": "Multispectral Classification",
    "section": "Prepare Image Paths and Update Metadata",
    "text": "Prepare Image Paths and Update Metadata\nIn the next cell, we will prepare the image paths for each channel and update the metadata CSV file with these paths. This step is crucial for organizing the dataset and ensuring that each image is correctly associated with its corresponding metadata.\n\nWe will read the metadata CSV file and extract the site IDs.\nFor each site ID, we will generate the paths for the six channels of images.\nThese paths will be stored in a dictionary and added as new columns to the metadata CSV file.\nA new CSV file will be created to avoid overwriting the original metadata file.\n\n\n\nYou can modify the data_folder and csv_file variables to point to a different dataset or metadata file.\nIf your dataset contains a different number of channels, adjust the range in the channel_list generation accordingly.\nEnsure that the directory structure and file naming conventions match those expected by the code.\n\n\n\ndata_folder = '../_data/rxrx1_subset_monai/'\ncsv_file = data_folder + 'metadata.csv'\n\nimport pandas as pd\ndf = pd.read_csv(csv_file)\n\nch1, ch2, ch3, ch4, ch5,ch6 = [],[],[],[],[],[]\nfor sid in df['site_id']: \n    site_id = sid.split('_')\n    base_image_path = os.path.join('images', site_id[0], f'Plate{site_id[1]}', f'{site_id[2]}_s{site_id[3]}_w')\n    channel_list = [f'{base_image_path}{i}.png' for i in range(1,7)]\n    ch1.append(channel_list[0])\n    ch2.append(channel_list[1])\n    ch3.append(channel_list[2])\n    ch4.append(channel_list[3])\n    ch5.append(channel_list[4])\n    ch6.append(channel_list[5])\nimage_paths = {'channel 1': ch1, 'channel 2': ch2, 'channel 3': ch3, 'channel 4': ch4, 'channel 5': ch5, 'channel 6': ch6}\n# Let's create a new csv file to avoid overwriting the original one\nnew_csv_file = data_folder + 'metadata_updated.csv'\nadd_columns_to_csv(csv_file, image_paths, new_csv_file)\n\nColumns ['channel 1', 'channel 2', 'channel 3', 'channel 4', 'channel 5', 'channel 6'] added successfully. Updated file saved to '../_data/rxrx1_subset_monai/metadata_updated.csv'"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#split-dataset-into-train-validation-and-test-sets",
    "href": "Tutorials/tutorial_multispectral_classification.html#split-dataset-into-train-validation-and-test-sets",
    "title": "Multispectral Classification",
    "section": "Split Dataset into Train, Validation, and Test Sets",
    "text": "Split Dataset into Train, Validation, and Test Sets\nIn the next cell, we will split the updated metadata CSV file into training, validation, and test sets. This step is essential for training and evaluating our classification model. The split_dataframe function is used to perform the split based on the specified fractions.\n\nThe train_fraction parameter determines the proportion of the dataset to be used for training.\nThe valid_fraction parameter determines the proportion of the dataset to be used for validation.\nThe split_column parameter specifies the column to be used for splitting the dataset.\nThe add_is_valid parameter adds a column to indicate whether a sample belongs to the validation set.\nThe train_path, test_path, and valid_path parameters specify the file paths for the resulting CSV files.\nThe data_save_path parameter specifies the directory where the CSV files will be saved.\n\n\n\nYou can adjust the train_fraction and valid_fraction parameters to change the proportions of the splits.\nModify the split_column parameter if you want to use a different column for splitting.\nEnsure that the data_save_path directory exists and has write permissions.\n\n\n\nsplit_dataframe(new_csv_file, \n                train_fraction=0.7, \n                valid_fraction=0.05, \n                split_column='dataset', \n                add_is_valid=True, \n                train_path=\"train.csv\", \n                test_path=\"test.csv\", \n                valid_path=\"valid.csv\", \n                data_save_path=data_folder\n                )\n\nTrain and test files saved as '../_data/rxrx1_subset_monai/train.csv' and '../_data/rxrx1_subset_monai/test.csv' respectively.\n'is_valid' column added to '../_data/rxrx1_subset_monai/train.csv' for validation samples."
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#data-augmentation-and-dataloader-preparation",
    "href": "Tutorials/tutorial_multispectral_classification.html#data-augmentation-and-dataloader-preparation",
    "title": "Multispectral Classification",
    "section": "Data Augmentation and DataLoader Preparation",
    "text": "Data Augmentation and DataLoader Preparation\nIn the next cell, we will define the data augmentation techniques and prepare the data loaders for training and validation. Data augmentation is crucial for improving the generalization of our model by artificially increasing the diversity of the training dataset. We will use a combination of intensity scaling, random cropping, rotation, and flipping transformations.\n\nThe ScaleIntensityPercentiles transformation scales the intensity values of the images based on the specified percentiles.\nThe RandomResizedCrop transformation randomly crops the images to the specified size with a random scale.\nThe RandRot90 transformation randomly rotates the images by 90 degrees with the specified probability.\nThe RandFlip transformation randomly flips the images horizontally or vertically with the specified probability.\nThe BioDataLoaders.class_from_csv function is used to create the data loaders from the CSV file containing the image paths and labels.\n\n\n\nYou can adjust the bs variable to change the batch size.\nModify the parameters of the transformations to experiment with different augmentation techniques.\nEnsure that the fn_col and label_col parameters match the columns in your CSV file.\nSet show_summary to True to display a summary of the data loaders.\n\n\n\nfrom fastai.vision.all import RandomResizedCrop\n\nbs = 8\n\nitemTfms = [ScaleIntensityPercentiles(1,99), RandomResizedCrop(512,min_scale=0.9, max_scale=1.1), RandRot90(prob=.75), RandFlip(prob=0.5)]\nbatchTfms = []\n\ndata = BioDataLoaders.class_from_csv(\n    data_folder,\n    'train.csv',\n    fn_col=[12,13,14,15,16,17],\n    label_col=3,\n    valid_col=-1,\n    seed=42, \n    img_cls=BioImageMulti,\n    item_tfms=itemTfms,\n    batch_tfms=batchTfms, \n    show_summary=False,\n    bs = bs,\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 929 \nvalidation images: 71"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#visualize-data-batch",
    "href": "Tutorials/tutorial_multispectral_classification.html#visualize-data-batch",
    "title": "Multispectral Classification",
    "section": "Visualize Data Batch",
    "text": "Visualize Data Batch\nIn the next cell, we will visualize a batch of images from the training dataset. This step is essential for verifying that the data augmentation techniques are applied correctly and that the images are loaded as expected. The show_batch method of the BioDataLoaders class is used to display a batch of images with their corresponding labels.\n\nThe max_slices parameter specifies the maximum number of slices to display for each image.\nThe layout parameter determines the layout of the displayed images. The ‘multirow’ layout arranges the images in multiple rows.\n\n\n\nYou can adjust the max_slices parameter to display more or fewer slices per image.\nModify the layout parameter to experiment with different layouts, such as ‘single’ or ‘grid’.\nEnsure that the data loaders are correctly defined and contain the expected images and labels.\n\n\n\ndata.show_batch(max_slices=6, layout='multirow')"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#visualize-a-specific-image",
    "href": "Tutorials/tutorial_multispectral_classification.html#visualize-a-specific-image",
    "title": "Multispectral Classification",
    "section": "Visualize a Specific Image",
    "text": "Visualize a Specific Image\nIn the next cell, we will visualize a specific image from the dataset using its index. This step is useful for inspecting individual images and verifying their quality and labels. The do_item method of the BioDataLoaders class is used to retrieve the image and its label, and the show method is used to display the image.\n\na = data.do_item(100)\na[0].show(max_slices=6, layout='multirow');"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#define-and-train-the-model",
    "href": "Tutorials/tutorial_multispectral_classification.html#define-and-train-the-model",
    "title": "Multispectral Classification",
    "section": "Define and Train the Model",
    "text": "Define and Train the Model\nIn the next cell, we will define and train a DenseNet169 model for our multispectral classification task. The model is initialized with the following parameters: - spatial_dims=2: Specifies that the input images are 2D. - in_channels=6: Specifies the number of input channels, which corresponds to the six multispectral channels. - out_channels=data.c: Specifies the number of output channels, which corresponds to the number of classes in our dataset. - pretrained=True: Initializes the model with pretrained weights.\nWe will also define the metrics to evaluate the model’s performance during training. The RocAuc and accuracy metrics are used to measure the model’s performance.\nThe fastTrainer class is used to train the model with the specified data loaders and metrics. The fine_tune method is called to fine-tune the model for a specified number of epochs, with an initial phase of freezing the pretrained layers.\n\n\nYou can experiment with different model architectures by replacing DenseNet169 with other models from the monai.networks.nets module.\nAdjust the in_channels parameter if your dataset contains a different number of channels.\nModify the out_channels parameter if your dataset has a different number of classes.\nExperiment with different metrics by adding or removing metrics from the metrics list.\nAdjust the number of epochs and the freeze_epochs parameter to control the training process.\n\n\n\nfrom monai.networks.nets import DenseNet169\n\nmodel = DenseNet169(spatial_dims=2, in_channels=6, out_channels=data.c, pretrained=True)\n\n\nfrom fastai.vision.all import RocAuc, accuracy\nmetrics = [RocAuc(), accuracy]\n\ntrainer = fastTrainer(data, model, metrics=metrics, show_summary=False)\n\n\ntrainer.fine_tune(4, freeze_epochs=2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.559674\n9.052959\n0.772129\n0.492958\n00:21\n\n\n1\n0.633247\n0.418777\n0.968444\n0.845070\n00:14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n0\n0.399818\n0.763117\n0.975527\n0.887324\n00:14\n\n\n1\n0.404023\n0.498850\n0.985939\n0.901408\n00:14\n\n\n2\n0.247609\n0.053720\n1.000000\n0.985915\n00:14\n\n\n3\n0.182609\n0.046695\n1.000000\n0.985915\n00:14"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#save-the-trained-model",
    "href": "Tutorials/tutorial_multispectral_classification.html#save-the-trained-model",
    "title": "Multispectral Classification",
    "section": "Save the Trained Model",
    "text": "Save the Trained Model\nIn the next cell, we will save the trained model to a file. This step is crucial for preserving the model’s state after training, allowing us to load and use the model later without retraining. The save method of the fastTrainer class is used to save the model to the specified file path.\n\nThe save method takes the file name as an argument and saves the model’s state dictionary to a file with the .pth extension.\nThe saved model can be loaded later using the load method of the fastTrainer class.\n\n\n\nYou can change the file name to save the model with a different name.\nEnsure that the directory where the model is saved exists and has write permissions.\nConsider saving multiple versions of the model during training to keep track of different checkpoints.\n\n\n\ntrainer.save('multispectral-classification-model')\n\nPath('../_data/rxrx1_subset_monai/models/multispectral-classification-model.pth')"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html#evaluate-the-model-on-test-data",
    "href": "Tutorials/tutorial_multispectral_classification.html#evaluate-the-model-on-test-data",
    "title": "Multispectral Classification",
    "section": "Evaluate the Model on Test Data",
    "text": "Evaluate the Model on Test Data\nIn the next cell, we will evaluate the trained model on the test dataset. This step is crucial for assessing the model’s performance on unseen data and understanding its generalization capabilities. The BioDataLoaders.class_from_csv function is used to create the data loader for the test dataset, and the evaluate_classification_model function is used to compute the evaluation metrics.\n\nThe fn_col parameter specifies the columns containing the file paths for the multispectral channels.\nThe label_col parameter specifies the column containing the labels.\nThe valid_pct parameter is set to 0, indicating that no validation split is needed for the test dataset.\nThe item_tfms parameter applies the ScaleIntensityPercentiles transformation to the test images.\nThe batch_tfms parameter applies any batch-level transformations (if defined).\nThe bs parameter specifies the batch size for loading the test data.\nThe evaluate_classification_model function takes the trained model, test data loader, and evaluation metrics as inputs and returns the computed scores.\n\n\n\nYou can adjust the bs variable to change the batch size for loading the test data.\nModify the fn_col and label_col parameters to match the columns in your test CSV file.\nAdd or remove transformations in the item_tfms and batch_tfms lists to experiment with different preprocessing techniques.\nSet show_graph to True to visualize the evaluation results.\n\n\n\ntest_data = BioDataLoaders.class_from_csv(\n    data_folder,\n    'test.csv',\n    fn_col=[12,13,14,15,16,17],\n    label_col=3,\n    valid_pct=0,\n    seed=42, \n    img_cls=BioImageMulti,\n    item_tfms=[ScaleIntensityPercentiles(1,99)],\n    batch_tfms=batchTfms, \n    show_summary=False,\n    bs = 50,\n    )\n\n\nscores = evaluate_classification_model(trainer, test_data, metrics=accuracy, show_graph=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n       HEPG2       0.56      0.78      0.65        50\n       HUVEC       0.94      0.90      0.92        50\n         RPE       0.67      0.94      0.78        50\n        U2OS       1.00      0.24      0.39        50\n\n    accuracy                           0.71       200\n   macro avg       0.79      0.72      0.68       200\nweighted avg       0.79      0.71      0.68       200\n\n\nMost Confused Classes:\n[('U2OS', 'HEPG2', 31), ('HEPG2', 'RPE', 11), ('U2OS', 'RPE', 7), ('HUVEC', 'RPE', 5), ('RPE', 'HUVEC', 3)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n1.013312\n\n\nMedian\n0.796461\n\n\nStandard Deviation\n0.357889\n\n\nMin\n0.743670\n\n\nMax\n1.743668\n\n\nQ1\n0.747447\n\n\nQ3\n1.290924\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\naccuracy\n\n\n\n\n\nMean\n0.715000\n\n\nMedian\n1.000000\n\n\nStandard Deviation\n0.451414\n\n\nMin\n0.000000\n\n\nMax\n1.000000\n\n\nQ1\n0.000000\n\n\nQ3\n1.000000"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "bioMONAI",
    "section": "Overview",
    "text": "Overview\nbioMONAI is a low-code Python-based platform for developing and deploying deep learning models in biomedical imaging built on top of the MONAI framework, fastai, and TorchIO. This project aims to facilitate interoperability, reproducibility, and community collaboration in biomedical research."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "bioMONAI",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nbioMONAI\n\nOverview\nTable of Contents\nInstallation\nGetting Started\nUsage\nContributing\nLicense\nContact"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "bioMONAI",
    "section": "Installation",
    "text": "Installation\nTo install the bioMONAI environment, follow these steps:\n\nClone the repository:\ngit clone https://github.com/deepclem/biomonai.git\ncd biomonai\nCreate a new Conda environment and install dependencies:\nconda env create --file bioMONAI-env.yml\nActivate the environment and install MONAI:\nconda activate bioMONAI-env\npip install -e ."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "bioMONAI",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started with bioMONAI, we recommend exploring our tutorials, which guide you through model training for various tasks such as classification and denoising.\n\n\n\nNotebook\nOpen in Colab\n\n\n\n\nTutorial: Classification 2D  This notebook provides a comprehensive guide on training deep learning models for 2D image classification tasks, covering data loading, preprocessing, model building, training, and evaluation.\n\n\n\nTutorial: Denoising 2D  This notebook offers a detailed guide on applying deep learning techniques to denoise biological microscopy images. It covers data preparation, model architecture, training processes, and evaluation methods, providing a comprehensive resource for enhancing image quality in biological research."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "bioMONAI",
    "section": "Usage",
    "text": "Usage\nTo use bioMONAI for your own projects, follow these steps:\n\nCreate a new Jupyter notebook or open an existing one.\nImport necessary modules:\nimport bioMONAI\nStart coding! You can now leverage MONAI’s capabilities alongside the interactive features of Jupyter notebooks."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "bioMONAI",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions from the community! To contribute to BioMONAI nbs, follow these steps:\n\nFork the repository on GitHub.\nClone your fork:\ngit clone https://github.com/your_username/biomonai.git\ncd biomonai\nCreate a new Conda environment and install dependencies:\nconda env create --file bioMONAI-env.yml\nCreate a new branch for your changes:\ngit checkout -b feature/new-feature\nActivate the environment and install MONAI in dev mode:\nconda activate bioMONAI-env\npip install -e .[dev]\nMake your changes and commit them:\ngit add .\ngit commit -m \"Add new feature: &lt;feature description&gt;\"\nPush to your fork and create a pull request on GitHub.\nWait for the review, and merge if everything looks good!"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "bioMONAI",
    "section": "License",
    "text": "License\nbioMONAI is released under the Apache 2.0 license. See LICENSE for more details."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "bioMONAI",
    "section": "Contact",
    "text": "Contact\nIf you have any questions or need further assistance, please open an issue on GitHub or contact us directly at:\n\nProject Lead: Biagio Mandracchia\nContributors: Sara Cruz-Adrados, Juan Pita-López, Rosa-María Menchón-Lara"
  },
  {
    "objectID": "io.html#image-writers",
    "href": "io.html#image-writers",
    "title": "I/O",
    "section": "Image Writers",
    "text": "Image Writers\n\nsource\n\nwrite_image\n\n write_image (data, file_path, dimension_order='TCZYX')\n\n*Writes an image to a file.\n:param data: Image data (numpy array, tensor, or AICSImage) :param file_path: Path to save the image :param format: Format to save the image in (default is png)*\n\n# Example usage:\nnumpy_array = np.random.rand(3, 100, 100)\nwrite_image(numpy_array, './data_examples/output_from_numpy.tiff')\n\ntensor = torch.rand(3, 100, 100)\nwrite_image(tensor, './data_examples/output_from_tensor.tiff')\n\naics_image = AICSImage('./data_examples/example_tiff.tiff')\nwrite_image(aics_image, './data_examples/output_from_tiff.png')\n\nImage successfully saved to ./data_examples/output_from_numpy.tiff\nImage successfully saved to ./data_examples/output_from_tensor.tiff\nImage successfully saved to ./data_examples/output_from_tiff.png"
  },
  {
    "objectID": "io.html#image-readers",
    "href": "io.html#image-readers",
    "title": "I/O",
    "section": "Image Readers",
    "text": "Image Readers\n\nsource\n\ntiff2torch\n\n tiff2torch (file_path:str)\n\nLoad tiff into pytorch tensor\n\nsource\n\n\nstring2dict\n\n string2dict (input_string:str)\n\n\nsource\n\n\nsplit_path\n\n split_path (file_path,\n             exts:(&lt;class'fastcore.foundation.L'&gt;,&lt;class'list'&gt;)=['.ome.ti\n             ff', '.tiff', '.tif', '.png'])\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nThe path to the file to split\n\n\nexts\n(&lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n[‘.ome.tiff’, ‘.tiff’, ‘.tif’, ‘.png’]\nList of filename extensions\n\n\n\n\nsource\n\n\naics_image_reader\n\n aics_image_reader (ind_dict=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nind_dict\nNoneType\nNone\nDictionary indicating the channels to load\n\n\n\n\nfile_path = 'data_examples/example_tiff.tiff'\ntest_img, _ = aics_image_reader({'Z': 0})(file_path)\ntest_img.shape\n\n(1, 1, 512, 512)\n\n\n\n\nHierarchical Data Format\n\nsource\n\n\nsplit_hdf_path\n\n split_hdf_path (file_path,\n                 hdf5_exts:(&lt;class'fastcore.foundation.L'&gt;,&lt;class'list'&gt;)=\n                 ['.h5', '.hdf5'])\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nThe path to the HDF5 file to split\n\n\nhdf5_exts\n(&lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n[‘.h5’, ‘.hdf5’]\nList of filename extensions\n\n\n\n\nsource\n\n\nhdf5_reader\n\n hdf5_reader (dataset=None, patch=0,\n              hdf5_exts:(&lt;class'fastcore.foundation.L'&gt;,&lt;class'list'&gt;)=['.\n              h5', '.hdf5'])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nNoneType\nNone\nThe dataset to load\n\n\npatch\nint\n0\nThe patch to load from the dataset\n\n\nhdf5_exts\n(&lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n[‘.h5’, ‘.hdf5’]\nList of filename extensions\n\n\n\nImages can be loaded by explicitly writing dataset name and path number…\n\nfrom bioMONAI.visualize import plot_image\n\n\nfile_path = './data_examples/0450_1.hdf5'\ndataset_name='clean'\npatch_num=10\n\nim , _ = hdf5_reader(dataset=dataset_name, patch=patch_num)(file_path)\nplot_image(im[0])\n\n\n\n\n… or enconding them in the path, where datasets are subfolders and patches the image files. The latter being compatible with image_reader syntaxis.\n\nf = file_path + '/' + dataset_name + '/' + '%d'%(patch_num)\nim , _ = hdf5_reader()(f)\nplot_image(im[0])\n\n\n\n\n\n\nPreprocessing\n\n\nLoad and preprocess\n\norg_img, _, _ = _load_and_preprocess(f)\n\ntest_eq(org_img.data[0].shape, im.shape)\n\n\n\nRead multichannel data\n\nt = _multi_sequence([f], only_tensor=True);\ntest_eq(t[0].shape, im.shape)\n\n\nt.shape\n\ntorch.Size([1, 1, 96, 96])\n\n\n\n\nImage reader\n\nsource\n\n\nimage_reader\n\n image_reader (file_path:(&lt;class'str'&gt;,&lt;class'pathlib.Path'&gt;,&lt;class'fastco\n               re.foundation.L'&gt;,&lt;class'list'&gt;), dtype=&lt;class\n               'torch.Tensor'&gt;, only_tensor:bool=True, **kwargs)\n\n*Loads and preprocesses a medical image.\nArgs: file_path: Path to the image. Can be a string, Path object or a list. dtype: Datatype for the return value. Defaults to torchTensor. reorder: Whether to reorder the data to be closest to canonical (RAS+) orientation. Defaults to False. resample: Whether to resample image to different voxel sizes and image dimensions. Defaults to None. only_tensor: To return only an image tensor. Defaults to True.\nReturns: The preprocessed image. Returns only the image tensor if only_tensor is True, otherwise returns original image, preprocessed image, and original size.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n(&lt;class ‘str’&gt;, &lt;class ‘pathlib.Path’&gt;, &lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n\nPath to the image\n\n\ndtype\n_TensorMeta\nTensor\nDatatype for the return value. Defaults to torchTensor\n\n\nonly_tensor\nbool\nTrue\nTo return only an image tensor\n\n\nkwargs\n\n\n\n\n\n\n\ntest_eq(image_reader(f)[0].shape, im.shape)"
  },
  {
    "objectID": "callbacks.html",
    "href": "callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "Callbacks that add functionlities during the training phase, including Callbacks that make decisions depending how a monitored metric/loss behaves\n\nsource\n\n\n\n MeanLossGraphCallback (after_create=None, before_fit=None,\n                        before_epoch=None, before_train=None,\n                        before_batch=None, after_pred=None,\n                        after_loss=None, before_backward=None,\n                        after_cancel_backward=None, after_backward=None,\n                        before_step=None, after_cancel_step=None,\n                        after_step=None, after_cancel_batch=None,\n                        after_batch=None, after_cancel_train=None,\n                        after_train=None, before_validate=None,\n                        after_cancel_validate=None, after_validate=None,\n                        after_cancel_epoch=None, after_epoch=None,\n                        after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\n\nsource\n\n\n\n\n ShortEpochCallback (pct=0.01, short_valid=True)\n\nFit just pct of an epoch, then stop\n\nsource\n\n\n\n\n GradientAccumulation (n_acc=32)\n\nAccumulate gradients before updating weights\n\nsource\n\n\n\n\n EarlyStoppingCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                        patience=1, reset_on_fit=True)\n\nA TrackerCallback that terminates training when monitored quantity stops improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n SaveModelCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                    fname='model', every_epoch=False, at_end=False,\n                    with_opt=False, reset_on_fit=True)\n\nA TrackerCallback that saves the model’s best during training and loads it at the end.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\nfname\nstr\nmodel\nmodel name to be used when saving model.\n\n\nevery_epoch\nbool\nFalse\nif true, save model after every epoch; else save only when model is better than existing best.\n\n\nat_end\nbool\nFalse\nif true, save model when training ends; else load best model if there is only one saved model.\n\n\nwith_opt\nbool\nFalse\nif true, save optimizer state (if any available) when saving model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n ReduceLROnPlateau (monitor='valid_loss', comp=None, min_delta=0.0,\n                    patience=1, factor=10.0, min_lr=0, reset_on_fit=True)\n\nA TrackerCallback that reduces learning rate when a metric has stopped improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nfactor\nfloat\n10.0\nthe denominator to divide the learning rate by, when reducing the learning rate.\n\n\nmin_lr\nint\n0\nthe minimum learning rate allowed; learning rate cannot be reduced below this minimum.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss)."
  },
  {
    "objectID": "callbacks.html#training-callbacks",
    "href": "callbacks.html#training-callbacks",
    "title": "Callbacks",
    "section": "",
    "text": "Callbacks that add functionlities during the training phase, including Callbacks that make decisions depending how a monitored metric/loss behaves\n\nsource\n\n\n\n MeanLossGraphCallback (after_create=None, before_fit=None,\n                        before_epoch=None, before_train=None,\n                        before_batch=None, after_pred=None,\n                        after_loss=None, before_backward=None,\n                        after_cancel_backward=None, after_backward=None,\n                        before_step=None, after_cancel_step=None,\n                        after_step=None, after_cancel_batch=None,\n                        after_batch=None, after_cancel_train=None,\n                        after_train=None, before_validate=None,\n                        after_cancel_validate=None, after_validate=None,\n                        after_cancel_epoch=None, after_epoch=None,\n                        after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\n\nsource\n\n\n\n\n ShortEpochCallback (pct=0.01, short_valid=True)\n\nFit just pct of an epoch, then stop\n\nsource\n\n\n\n\n GradientAccumulation (n_acc=32)\n\nAccumulate gradients before updating weights\n\nsource\n\n\n\n\n EarlyStoppingCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                        patience=1, reset_on_fit=True)\n\nA TrackerCallback that terminates training when monitored quantity stops improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n SaveModelCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                    fname='model', every_epoch=False, at_end=False,\n                    with_opt=False, reset_on_fit=True)\n\nA TrackerCallback that saves the model’s best during training and loads it at the end.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\nfname\nstr\nmodel\nmodel name to be used when saving model.\n\n\nevery_epoch\nbool\nFalse\nif true, save model after every epoch; else save only when model is better than existing best.\n\n\nat_end\nbool\nFalse\nif true, save model when training ends; else load best model if there is only one saved model.\n\n\nwith_opt\nbool\nFalse\nif true, save optimizer state (if any available) when saving model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n ReduceLROnPlateau (monitor='valid_loss', comp=None, min_delta=0.0,\n                    patience=1, factor=10.0, min_lr=0, reset_on_fit=True)\n\nA TrackerCallback that reduces learning rate when a metric has stopped improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nfactor\nfloat\n10.0\nthe denominator to divide the learning rate by, when reducing the learning rate.\n\n\nmin_lr\nint\n0\nthe minimum learning rate allowed; learning rate cannot be reduced below this minimum.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss)."
  },
  {
    "objectID": "callbacks.html#schedulers",
    "href": "callbacks.html#schedulers",
    "title": "Callbacks",
    "section": "Schedulers",
    "text": "Schedulers\nCallback and helper functions to schedule hyper-parameters\n\nsource\n\nParamScheduler\n\n ParamScheduler (scheds)\n\nSchedule hyper-parameters according to scheds\nscheds is a dictionary with one key for each hyper-parameter you want to schedule, with either a scheduler or a list of schedulers as values (in the second case, the list must have the same length as the the number of parameters groups of the optimizer).\n\nsource\n\n\nSchedCos\n\n SchedCos (start, end)\n\nCosine schedule function from start to end\n\nsource\n\n\nSchedExp\n\n SchedExp (start, end)\n\nExponential schedule function from start to end\n\nsource\n\n\nSchedLin\n\n SchedLin (start, end)\n\nLinear schedule function from start to end\n\nsource\n\n\nSchedNo\n\n SchedNo (start, end)\n\nConstant schedule function with start value"
  }
]